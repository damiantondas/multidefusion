{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MultiDEFusion","text":"<p>MultiDEFusion is an open-source library that enables the Fusion of Multi geodetic techniques ensuring long-term observations of ground DEFormation. The software provides integration of permanent GNSS data and radar InSAR observations, considering a particular computational method such as DInSAR, SBAS and PSI.</p> <p>The MultiDEFusion library expands on the approach to integrating geodetic observations outlined in the article published by Tonda\u015b et al., 2023. The software documentation can be found at https://damiantondas.github.io/multidefusion (Free software: MIT license).</p> <p>If you find the MultiDEFusion program useful, please cite it in your work as: Tonda\u015b D., Ilieva M., van Leijen F., van der Marel H., Rohm W. Kalman filter-based integration of GNSS and InSAR observations for local nonlinear strong deformations. Journal of Geodesy, 97, 109 (2023). DOI: 10.1007/s00190-023-01789-z</p>"},{"location":"authors/","title":"Authors","text":"<p>MultiDEFusion library code developers:</p>"},{"location":"authors/#damian-tondas","title":"Damian Tonda\u015b","text":"<p>damian.tondas@gmail.com</p>"},{"location":"authors/#krzysztof-knop","title":"Krzysztof Knop","text":"<p>knop.krzysiek@gmail.com</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#01-24042024","title":"[0.1] - 24.04.2024","text":"<ul> <li>This is first version of the MultiDEFusion library.</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/damiantondas/multidefusion/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>MultiDEFusion could always use more documentation, whether as part of the official multidefusion docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/damiantondas/multidefusion/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome!</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up multidefusion for local development.</p> <ol> <li> <p>Fork the multidefusion repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/multidefusion.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv multidefusion\n$ cd multidefusion/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 multidefusion tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and     for PyPy. Check https://github.com/damiantondas/multidefusion/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"datainterface/","title":"datainterface module","text":""},{"location":"datainterface/#multidefusion.datainterface.BaseData","title":"<code>BaseData</code>","text":"<p>Base class for handling common data operations.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data file.</p> required <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The path to the data file.</p> <code>sublabel</code> <code>str</code> <p>The sublabel extracted from the file path.</p> <code>label</code> <code>str</code> <p>The label extracted from the file path.</p> <code>type</code> <code>str</code> <p>The type extracted from the label.</p> <code>data</code> <code>DataFrame</code> <p>The loaded data.</p> <code>latest_date</code> <code>Timestamp</code> <p>The latest date in the data.</p> <code>oldest_date</code> <code>Timestamp</code> <p>The oldest date in the data.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>class BaseData:\n    \"\"\"\n    Base class for handling common data operations.\n\n    Args:\n        path (str): The path to the data file.\n\n    Attributes:\n        path (str): The path to the data file.\n        sublabel (str): The sublabel extracted from the file path.\n        label (str): The label extracted from the file path.\n        type (str): The type extracted from the label.\n        data (pd.DataFrame): The loaded data.\n        latest_date (pd.Timestamp): The latest date in the data.\n        oldest_date (pd.Timestamp): The oldest date in the data.\n    \"\"\"\n\n    TIME_INTERVAL_NUM = 1\n    TIME_INTERVAL = 'D'\n\n    def __init__(self, path: str) -&gt; None:\n        \"\"\"\n        Initializes BaseData object.\n\n        Args:\n            path (str): The path to the data file.\n        \"\"\"\n        self.path = path\n        self.sublabel = None\n        self.label = self.add_label_to_data()\n        self.type = self.label.split(\"_\")[0]\n        self.data = self.load_data()\n        self.latest_date = self.data.index.max()\n        self.oldest_date = self.data.index.min()\n\n    def load_csv_data(self, header: List[str]) -&gt; pd.DataFrame:\n        \"\"\"\n        Loads data from the specified file.\n\n        Args:\n            header (List[str]): List of column names.\n\n        Returns:\n            pd.DataFrame: Loaded data.\n        \"\"\"\n        with open(self.path, 'r') as file:\n            first_line = file.readline()\n        sep = \"\\s+|,\" if \",\" in first_line else \"\\s+\"\n        data = pd.read_csv(self.path, sep=sep, header=None, skiprows=1, names=header, engine=\"python\")\n        return data\n\n    def load_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Abstract method to be implemented by subclasses for loading data.\n\n        Returns:\n            pd.DataFrame: Loaded data.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement the load_data method\")\n\n    def get_observation(self, row_of_data: Any) -&gt; Any:\n        \"\"\"\n        Abstract method to be implemented by subclasses for extracting observations.\n\n        Args:\n            row_of_data: A row of data.\n\n        Returns:\n            Any: Extracted observation.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement the get_observation method\")\n\n    def add_label_to_data(self) -&gt; str:\n        \"\"\"\n        Extracts and returns the label and set sublabel from the file path.\n\n        Returns:\n            str: Extracted label.\n        \"\"\"\n        splitted = os.path.split(self.path)[-1].split(\".\")[0].split(\"_\")\n        if len(splitted) == 3:\n            self.sublabel = splitted[-1]\n            return splitted[0] + \"_\" + splitted[1]\n        elif len(splitted) == 2:\n            return splitted[0] + \"_\" + splitted[1]\n        elif len(splitted) == 1:\n            return splitted[0]\n\n    def get_data_by_date(self, date: pd.Timestamp, columns_list: List[str] = None) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"\n        Gets data for a specific date.\n\n        Args:\n            date (pd.Timestamp): The date for which data is requested.\n            columns_list (List[str]): List of column names to be returned.\n\n        Returns:\n            Union[pd.DataFrame, None]: Data for the specified date (or None if not found).\n        \"\"\"\n        if date in self.data.index:\n            data_by_date = self.data.loc[date]\n            if not data_by_date.isnull().values.any():\n                if columns_list:\n                    return data_by_date[columns_list]\n                else:\n                    return data_by_date\n        return None\n\n    def process_timestamp_columns(self) -&gt; None:\n        \"\"\"\n        Process timestamp columns in the data.\n\n        Converts GNSS, SBAS, and PSI \"YYYY\", \"MM\", \"DD\" columns into a single \"timestamp\" column,\n        sets it as the index, and resamples the data based on the time interval.\n        \"\"\"\n        self.data[\"timestamp\"] = pd.to_datetime(self.data[[\"YYYY\", \"MM\", \"DD\"]].astype(int).astype(str).apply(\" \".join, 1), format=\"%Y %m %d\")\n        self.data = self.data.drop([\"YYYY\", \"MM\", \"DD\"], axis=1)\n        self.data.set_index([\"timestamp\"], inplace=True)\n        self.data = self.data.resample(self.TIME_INTERVAL).asfreq()\n\n    def convert_range_into_two_timestamps(self) -&gt; None:\n        \"\"\"\n        Convert DInSAR range columns into two timestamp columns.\n\n        Converts \"YYYY1\", \"MM1\", \"DD1\", \"YYYY2\", \"MM2\", \"DD2\" columns into\n        \"timestamp1\" and \"timestamp2\" columns, and sets them as the index.\n        \"\"\"\n        self.data[\"timestamp1\"] = pd.to_datetime(self.data[[\"YYYY1\", \"MM1\", \"DD1\"]].astype(int).astype(str).apply(\" \".join, 1), format=\"%Y %m %d\")\n        self.data[\"timestamp2\"] = pd.to_datetime(self.data[[\"YYYY2\", \"MM2\", \"DD2\"]].astype(int).astype(str).apply(\" \".join, 1), format=\"%Y %m %d\")\n        self.data = self.data.drop([\"YYYY1\", \"MM1\", \"DD1\", \"YYYY2\", \"MM2\", \"DD2\"], axis=1)\n        self.data.set_index([\"timestamp1\", \"timestamp2\"], inplace=True)\n\n    def replace_decimal_sep(self) -&gt; None:\n        \"\"\"\n        Replace decimal separators in non-float columns.\n\n        Replaces commas with dots in non-float columns and converts them to float.\n        \"\"\"\n        for column in self.data.columns:\n            if self.data[column].dtype != float:\n                try:\n                    self.data[column] = self.data[column].replace(\",\", \".\", regex=True).astype(float)\n                except ValueError:\n                    return None\n\n    def create_projection_matrix_and_error(self, row_of_data: pd.Series) -&gt; Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n        \"\"\"\n        Abstract method to be implemented by subclasses for creating projection matrix and error matrix.\n\n        Args:\n            row_of_data (pd.Series): A row of data.\n\n        Returns:\n            Tuple[Optional[np.ndarray], Optional[np.ndarray]]: Projection matrix and error matrix.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement the create_projection_matrix_and_error method\")\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.BaseData.__init__","title":"<code>__init__(path)</code>","text":"<p>Initializes BaseData object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data file.</p> required Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def __init__(self, path: str) -&gt; None:\n    \"\"\"\n    Initializes BaseData object.\n\n    Args:\n        path (str): The path to the data file.\n    \"\"\"\n    self.path = path\n    self.sublabel = None\n    self.label = self.add_label_to_data()\n    self.type = self.label.split(\"_\")[0]\n    self.data = self.load_data()\n    self.latest_date = self.data.index.max()\n    self.oldest_date = self.data.index.min()\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.BaseData.add_label_to_data","title":"<code>add_label_to_data()</code>","text":"<p>Extracts and returns the label and set sublabel from the file path.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted label.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def add_label_to_data(self) -&gt; str:\n    \"\"\"\n    Extracts and returns the label and set sublabel from the file path.\n\n    Returns:\n        str: Extracted label.\n    \"\"\"\n    splitted = os.path.split(self.path)[-1].split(\".\")[0].split(\"_\")\n    if len(splitted) == 3:\n        self.sublabel = splitted[-1]\n        return splitted[0] + \"_\" + splitted[1]\n    elif len(splitted) == 2:\n        return splitted[0] + \"_\" + splitted[1]\n    elif len(splitted) == 1:\n        return splitted[0]\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.BaseData.convert_range_into_two_timestamps","title":"<code>convert_range_into_two_timestamps()</code>","text":"<p>Convert DInSAR range columns into two timestamp columns.</p> <p>Converts \"YYYY1\", \"MM1\", \"DD1\", \"YYYY2\", \"MM2\", \"DD2\" columns into \"timestamp1\" and \"timestamp2\" columns, and sets them as the index.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def convert_range_into_two_timestamps(self) -&gt; None:\n    \"\"\"\n    Convert DInSAR range columns into two timestamp columns.\n\n    Converts \"YYYY1\", \"MM1\", \"DD1\", \"YYYY2\", \"MM2\", \"DD2\" columns into\n    \"timestamp1\" and \"timestamp2\" columns, and sets them as the index.\n    \"\"\"\n    self.data[\"timestamp1\"] = pd.to_datetime(self.data[[\"YYYY1\", \"MM1\", \"DD1\"]].astype(int).astype(str).apply(\" \".join, 1), format=\"%Y %m %d\")\n    self.data[\"timestamp2\"] = pd.to_datetime(self.data[[\"YYYY2\", \"MM2\", \"DD2\"]].astype(int).astype(str).apply(\" \".join, 1), format=\"%Y %m %d\")\n    self.data = self.data.drop([\"YYYY1\", \"MM1\", \"DD1\", \"YYYY2\", \"MM2\", \"DD2\"], axis=1)\n    self.data.set_index([\"timestamp1\", \"timestamp2\"], inplace=True)\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.BaseData.create_projection_matrix_and_error","title":"<code>create_projection_matrix_and_error(row_of_data)</code>","text":"<p>Abstract method to be implemented by subclasses for creating projection matrix and error matrix.</p> <p>Parameters:</p> Name Type Description Default <code>row_of_data</code> <code>Series</code> <p>A row of data.</p> required <p>Returns:</p> Type Description <code>Tuple[Optional[ndarray], Optional[ndarray]]</code> <p>Tuple[Optional[np.ndarray], Optional[np.ndarray]]: Projection matrix and error matrix.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def create_projection_matrix_and_error(self, row_of_data: pd.Series) -&gt; Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n    \"\"\"\n    Abstract method to be implemented by subclasses for creating projection matrix and error matrix.\n\n    Args:\n        row_of_data (pd.Series): A row of data.\n\n    Returns:\n        Tuple[Optional[np.ndarray], Optional[np.ndarray]]: Projection matrix and error matrix.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement the create_projection_matrix_and_error method\")\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.BaseData.get_data_by_date","title":"<code>get_data_by_date(date, columns_list=None)</code>","text":"<p>Gets data for a specific date.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>Timestamp</code> <p>The date for which data is requested.</p> required <code>columns_list</code> <code>List[str]</code> <p>List of column names to be returned.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>Union[pd.DataFrame, None]: Data for the specified date (or None if not found).</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def get_data_by_date(self, date: pd.Timestamp, columns_list: List[str] = None) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"\n    Gets data for a specific date.\n\n    Args:\n        date (pd.Timestamp): The date for which data is requested.\n        columns_list (List[str]): List of column names to be returned.\n\n    Returns:\n        Union[pd.DataFrame, None]: Data for the specified date (or None if not found).\n    \"\"\"\n    if date in self.data.index:\n        data_by_date = self.data.loc[date]\n        if not data_by_date.isnull().values.any():\n            if columns_list:\n                return data_by_date[columns_list]\n            else:\n                return data_by_date\n    return None\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.BaseData.get_observation","title":"<code>get_observation(row_of_data)</code>","text":"<p>Abstract method to be implemented by subclasses for extracting observations.</p> <p>Parameters:</p> Name Type Description Default <code>row_of_data</code> <code>Any</code> <p>A row of data.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Extracted observation.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def get_observation(self, row_of_data: Any) -&gt; Any:\n    \"\"\"\n    Abstract method to be implemented by subclasses for extracting observations.\n\n    Args:\n        row_of_data: A row of data.\n\n    Returns:\n        Any: Extracted observation.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement the get_observation method\")\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.BaseData.load_csv_data","title":"<code>load_csv_data(header)</code>","text":"<p>Loads data from the specified file.</p> <p>Parameters:</p> Name Type Description Default <code>header</code> <code>List[str]</code> <p>List of column names.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Loaded data.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def load_csv_data(self, header: List[str]) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads data from the specified file.\n\n    Args:\n        header (List[str]): List of column names.\n\n    Returns:\n        pd.DataFrame: Loaded data.\n    \"\"\"\n    with open(self.path, 'r') as file:\n        first_line = file.readline()\n    sep = \"\\s+|,\" if \",\" in first_line else \"\\s+\"\n    data = pd.read_csv(self.path, sep=sep, header=None, skiprows=1, names=header, engine=\"python\")\n    return data\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.BaseData.load_data","title":"<code>load_data()</code>","text":"<p>Abstract method to be implemented by subclasses for loading data.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Loaded data.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def load_data(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Abstract method to be implemented by subclasses for loading data.\n\n    Returns:\n        pd.DataFrame: Loaded data.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement the load_data method\")\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.BaseData.process_timestamp_columns","title":"<code>process_timestamp_columns()</code>","text":"<p>Process timestamp columns in the data.</p> <p>Converts GNSS, SBAS, and PSI \"YYYY\", \"MM\", \"DD\" columns into a single \"timestamp\" column, sets it as the index, and resamples the data based on the time interval.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def process_timestamp_columns(self) -&gt; None:\n    \"\"\"\n    Process timestamp columns in the data.\n\n    Converts GNSS, SBAS, and PSI \"YYYY\", \"MM\", \"DD\" columns into a single \"timestamp\" column,\n    sets it as the index, and resamples the data based on the time interval.\n    \"\"\"\n    self.data[\"timestamp\"] = pd.to_datetime(self.data[[\"YYYY\", \"MM\", \"DD\"]].astype(int).astype(str).apply(\" \".join, 1), format=\"%Y %m %d\")\n    self.data = self.data.drop([\"YYYY\", \"MM\", \"DD\"], axis=1)\n    self.data.set_index([\"timestamp\"], inplace=True)\n    self.data = self.data.resample(self.TIME_INTERVAL).asfreq()\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.BaseData.replace_decimal_sep","title":"<code>replace_decimal_sep()</code>","text":"<p>Replace decimal separators in non-float columns.</p> <p>Replaces commas with dots in non-float columns and converts them to float.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def replace_decimal_sep(self) -&gt; None:\n    \"\"\"\n    Replace decimal separators in non-float columns.\n\n    Replaces commas with dots in non-float columns and converts them to float.\n    \"\"\"\n    for column in self.data.columns:\n        if self.data[column].dtype != float:\n            try:\n                self.data[column] = self.data[column].replace(\",\", \".\", regex=True).astype(float)\n            except ValueError:\n                return None\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.GNSSData","title":"<code>GNSSData</code>","text":"<p>             Bases: <code>BaseData</code></p> <p>Class for handling GNSS data operations, inheriting from BaseData.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the GNSS data file.</p> required Source code in <code>multidefusion\\datainterface.py</code> <pre><code>class GNSSData(BaseData):\n    \"\"\"\n    Class for handling GNSS data operations, inheriting from BaseData.\n\n    Args:\n        path (str): The path to the GNSS data file.\n\n    Attributes:\n        Inherits attributes from BaseData.\n    \"\"\"\n\n    HEADER_GNSS = ['YYYY', 'MM', 'DD', 'X', 'Y', 'Z', 'mX', 'mY', 'mZ']\n\n    def __init__(self, path: str) -&gt; None:\n        \"\"\"\n        Initializes GNSSData object.\n\n        Args:\n            path (str): The path to the GNSS data file.\n        \"\"\"\n        super().__init__(path)\n\n    def load_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Loads GNSS data from the specified file.\n\n        Returns:\n            pd.DataFrame: Loaded GNSS data.\n        \"\"\"\n        header = getattr(GNSSData, \"HEADER_\" + self.type)\n        self.data = self.load_csv_data(header)\n\n        mean_xyz_first_five_epochs, F = self.create_rotation_matrix()\n        self.xyz_to_neu(mean_xyz_first_five_epochs, F)\n\n        self.process_timestamp_columns()\n        self.replace_decimal_sep()\n        return self.data\n\n    def create_projection_matrix_and_error(self, row_of_data: pd.Series) -&gt; Tuple[Union[np.ndarray, None], Union[np.ndarray, None]]:\n        \"\"\"\n        Creates a projection matrix and error matrix based on the provided row of data.\n\n        Args:\n            row_of_data (pd.Series): A row of GNSS data.\n\n        Returns:\n            Tuple[Union[np.ndarray, None], Union[np.ndarray, None]]: Projection matrix and error matrix.\n        \"\"\"\n        if row_of_data is not None:\n            projection_matrix = np.array([[1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0]])\n            # error_matrix = np.diag(row_of_data[[\"mN\", \"mE\", \"mU\"]].values ** 2) * 100  # ATTENTION!!!! Rephrasing the error due to the results reported by Bernese!!!!\n            error_matrix = row_of_data['error']\n            return projection_matrix, error_matrix\n        return None, None\n\n    def get_observation(self, row_of_data: pd.Series) -&gt; np.ndarray:\n        \"\"\"\n        Gets GNSS observation from the provided row of data.\n\n        Args:\n            row_of_data (pd.Series): A row of GNSS data.\n\n        Returns:\n            np.ndarray: GNSS observation.\n        \"\"\"\n        return row_of_data[[\"N\", \"E\", \"U\"]].values\n\n    @staticmethod\n    def xyz_to_blh(X, Y, Z):\n        \"\"\"\n        Converts Cartesian coordinates (X, Y, Z) to geodetic coordinates (latitude, longitude, height).\n\n        Args:\n            X (float): Cartesian coordinate in the X direction.\n            Y (float): Cartesian coordinate in the Y direction.\n            Z (float): Cartesian coordinate in the Z direction.\n\n        Returns:\n            Tuple[float, float, float]: Geodetic coordinates (latitude, longitude, height).\n        \"\"\"\n        a = 6378137\n        b = 6356752.31414\n        e2 = ((a * a) - (b * b)) / (a * a)\n        elat = 1e-12\n        eht = 1e-05\n        p = np.sqrt(X ** 2 + Y ** 2)\n        lat = np.arctan2(Z, p / (1 - e2))\n        h = 0\n        dh = 1\n        dlat = 1\n        i = 0\n        while np.any(dlat &gt; elat) or np.any(dh &gt; eht):\n            i += 1\n            lat0 = lat\n            h0 = h\n            v = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n            h = p / np.cos(lat) - v\n            lat = np.arctan2(Z, p * (1 - e2 * v / (v + h)))\n            dlat = np.abs(lat - lat0)\n            dh = np.abs(h - h0)\n        lon = np.arctan2(Y, X)\n        return lat, lon, h\n\n    def create_rotation_matrix(self):\n        \"\"\"\n        Creates a rotation matrix based on the mean coordinates of the first five epochs.\n\n        Returns:\n            Tuple[pd.Series, np.ndarray]: Mean coordinates of the first five epochs and the rotation matrix.\n        \"\"\"\n        mean_xyz_first_five_epochs = self.data.loc[:, [\"X\", \"Y\", \"Z\"]].head(5).mean(axis=0)\n        B, L, h = self.xyz_to_blh(mean_xyz_first_five_epochs[\"X\"], mean_xyz_first_five_epochs[\"Y\"], mean_xyz_first_five_epochs[\"Z\"])\n        F = np.array([[-np.sin(B) * np.cos(L), -np.sin(B) * np.sin(L), np.cos(B)],\n                      [-np.sin(L), np.cos(L), 0],\n                      [np.cos(B) * np.cos(L), np.cos(B) * np.sin(L), np.sin(B)]])\n        return mean_xyz_first_five_epochs, F\n\n    def xyz_to_neu(self, mean_xyz_first_five_epochs, F):\n        \"\"\"\n        Converts Cartesian coordinates (X, Y, Z) to local coordinates (North, East, Up).\n\n        Args:\n            mean_xyz_first_five_epochs (pd.Series): Mean coordinates of the first five epochs.\n            F (np.ndarray): Rotation matrix.\n\n        Returns:\n            None: Modifies the 'data' attribute in-place by updating coordinates and errors.\n        \"\"\"\n        columns_to_modify = [\"X\", \"Y\", \"Z\"]\n\n        self.data[columns_to_modify] -= mean_xyz_first_five_epochs[columns_to_modify]\n\n        def calculate_coordinates_and_errors_for_row(row):\n            NEU = np.dot(F, np.array([row[\"X\"], row[\"Y\"], row[\"Z\"]]))\n            errors = np.dot(np.dot(F, np.diag([row[\"mX\"] ** 2, row[\"mY\"] ** 2, row[\"mZ\"] ** 2])), F.transpose())\n            row[\"N\"], row[\"E\"], row[\"U\"] = NEU\n            row[\"error\"] = errors\n            return row\n\n        self.data = self.data.apply(calculate_coordinates_and_errors_for_row, axis=1)\n        self.data = self.data.drop([\"X\", \"Y\", \"Z\", \"mX\", \"mY\", \"mZ\"], axis=1)\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.GNSSData.__init__","title":"<code>__init__(path)</code>","text":"<p>Initializes GNSSData object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the GNSS data file.</p> required Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def __init__(self, path: str) -&gt; None:\n    \"\"\"\n    Initializes GNSSData object.\n\n    Args:\n        path (str): The path to the GNSS data file.\n    \"\"\"\n    super().__init__(path)\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.GNSSData.create_projection_matrix_and_error","title":"<code>create_projection_matrix_and_error(row_of_data)</code>","text":"<p>Creates a projection matrix and error matrix based on the provided row of data.</p> <p>Parameters:</p> Name Type Description Default <code>row_of_data</code> <code>Series</code> <p>A row of GNSS data.</p> required <p>Returns:</p> Type Description <code>Tuple[Union[ndarray, None], Union[ndarray, None]]</code> <p>Tuple[Union[np.ndarray, None], Union[np.ndarray, None]]: Projection matrix and error matrix.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def create_projection_matrix_and_error(self, row_of_data: pd.Series) -&gt; Tuple[Union[np.ndarray, None], Union[np.ndarray, None]]:\n    \"\"\"\n    Creates a projection matrix and error matrix based on the provided row of data.\n\n    Args:\n        row_of_data (pd.Series): A row of GNSS data.\n\n    Returns:\n        Tuple[Union[np.ndarray, None], Union[np.ndarray, None]]: Projection matrix and error matrix.\n    \"\"\"\n    if row_of_data is not None:\n        projection_matrix = np.array([[1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0]])\n        # error_matrix = np.diag(row_of_data[[\"mN\", \"mE\", \"mU\"]].values ** 2) * 100  # ATTENTION!!!! Rephrasing the error due to the results reported by Bernese!!!!\n        error_matrix = row_of_data['error']\n        return projection_matrix, error_matrix\n    return None, None\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.GNSSData.create_rotation_matrix","title":"<code>create_rotation_matrix()</code>","text":"<p>Creates a rotation matrix based on the mean coordinates of the first five epochs.</p> <p>Returns:</p> Type Description <p>Tuple[pd.Series, np.ndarray]: Mean coordinates of the first five epochs and the rotation matrix.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def create_rotation_matrix(self):\n    \"\"\"\n    Creates a rotation matrix based on the mean coordinates of the first five epochs.\n\n    Returns:\n        Tuple[pd.Series, np.ndarray]: Mean coordinates of the first five epochs and the rotation matrix.\n    \"\"\"\n    mean_xyz_first_five_epochs = self.data.loc[:, [\"X\", \"Y\", \"Z\"]].head(5).mean(axis=0)\n    B, L, h = self.xyz_to_blh(mean_xyz_first_five_epochs[\"X\"], mean_xyz_first_five_epochs[\"Y\"], mean_xyz_first_five_epochs[\"Z\"])\n    F = np.array([[-np.sin(B) * np.cos(L), -np.sin(B) * np.sin(L), np.cos(B)],\n                  [-np.sin(L), np.cos(L), 0],\n                  [np.cos(B) * np.cos(L), np.cos(B) * np.sin(L), np.sin(B)]])\n    return mean_xyz_first_five_epochs, F\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.GNSSData.get_observation","title":"<code>get_observation(row_of_data)</code>","text":"<p>Gets GNSS observation from the provided row of data.</p> <p>Parameters:</p> Name Type Description Default <code>row_of_data</code> <code>Series</code> <p>A row of GNSS data.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: GNSS observation.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def get_observation(self, row_of_data: pd.Series) -&gt; np.ndarray:\n    \"\"\"\n    Gets GNSS observation from the provided row of data.\n\n    Args:\n        row_of_data (pd.Series): A row of GNSS data.\n\n    Returns:\n        np.ndarray: GNSS observation.\n    \"\"\"\n    return row_of_data[[\"N\", \"E\", \"U\"]].values\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.GNSSData.load_data","title":"<code>load_data()</code>","text":"<p>Loads GNSS data from the specified file.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Loaded GNSS data.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def load_data(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads GNSS data from the specified file.\n\n    Returns:\n        pd.DataFrame: Loaded GNSS data.\n    \"\"\"\n    header = getattr(GNSSData, \"HEADER_\" + self.type)\n    self.data = self.load_csv_data(header)\n\n    mean_xyz_first_five_epochs, F = self.create_rotation_matrix()\n    self.xyz_to_neu(mean_xyz_first_five_epochs, F)\n\n    self.process_timestamp_columns()\n    self.replace_decimal_sep()\n    return self.data\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.GNSSData.xyz_to_blh","title":"<code>xyz_to_blh(X, Y, Z)</code>  <code>staticmethod</code>","text":"<p>Converts Cartesian coordinates (X, Y, Z) to geodetic coordinates (latitude, longitude, height).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>float</code> <p>Cartesian coordinate in the X direction.</p> required <code>Y</code> <code>float</code> <p>Cartesian coordinate in the Y direction.</p> required <code>Z</code> <code>float</code> <p>Cartesian coordinate in the Z direction.</p> required <p>Returns:</p> Type Description <p>Tuple[float, float, float]: Geodetic coordinates (latitude, longitude, height).</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>@staticmethod\ndef xyz_to_blh(X, Y, Z):\n    \"\"\"\n    Converts Cartesian coordinates (X, Y, Z) to geodetic coordinates (latitude, longitude, height).\n\n    Args:\n        X (float): Cartesian coordinate in the X direction.\n        Y (float): Cartesian coordinate in the Y direction.\n        Z (float): Cartesian coordinate in the Z direction.\n\n    Returns:\n        Tuple[float, float, float]: Geodetic coordinates (latitude, longitude, height).\n    \"\"\"\n    a = 6378137\n    b = 6356752.31414\n    e2 = ((a * a) - (b * b)) / (a * a)\n    elat = 1e-12\n    eht = 1e-05\n    p = np.sqrt(X ** 2 + Y ** 2)\n    lat = np.arctan2(Z, p / (1 - e2))\n    h = 0\n    dh = 1\n    dlat = 1\n    i = 0\n    while np.any(dlat &gt; elat) or np.any(dh &gt; eht):\n        i += 1\n        lat0 = lat\n        h0 = h\n        v = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - v\n        lat = np.arctan2(Z, p * (1 - e2 * v / (v + h)))\n        dlat = np.abs(lat - lat0)\n        dh = np.abs(h - h0)\n    lon = np.arctan2(Y, X)\n    return lat, lon, h\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.GNSSData.xyz_to_neu","title":"<code>xyz_to_neu(mean_xyz_first_five_epochs, F)</code>","text":"<p>Converts Cartesian coordinates (X, Y, Z) to local coordinates (North, East, Up).</p> <p>Parameters:</p> Name Type Description Default <code>mean_xyz_first_five_epochs</code> <code>Series</code> <p>Mean coordinates of the first five epochs.</p> required <code>F</code> <code>ndarray</code> <p>Rotation matrix.</p> required <p>Returns:</p> Name Type Description <code>None</code> <p>Modifies the 'data' attribute in-place by updating coordinates and errors.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def xyz_to_neu(self, mean_xyz_first_five_epochs, F):\n    \"\"\"\n    Converts Cartesian coordinates (X, Y, Z) to local coordinates (North, East, Up).\n\n    Args:\n        mean_xyz_first_five_epochs (pd.Series): Mean coordinates of the first five epochs.\n        F (np.ndarray): Rotation matrix.\n\n    Returns:\n        None: Modifies the 'data' attribute in-place by updating coordinates and errors.\n    \"\"\"\n    columns_to_modify = [\"X\", \"Y\", \"Z\"]\n\n    self.data[columns_to_modify] -= mean_xyz_first_five_epochs[columns_to_modify]\n\n    def calculate_coordinates_and_errors_for_row(row):\n        NEU = np.dot(F, np.array([row[\"X\"], row[\"Y\"], row[\"Z\"]]))\n        errors = np.dot(np.dot(F, np.diag([row[\"mX\"] ** 2, row[\"mY\"] ** 2, row[\"mZ\"] ** 2])), F.transpose())\n        row[\"N\"], row[\"E\"], row[\"U\"] = NEU\n        row[\"error\"] = errors\n        return row\n\n    self.data = self.data.apply(calculate_coordinates_and_errors_for_row, axis=1)\n    self.data = self.data.drop([\"X\", \"Y\", \"Z\", \"mX\", \"mY\", \"mZ\"], axis=1)\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.SARData","title":"<code>SARData</code>","text":"<p>             Bases: <code>BaseData</code></p> <p>Class for handling SAR data operations, inheriting from BaseData.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the SAR data file.</p> required Source code in <code>multidefusion\\datainterface.py</code> <pre><code>class SARData(BaseData):\n    \"\"\"\n    Class for handling SAR data operations, inheriting from BaseData.\n\n    Args:\n        path (str): The path to the SAR data file.\n\n    Attributes:\n        Inherits attributes from BaseData.\n    \"\"\"\n\n    SENTINEL_WAVELENGTH = 0.055465763\n    HEADER_DInSAR = ['YYYY1', 'MM1', 'DD1', 'YYYY2', 'MM2', 'DD2', 'DSP', 'INC_ANG', 'HEAD_ANG']\n    HEADER_PSI = ['YYYY', 'MM', 'DD', 'DSP', 'INC_ANG', 'HEAD_ANG']\n    HEADER_SBAS = ['YYYY', 'MM', 'DD', 'DSP', 'INC_ANG', 'HEAD_ANG']\n\n    def __init__(self, path: str) -&gt; None:\n        \"\"\"\n        Initializes SARData object.\n\n        Args:\n            path (str): The path to the SAR data file.\n        \"\"\"\n        super().__init__(path)\n        self.bias_reduction = False\n        self.get_header_from_file()\n\n    def get_header_from_file(self):\n        with open(self.path, 'r') as file:\n            first_line = file.readline()\n        try:\n            if 'COH' in first_line:\n                header = getattr(SARData, \"HEADER_\" + self.type) + [\"COH\"]\n                return header\n            elif 'ERROR' in first_line:\n                header = getattr(SARData, \"HEADER_\" + self.type) + [\"ERROR\"]\n                return header\n            else:\n                raise ValueError(f\"The header cannot be identified in the {self.path} file.\")\n        except ZeroDivisionError:\n            sys.exit(1)\n\n    def load_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Loads SAR data from the specified file.\n\n        Returns:\n            pd.DataFrame: Loaded SAR data.\n        \"\"\"\n        header = self.get_header_from_file()\n        self.data = self.load_csv_data(header)\n        self.replace_decimal_sep()\n        if \"COH\" in header:\n            self.coherence_to_error()\n        if self.type == \"DInSAR\":\n            self.convert_range_into_two_timestamps()\n        else:\n            self.process_timestamp_columns()\n        self.expand_dataframe_by_date_range()\n        return self.data\n\n    def reduce_bias_to_gnss(self, date: pd.Timestamp):\n        \"\"\"\n        Reduces bias in SAR data to GNSS data.\n\n        This method reduces the bias in the SAR data by removing data points prior to\n        the specified date and adjusting the 'DSP' values based on the first non-null value\n        after the specified date.\n\n        Args:\n            date (pd.Timestamp): The timestamp used as a reference point for bias reduction.\n\n        Returns:\n            None\n        \"\"\"\n        self.data = self.data[self.data.index &gt; date]\n        first_non_null_value = self.data[\"DSP\"].first_valid_index()\n        if first_non_null_value is not None:\n            self.data[\"DSP\"] = self.data[\"DSP\"] - self.data.at[first_non_null_value, \"DSP\"]\n\n    def coherence_to_error(self) -&gt; None:\n        \"\"\"\n        Convert coherence column to error column in the data.\n        \"\"\"\n        sentinel_wavelength = self.SENTINEL_WAVELENGTH\n\n        def calculate_error(coherence: float) -&gt; float:\n            \"\"\"\n            Calculate error from coherence.\n\n            Args:\n                coherence (float): Coherence value.\n\n            Returns:\n                float: Calculated error.\n            \"\"\"\n            li = 0\n            k = 1\n            while True:\n                li += (coherence ** (2 * k)) / (k ** 2)\n                li2 = li + (coherence ** (2 * k)) / (k ** 2)\n                if abs(li - li2) &lt;= 1e-10:\n                    break\n                k += 1\n            phase = pi ** 2/3 - pi * asin(coherence) + asin(coherence) ** 2 - 0.5 * li2\n            return sqrt(phase) * sentinel_wavelength / (4 * pi)\n\n        self.data[\"ERROR\"] = self.data[\"COH\"].apply(calculate_error)\n\n    def create_projection_matrix_and_error(self, row_of_data: pd.Series) -&gt; Tuple[Optional[np.ndarray], Optional[float]]:\n        \"\"\"\n        Creates a projection matrix and error from the provided row of data.\n\n        Args:\n            row_of_data (pd.Series): A row of SAR data.\n\n        Returns:\n            Tuple[Optional[np.ndarray], Optional[float]]: Projection matrix and error.\n        \"\"\"\n        if row_of_data is not None:\n            inc_ang = row_of_data[\"INC_ANG\"]\n            head_ang = row_of_data[\"HEAD_ANG\"]\n            error = row_of_data[\"ERROR\"]\n            if self.type == \"DInSAR\":\n                projection_matrix = np.array([[0, sin(inc_ang) * sin(head_ang), 0, -sin(inc_ang) * cos(head_ang), 0, cos(inc_ang)]])\n            else:\n                projection_matrix = np.array([[sin(inc_ang) * sin(head_ang), 0, -sin(inc_ang) * cos(head_ang), 0, cos(inc_ang), 0]])\n            error_matrix = error ** 2\n            return projection_matrix, error_matrix\n        return None, None\n\n    def get_observation(self, row_of_data: pd.Series) -&gt; Union[float, None]:\n        \"\"\"\n        Gets SAR observation from the provided row of data.\n\n        Args:\n            row_of_data (pd.Series): A row of SAR data.\n\n        Returns:\n            Union[float, None]: SAR observation.\n        \"\"\"\n        return row_of_data[\"DSP\"]\n\n    def expand_dataframe_by_date_range(self) -&gt; None:\n        \"\"\"\n        Expands the DataFrame by date range for \"DInSAR\" label.\n\n        For SAR data labeled as \"DInSAR\", this method adds a temporary column \"temp\" to the DataFrame\n        containing a date range between \"timestamp1\" and \"timestamp2\" based on the specified time interval.\n        The DataFrame is then exploded based on the \"temp\" column, duplicates are removed, and unnecessary\n        columns are dropped to create a new \"timestamp1\" index.\n\n        Note:\n        - This method is specifically designed for SAR data labeled as \"DInSAR\".\n        - It modifies the existing DataFrame in-place.\n\n        Example:\n        If the original DataFrame has a row with \"timestamp1\" and \"timestamp2\" as [\"2022-01-01\", \"2022-01-03\"],\n        and the time interval is set to \"1D\" (daily), the resulting DataFrame will have individual rows for\n        \"2022-01-01\", \"2022-01-02\", and \"2022-01-03\".\n        \"\"\"\n        if self.type == \"DInSAR\":\n            # Create a new column to store timestamps from the range of the given row\n            self.data[\"temp\"] = self.data.apply(\n                lambda row: pd.date_range(\n                    row.name[0],\n                    row.name[1] - timedelta(days=self.TIME_INTERVAL_NUM),\n                    freq=self.TIME_INTERVAL\n                ),\n                axis=1\n            )\n            # Count the difference in days for each row between the date range\n            self.data[\"day_diff\"] = (self.data.index.get_level_values(1) - self.data.index.get_level_values(0)).days\n            # Calculate rates of daily changes\n            self.data[\"DSP\"] = self.data[\"DSP\"] / self.data[\"day_diff\"]\n            # Extend DataFrame to a specific time interval and add NaN where data is missing\n            self.data = (\n                self.data.explode(\"temp\")\n                .reset_index()\n                .drop_duplicates(subset=\"temp\", keep=\"last\")\n                .drop(columns=[\"timestamp1\", \"timestamp2\", \"day_diff\"])\n                .rename(columns={\"temp\": \"timestamp1\"})\n                .set_index(\"timestamp1\")\n                .resample(self.TIME_INTERVAL)\n                .asfreq()\n            )\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.SARData.__init__","title":"<code>__init__(path)</code>","text":"<p>Initializes SARData object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the SAR data file.</p> required Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def __init__(self, path: str) -&gt; None:\n    \"\"\"\n    Initializes SARData object.\n\n    Args:\n        path (str): The path to the SAR data file.\n    \"\"\"\n    super().__init__(path)\n    self.bias_reduction = False\n    self.get_header_from_file()\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.SARData.coherence_to_error","title":"<code>coherence_to_error()</code>","text":"<p>Convert coherence column to error column in the data.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def coherence_to_error(self) -&gt; None:\n    \"\"\"\n    Convert coherence column to error column in the data.\n    \"\"\"\n    sentinel_wavelength = self.SENTINEL_WAVELENGTH\n\n    def calculate_error(coherence: float) -&gt; float:\n        \"\"\"\n        Calculate error from coherence.\n\n        Args:\n            coherence (float): Coherence value.\n\n        Returns:\n            float: Calculated error.\n        \"\"\"\n        li = 0\n        k = 1\n        while True:\n            li += (coherence ** (2 * k)) / (k ** 2)\n            li2 = li + (coherence ** (2 * k)) / (k ** 2)\n            if abs(li - li2) &lt;= 1e-10:\n                break\n            k += 1\n        phase = pi ** 2/3 - pi * asin(coherence) + asin(coherence) ** 2 - 0.5 * li2\n        return sqrt(phase) * sentinel_wavelength / (4 * pi)\n\n    self.data[\"ERROR\"] = self.data[\"COH\"].apply(calculate_error)\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.SARData.create_projection_matrix_and_error","title":"<code>create_projection_matrix_and_error(row_of_data)</code>","text":"<p>Creates a projection matrix and error from the provided row of data.</p> <p>Parameters:</p> Name Type Description Default <code>row_of_data</code> <code>Series</code> <p>A row of SAR data.</p> required <p>Returns:</p> Type Description <code>Tuple[Optional[ndarray], Optional[float]]</code> <p>Tuple[Optional[np.ndarray], Optional[float]]: Projection matrix and error.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def create_projection_matrix_and_error(self, row_of_data: pd.Series) -&gt; Tuple[Optional[np.ndarray], Optional[float]]:\n    \"\"\"\n    Creates a projection matrix and error from the provided row of data.\n\n    Args:\n        row_of_data (pd.Series): A row of SAR data.\n\n    Returns:\n        Tuple[Optional[np.ndarray], Optional[float]]: Projection matrix and error.\n    \"\"\"\n    if row_of_data is not None:\n        inc_ang = row_of_data[\"INC_ANG\"]\n        head_ang = row_of_data[\"HEAD_ANG\"]\n        error = row_of_data[\"ERROR\"]\n        if self.type == \"DInSAR\":\n            projection_matrix = np.array([[0, sin(inc_ang) * sin(head_ang), 0, -sin(inc_ang) * cos(head_ang), 0, cos(inc_ang)]])\n        else:\n            projection_matrix = np.array([[sin(inc_ang) * sin(head_ang), 0, -sin(inc_ang) * cos(head_ang), 0, cos(inc_ang), 0]])\n        error_matrix = error ** 2\n        return projection_matrix, error_matrix\n    return None, None\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.SARData.expand_dataframe_by_date_range","title":"<code>expand_dataframe_by_date_range()</code>","text":"<p>Expands the DataFrame by date range for \"DInSAR\" label.</p> <p>For SAR data labeled as \"DInSAR\", this method adds a temporary column \"temp\" to the DataFrame containing a date range between \"timestamp1\" and \"timestamp2\" based on the specified time interval. The DataFrame is then exploded based on the \"temp\" column, duplicates are removed, and unnecessary columns are dropped to create a new \"timestamp1\" index.</p> <p>Note: - This method is specifically designed for SAR data labeled as \"DInSAR\". - It modifies the existing DataFrame in-place.</p> <p>Example: If the original DataFrame has a row with \"timestamp1\" and \"timestamp2\" as [\"2022-01-01\", \"2022-01-03\"], and the time interval is set to \"1D\" (daily), the resulting DataFrame will have individual rows for \"2022-01-01\", \"2022-01-02\", and \"2022-01-03\".</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def expand_dataframe_by_date_range(self) -&gt; None:\n    \"\"\"\n    Expands the DataFrame by date range for \"DInSAR\" label.\n\n    For SAR data labeled as \"DInSAR\", this method adds a temporary column \"temp\" to the DataFrame\n    containing a date range between \"timestamp1\" and \"timestamp2\" based on the specified time interval.\n    The DataFrame is then exploded based on the \"temp\" column, duplicates are removed, and unnecessary\n    columns are dropped to create a new \"timestamp1\" index.\n\n    Note:\n    - This method is specifically designed for SAR data labeled as \"DInSAR\".\n    - It modifies the existing DataFrame in-place.\n\n    Example:\n    If the original DataFrame has a row with \"timestamp1\" and \"timestamp2\" as [\"2022-01-01\", \"2022-01-03\"],\n    and the time interval is set to \"1D\" (daily), the resulting DataFrame will have individual rows for\n    \"2022-01-01\", \"2022-01-02\", and \"2022-01-03\".\n    \"\"\"\n    if self.type == \"DInSAR\":\n        # Create a new column to store timestamps from the range of the given row\n        self.data[\"temp\"] = self.data.apply(\n            lambda row: pd.date_range(\n                row.name[0],\n                row.name[1] - timedelta(days=self.TIME_INTERVAL_NUM),\n                freq=self.TIME_INTERVAL\n            ),\n            axis=1\n        )\n        # Count the difference in days for each row between the date range\n        self.data[\"day_diff\"] = (self.data.index.get_level_values(1) - self.data.index.get_level_values(0)).days\n        # Calculate rates of daily changes\n        self.data[\"DSP\"] = self.data[\"DSP\"] / self.data[\"day_diff\"]\n        # Extend DataFrame to a specific time interval and add NaN where data is missing\n        self.data = (\n            self.data.explode(\"temp\")\n            .reset_index()\n            .drop_duplicates(subset=\"temp\", keep=\"last\")\n            .drop(columns=[\"timestamp1\", \"timestamp2\", \"day_diff\"])\n            .rename(columns={\"temp\": \"timestamp1\"})\n            .set_index(\"timestamp1\")\n            .resample(self.TIME_INTERVAL)\n            .asfreq()\n        )\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.SARData.get_observation","title":"<code>get_observation(row_of_data)</code>","text":"<p>Gets SAR observation from the provided row of data.</p> <p>Parameters:</p> Name Type Description Default <code>row_of_data</code> <code>Series</code> <p>A row of SAR data.</p> required <p>Returns:</p> Type Description <code>Union[float, None]</code> <p>Union[float, None]: SAR observation.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def get_observation(self, row_of_data: pd.Series) -&gt; Union[float, None]:\n    \"\"\"\n    Gets SAR observation from the provided row of data.\n\n    Args:\n        row_of_data (pd.Series): A row of SAR data.\n\n    Returns:\n        Union[float, None]: SAR observation.\n    \"\"\"\n    return row_of_data[\"DSP\"]\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.SARData.load_data","title":"<code>load_data()</code>","text":"<p>Loads SAR data from the specified file.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Loaded SAR data.</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def load_data(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads SAR data from the specified file.\n\n    Returns:\n        pd.DataFrame: Loaded SAR data.\n    \"\"\"\n    header = self.get_header_from_file()\n    self.data = self.load_csv_data(header)\n    self.replace_decimal_sep()\n    if \"COH\" in header:\n        self.coherence_to_error()\n    if self.type == \"DInSAR\":\n        self.convert_range_into_two_timestamps()\n    else:\n        self.process_timestamp_columns()\n    self.expand_dataframe_by_date_range()\n    return self.data\n</code></pre>"},{"location":"datainterface/#multidefusion.datainterface.SARData.reduce_bias_to_gnss","title":"<code>reduce_bias_to_gnss(date)</code>","text":"<p>Reduces bias in SAR data to GNSS data.</p> <p>This method reduces the bias in the SAR data by removing data points prior to the specified date and adjusting the 'DSP' values based on the first non-null value after the specified date.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>Timestamp</code> <p>The timestamp used as a reference point for bias reduction.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>multidefusion\\datainterface.py</code> <pre><code>def reduce_bias_to_gnss(self, date: pd.Timestamp):\n    \"\"\"\n    Reduces bias in SAR data to GNSS data.\n\n    This method reduces the bias in the SAR data by removing data points prior to\n    the specified date and adjusting the 'DSP' values based on the first non-null value\n    after the specified date.\n\n    Args:\n        date (pd.Timestamp): The timestamp used as a reference point for bias reduction.\n\n    Returns:\n        None\n    \"\"\"\n    self.data = self.data[self.data.index &gt; date]\n    first_non_null_value = self.data[\"DSP\"].first_valid_index()\n    if first_non_null_value is not None:\n        self.data[\"DSP\"] = self.data[\"DSP\"] - self.data.at[first_non_null_value, \"DSP\"]\n</code></pre>"},{"location":"fusion/","title":"fusion module","text":""},{"location":"fusion/#multidefusion.fusion.run_fusion","title":"<code>run_fusion(stations, path, method, noise)</code>","text":"<p>The library is based on geodetic observations stored in the form of text files. Maintenance of the specific structure for all input files is necessary to ensure the successful completion of the integration procedure. </p> <p>Important remarks: 1. The integration procedure can include a single station folder (e.g., stations = [\"ID01\"]) stored in the path, a list of stations (e.g., stations = [\"ID01\", \"ID02\", \"POINT_5\"]) or ALL of them (stations = \"ALL\"). 2. For each particular station's folder, it is necessary to store the geodetic data in the ASCII files (see Input). 3. Every ASCII file stored in the station's folder will be included in the integration procedure with respect to the chosen method (\"forward\" or \"forward-backward\"). 4. The noise level expressed as acceleration in mm/day^2^ should by assigned by user in the empirical way. 5. In the library, the zero-mean acceleration model is introduced as the system noise matrix (Teunissen, 2009).</p> <p>Teunissen, P. (2009). Dynamic data processing: Recursive least-squares.</p> <p>Parameters:</p> Name Type Description Default <code>stations</code> <code>list or str</code> <p>List of station names or \"ALL\" to process all stations found in the specified path.</p> required <code>path</code> <code>str</code> <p>Path to the directory containing station data.</p> required <code>method</code> <code>str</code> <p>Fusion method. Options are \"forward\" or \"forward-backward\".</p> required <code>noise</code> <code>float</code> <p>Noise level of the integration system [mm/day^2].</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid method is provided.</p> <p>Returns:</p> Name Type Description <code>integration_results</code> <code>dict</code> <p>DataIntegration objects</p> Source code in <code>multidefusion\\fusion.py</code> <pre><code>def run_fusion(stations, path, method, noise):\n    \"\"\"\n    The library is based on geodetic observations stored in the form of text files. Maintenance of the specific structure for all input files is necessary to ensure the successful completion of the integration procedure. \n\n    Important remarks:\n    1. The integration procedure can include a single station folder (e.g., stations = [\"ID01\"]) stored in the path, a list of stations (e.g., stations = [\"ID01\", \"ID02\", \"POINT_5\"]) or ALL of them (stations = \"ALL\").\n    2. For each particular station's folder, it is necessary to store the geodetic data in the ASCII files (see [Input](https://damiantondas.github.io/multidefusion/input/)).\n    3. Every ASCII file stored in the station's folder will be included in the integration procedure with respect to the chosen method (\"forward\" or \"forward-backward\").\n    4. The noise level expressed as acceleration in mm/day^2^ should by assigned by user in the empirical way.\n    5. In the library, the zero-mean acceleration model is introduced as the system noise matrix (Teunissen, 2009).\n\n    Teunissen, P. (2009). Dynamic data processing: Recursive least-squares.\n\n    Args:\n        stations (list or str): List of station names or \"ALL\" to process all stations found in the specified path.\n        path (str): Path to the directory containing station data.\n        method (str): Fusion method. Options are \"forward\" or \"forward-backward\".\n        noise (float): Noise level of the integration system [mm/day^2].\n\n    Raises:\n        ValueError: If an invalid method is provided.\n\n    Returns:\n        integration_results (dict): DataIntegration objects\n    \"\"\"\n    port = 8049\n    integration_results = {}\n    if stations == \"ALL\":\n        stations = [f.name for f in os.scandir(path) if f.is_dir()]\n    for station in stations:\n        print(f\"\\n___________________________________________________________\\nFusion started for station: {station}\\n\")\n        print(f\"Kalman {method} integration procedure in progress.\")\n        port +=1\n        integration = DataIntegration(station_name=station, path=path, noise=noise, port=port)\n        integration.connect_data()\n        try:\n            if method == \"forward\":\n                integration.kalman_forward()\n            elif method == \"forward-backward\":\n                integration.kalman_forward_backward() \n            else:\n                raise ValueError(f\"Invalid method '{method}'. Please enter 'forward' or 'forward-backward'.\")\n            integration.compute_mean_LOS_orbit()\n            integration_results[station] = integration\n\n            fig = Figures(integration)\n            fig.create_displacement_plot()\n\n        except ValueError as e:\n            print(e)\n\n    return integration_results\n</code></pre>"},{"location":"input/","title":"Input files structure","text":"<p>The library is based on geodetic observations stored in the form of text files stored in the defined folder. Maintenance of the specific structure for all input files is necessary to ensure the successful completion of the integration procedure.</p>"},{"location":"input/#the-structure-of-names-of-input-file","title":"The structure of names of input file:","text":"<p>The following files have to be located in the defined folder, the signature of which is given in the <code>stations</code> parameter of the <code>run_fusion</code> procedure (see Usage).</p> <ol> <li>GNSS: GNSS.txt (Mandatory file)</li> <li>InSAR: (e.g., \"DInSAR_Asc_1.txt\", \"DInSAR_Asc_2.txt\", \"SBAS_51_main.txt\", \"PSI_123.txt\", etc.).<ul> <li>Type_Orbit.txt or,</li> <li>Type_Orbit_Element.txt (Element is not mandatory):<ul> <li>Type is a mandatory signature of the InSAR calculation method. Allowed values: \"DInSAR\", \"SBAS\" or \"PSI\".</li> <li>Orbit is a mandatory signature of InSAR orbit. Allowed types: <code>str</code> or <code>int</code>, e.g., \"Asc\", \"Desc\", \"51\", \"123\", etc.</li> <li>Element is a non-mandatory signature of a particular pixel or PS point. Allowed types: <code>str</code> or <code>int</code>, e.g., \"1\", \"102fa\", \"main\", etc.</li> </ul> </li> </ul> </li> </ol>"},{"location":"input/#restrictions-on-the-input-files","title":"Restrictions on the input files:","text":"<ol> <li>The GNSS.txt is a mandatory file.</li> <li>The InSAR Type can be realised only by the \"DInSAR\", \"SBAS\" or \"PSI\" signature.</li> <li>The number of distinct InSAR Orbit signatures must be less than or equal to 3.</li> <li>The number of distinct InSAR Element signatures within particular Orbit must be less than or equal to 9.</li> <li>The number of distinct InSAR Element signatures within particular Type must be less than or equal to 10.</li> </ol>"},{"location":"input/#headers-and-data-columns-structure-in-the-input-files","title":"Headers and data columns structure in the input files:","text":"<ol> <li> <p>GNSS: <code>YYYY MM DD X Y Z mX mY mZ</code></p> Signature Type Parameter YYYY <code>int</code> Year MM <code>int</code> Month DD <code>int</code> Day X <code>float</code> Geocentric X coordinate [m] Y <code>float</code> Geocentric Y coordinate [m] Z <code>float</code> Geocentric Z coordinate [m] mX <code>float</code> Error of geocentric X coordinate [m] mY <code>float</code> Error of geocentric Y coordinate [m] mZ <code>float</code> Error of geocentric Z coordinate [m] </li> <li> <p>DInSAR: <code>YYYY1 MM1 DD1 YYYY2 MM2 DD2 DSP INC_ANG HEAD_ANG ERROR</code></p> Signature Type Parameter YYYY1 <code>int</code> Year of primary image YYYY2 <code>int</code> Year of secondary image MM1 <code>int</code> Month of primary image MM2 <code>int</code> Month of secondary image DD1 <code>int</code> Day of primary image DD2 <code>int</code> Day of secondary image DSP <code>float</code> LOS displacement [m] INC_ANG <code>float</code> Incidence angle [rad] HEAD_ANG <code>float</code> Heading angle [rad] ERROR <code>float</code> Error of LOS displacement [m] </li> <li> <p>SBAS &amp; PSI: <code>YYYY MM DD DSP INC_ANG HEAD_ANG ERROR</code></p> Signature Type Parameter YYYY <code>int</code> Year MM <code>int</code> Month DD <code>int</code> Day DSP <code>float</code> LOS displacement [m] INC_ANG <code>float</code> Incidence angle [rad] HEAD_ANG <code>float</code> Heading angle [rad] ERROR <code>float</code> Error of LOS displacement [m] <p>For all InSAR methods, the ERROR column can be replaced by: COH (<code>float</code>): Coherence factor. The coherence will be converted to the error value expressed in the metric domain using the theory presented in Hanssen, 2001 and Tonda\u015b et al., 2023. To calculate the error using the coherence factor, the wavelength of the Sentinel-1 is applied.</p> </li> </ol>"},{"location":"input/#to-get-the-trial-dataset-of-input-files-for-the-multidefusion-library-see-trial-packages","title":"To get the trial dataset of input files for the MultiDEFusion library, see Trial packages.","text":"<p>Hanssen, R. (2001). Radar interferometry: Data interpretation and error analysis. Tonda\u015b, D. et al. (2023). Kalman filter-based integration of GNSS and InSAR observations for local nonlinear strong deformations.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install the MultiDEFusion library, execute the following command in a terminal:</p> <pre><code>pip install multidefusion\n</code></pre> <p>If you do not have pip installed, you can find instructions to install pip here.</p>"},{"location":"installation/#python-requirements","title":"Python Requirements","text":"<p>This library is compatible with Python 3.8 and later versions.</p>"},{"location":"installation/#integrated-development-environments","title":"Integrated Development Environments","text":"<p>The library works in the Python environment. To run the integration procedure, the usage of one of the Python Integrated Development Environments (IDEs) is required. The current version of the library has been tested on four different IDEs: Spyder, PyCharm, Visual Studio Code, and Jupyter Notebook. Before starting the fusion process, ensure that your IDE is configured correctly.</p>"},{"location":"installation/#remarks","title":"Remarks","text":"<ol> <li>Within a particular IDE, ensure that the path of your Python interpreter is the same as that used in the terminal during installation. The example of changing the Python environment path in the Spyder can be found here.</li> <li>Using PyCharm, launch the script using Run File In Python Console. This setting can be defined as constant in Run \u2192 Edit Configurations \u2192 Run with Python console. The example can be found here.</li> <li>Using Visual Studio Code, launch the script using Run in Interactive Window. To make this option available, the installation of the Jupyter extension may be required. The example can be found here.</li> </ol>"},{"location":"integration/","title":"integration module","text":""},{"location":"integration/#multidefusion.integration.DataIntegration","title":"<code>DataIntegration</code>","text":"<p>Class for integrating and processing different types of data using the Kalman filter.</p> <p>Parameters:</p> Name Type Description Default <code>station_name</code> <code>str</code> <p>The name of the station.</p> required <code>path</code> <code>str</code> <p>The base path where data files are located.</p> required <code>noise</code> <code>float</code> <p>The noise parameter for the Kalman filter.</p> required <code>port</code> <code>int</code> <p>The port is necessary for parallel displaying of subsequent integration results on localhost.</p> required <p>Attributes:</p> Name Type Description <code>station</code> <code>str</code> <p>The name of the station.</p> <code>path</code> <code>str</code> <p>The base path where data files are located.</p> <code>data_dict</code> <code>Dict[str, Dict[str, Any]]</code> <p>A dictionary containing data objects organized by data type and label. More information about attributes can be found in the API Reference datainterface module section.</p> <code>mean_data_dict</code> <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary to store averaged data after integration is performed.</p> <code>predicted_state_and_variance</code> <code>Dict[date, Dict[str, Any]]</code> <p>Predicted state and variance for each date.</p> <code>forward</code> <code>Dict[date, Dict[str, Any]]</code> <p>Filtered state and variance for each date.</p> <code>backward</code> <code>Dict[date, Dict[str, Any]]</code> <p>Backward estimated state and variance for each date.</p> <code>latest_date_all_data</code> <code>Optional[date]</code> <p>The latest date among all data types.</p> <code>earliest_date_gnss</code> <code>Optional[date]</code> <p>The earliest date among GNSS data.</p> <code>noise</code> <code>float</code> <p>The noise parameter for the Kalman filter [mm/day^2 ].</p> <code>forward_df_xe</code> <code>DataFrame</code> <p>Converted data to pd.DataFrame from forward for xe keys.</p> <code>backward_df_xe</code> <code>DataFrame</code> <p>Converted data to pd.DataFrame from backward for xe keys.</p> Source code in <code>multidefusion\\integration.py</code> <pre><code>class DataIntegration:\n    \"\"\"\n    Class for integrating and processing different types of data using the Kalman filter.\n\n    Args:\n        station_name (str): The name of the station.\n        path (str): The base path where data files are located.\n        noise (float): The noise parameter for the Kalman filter.\n        port (int): The port is necessary for parallel displaying of subsequent integration results on localhost.\n\n    Attributes:\n        station (str): The name of the station.\n        path (str): The base path where data files are located.\n        data_dict (Dict[str, Dict[str, Any]]): A dictionary containing data objects organized by data type and label. More information about attributes can be found in the API Reference [datainterface module](https://damiantondas.github.io/multidefusion/datainterface/) section.\n        mean_data_dict (Dict[str, Dict[str, Any]]): Dictionary to store averaged data after integration is performed.\n        predicted_state_and_variance (Dict[datetime.date, Dict[str, Any]]): Predicted state and variance for each date.\n        forward (Dict[datetime.date, Dict[str, Any]]): Filtered state and variance for each date.\n        backward (Dict[datetime.date, Dict[str, Any]]): Backward estimated state and variance for each date.\n        latest_date_all_data (Optional[datetime.date]): The latest date among all data types.\n        earliest_date_gnss (Optional[datetime.date]): The earliest date among GNSS data.\n        noise (float): The noise parameter for the Kalman filter [mm/day^2 ].\n        forward_df_xe (pd.DataFrame): Converted data to pd.DataFrame from forward for xe keys.\n        backward_df_xe (pd.DataFrame): Converted data to pd.DataFrame from backward for xe keys.\n    \"\"\"\n\n    N = 6\n    TIME_INTERVAL_NUM = 1\n    TIME_INTERVAL = 'D'\n\n    def __init__(self, station_name: str, path: str, noise: float, port: int) -&gt; None:\n        self.station = station_name\n        self.path = os.path.normpath(path)\n        self.port = port\n        self.data_dict = {}\n        self.mean_data_dict = {}\n        self.predicted_state_and_variance = {}\n        self.forward = {}\n        self.backward = {}\n        self.latest_date_all_data = None\n        self.earliest_date_gnss = None\n        self.noise = noise/1000\n        self.forward_df_xe = None\n        self.backward_df_xe = None\n\n        self.xe = np.zeros((self.N, 1))\n        self.system_noise_matrix = self.noise ** 2 * np.kron(np.eye(int(self.N / 2)), np.array(\n            [[0.25 * self.TIME_INTERVAL_NUM ** 4, 0.5 * self.TIME_INTERVAL_NUM ** 3],\n             [0.5 * self.TIME_INTERVAL_NUM ** 3, self.TIME_INTERVAL_NUM ** 2]]))\n        self.transition_matrix = np.kron(np.eye(int(self.N / 2)), np.array([[1, self.TIME_INTERVAL_NUM], [0, 1]]))\n\n    @staticmethod\n    def extract_data_type(file_name: str) -&gt; str:\n        \"\"\"\n        Extracts data type from the file name.\n\n        Args:\n            file_name (str): The name of the data file.\n\n        Returns:\n            str: The extracted data type (\"GNSS\" or \"SAR\").\n        \"\"\"\n        if \"GNSS\" in file_name:\n            return \"GNSS\"\n        elif \"DInSAR\" in file_name or \"PSI\" in file_name or \"SBAS\" in file_name:\n            return \"SAR\"\n\n    @staticmethod\n    def create_data_object(data_type: str, file_path: str) -&gt; Any:\n        \"\"\"\n        Creates a data object based on the data type.\n\n        Args:\n            data_type (str): The type of data (\"GNSS\" or \"SAR\").\n            file_path (str): The path to the data file.\n\n        Returns:\n            Any: An instance of the corresponding data class.\n        \"\"\"\n        try:\n            if data_type == \"GNSS\":\n                return GNSSData(file_path)\n            elif data_type == \"SAR\":\n                return SARData(file_path)\n            # Add more conditions for other data types if needed\n            else:\n                raise ValueError(f\"Unsupported data type: {data_type}\")\n        except ZeroDivisionError:\n            sys.exit(1)\n\n    @staticmethod\n    def time_update(xe: np.ndarray, Pe: np.ndarray, Phi: np.ndarray, S: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Time update step of the Kalman filter.\n\n        Args:\n            xe (np.ndarray): Predicted state.\n            Pe (np.ndarray): Variance matrix.\n            Phi (np.ndarray): Transition matrix.\n            S (np.ndarray): Process noise matrix.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Updated predicted state and variance.\n        \"\"\"\n        # number of states\n        n = xe.shape[0]\n\n        # check sizes of input\n        assert xe.shape[1] == 1, \"predicted state must be a column vector\"\n        assert Pe.shape == (n, n), \"variance matrix must be n-by-n\"\n        assert Phi.shape == (n, n), \"transition matrix must be n-by-n\"\n        assert S.shape == (n, n), \"process noise matrix must be n-by-n\"\n\n        # time update\n        xp = np.dot(Phi, xe)\n        Pp = np.dot(np.dot(Phi, Pe), np.transpose(Phi)) + S\n\n        return xp, Pp\n\n    @staticmethod\n    def measurement_update(xp: np.ndarray, Pp: np.ndarray, z: np.ndarray, R: np.ndarray, A: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Measurement update step of the Kalman filter.\n\n        Args:\n            xp (np.ndarray): Predicted state.\n            Pp (np.ndarray): Variance matrix.\n            z (np.ndarray): Measurement vector.\n            R (np.ndarray): Measurement variance matrix.\n            A (np.ndarray): Design matrix.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n            Updated estimated state, variance, measurement residual, measurement variance, and Kalman gain.\n        \"\"\"\n        # number of states\n        n = xp.shape[0]\n        # number of measurements\n        m = z.shape[0]\n\n        # check sizes of input\n        assert xp.shape[1] == 1, \"predicted state must be a column vector\"\n        assert Pp.shape == (n, n), \"variance matrix must be n-by-n\"\n        assert z.shape[1] == 1, \"measurements must be a column vector\"\n        assert R.shape == (m, m), \"variance matrix must be m-by-m\"\n        assert A.shape == (m, n), \"design matrix must be m-by-n\"\n\n        # measurement update\n        v = z - np.dot(A, xp)\n        Qv = R + np.dot(np.dot(A, Pp), np.transpose(A))\n        K = np.dot(np.dot(Pp, np.transpose(A)), np.linalg.inv(Qv))\n        xe = xp + np.dot(K, v)\n        Pe = np.dot((np.eye(n) - np.dot(K, A)), Pp)\n\n        return xe, Pe, v, Qv, K\n\n    def remove_bias_in_sar_data(self) -&gt; None:\n        \"\"\"\n        Removes bias in SAR data based on GNSS data.\n\n        This method iterates through the SAR data stored in the 'data_dict' attribute and\n        applies bias reduction to each dataset, excluding the 'DInSAR' technique. The bias\n        reduction is performed using GNSS data up to the earliest date specified by\n        'earliest_date_gnss'.\n\n        Note:\n        - Bias reduction is applied to each dataset within the SAR data structure.\n\n        Returns:\n        None\n        \"\"\"\n        if self.data_dict.get(\"SAR\") is not None:\n            for technique, data in self.data_dict.get(\"SAR\").items():\n                if technique != \"DInSAR\":\n                    if isinstance(data, dict):\n                        for orbit, orbit_data in data.items():\n                            if isinstance(orbit_data, dict):\n                                for _, (subkey, subdata) in enumerate(orbit_data.items()):\n                                    subdata.reduce_bias_to_gnss(date=self.earliest_date_gnss)\n                            else:\n                                orbit_data.reduce_bias_to_gnss(date=self.earliest_date_gnss)\n\n    def connect_data(self) -&gt; None:\n        \"\"\"\n        Connects data objects based on data types and labels.\n        \"\"\"\n\n        file_list = sorted(os.listdir(os.path.join(self.path, self.station)), key=self.custom_sort)\n        try:\n            if 'GNSS.txt' not in file_list:\n                raise ValueError(f\"The GNSS.txt file is missing in the {os.path.join(self.path, self.station)} folder.\")\n        except ZeroDivisionError:\n            sys.exit(1)\n\n        data = {}\n        orb_elements = {}\n        for file_name in file_list:\n            file_path = os.path.join(self.path, self.station, file_name)\n            data_type = self.extract_data_type(file_name)\n            keys = file_name.replace(\".txt\", '').split(\"_\")\n            try:\n                if len(keys) &gt; 3:\n                    raise ValueError(f\"To many separators \\'_\\' in the {file_name} file name.\")\n                elif len(keys) == 3 and len(keys[2]) &gt; 10:\n                    raise ValueError(f\"The sub-name {keys[2]} for is too long and because it exceeds 10 characters.\")\n            except ZeroDivisionError:\n                sys.exit(1)\n\n            if data_type is not None:\n                data_obj = self.create_data_object(data_type, file_path)\n                current_dict = data\n                if len(keys) &gt; 1:\n                    orb_elements.setdefault(keys[0], {}).setdefault(keys[1], [])\n                    if len(keys) &gt; 2:\n                        orb_elements[keys[0]][keys[1]].append(keys[2])\n                    else:\n                        orb_elements[keys[0]][keys[1]].append('')\n\n                for key in keys[:-1]:\n                    if key in [\"PSI\", \"SBAS\", \"DInSAR\"]:\n                        current_dict = current_dict.setdefault(\"SAR\", {}).setdefault(key, {})\n                    else:\n                        current_dict = current_dict.setdefault(key, {})\n                current_dict[keys[-1]] = data_obj\n\n        uniq_values = {'GNSS': []}\n        if keys != ['GNSS']:\n            uniq_orbits = set()\n            for mainkey, subdict in orb_elements.items():\n                uniq_values[mainkey] = []\n                for subkey, elements in subdict.items():\n                    uniq_values[mainkey].extend(elements)\n                    uniq_values[mainkey] = list(set(uniq_values[mainkey]))\n\n                    uniq_orbits.add(subkey)\n\n                    try:\n                        if '' in elements:\n                            if len(elements) &gt; 1:\n                                raise ValueError(f\"One of the {mainkey} element for the {subkey} orbit does not have a sub-name determined.\")\n                            else:\n                                uniq_values[mainkey].remove('')\n                        elif len(elements) &gt; 9: \n                            raise ValueError(f\"The maximum allowed number of elements for the InSAR {subkey} orbit and {mainkey} calculation method is exceeded ({len(elements)} elements). The maximum acceptable number of elements is 9.\")\n\n                        if len(uniq_values[mainkey]) &gt; 10: \n                            raise ValueError(f\"The maximum allowed number of sub-names for the {mainkey} InSAR calculation method is exceeded (len(uniq_values[mainkey]) sub-names). The maximum acceptable number of sub-names is 10.\")\n                        elif len(uniq_orbits) &gt; 3:\n                             raise ValueError(f\"The maximum allowed number of the InSAR orbits is exceeded ({len(uniq_orbits)} orbits). The maximum acceptable number of orbits is 3.\")\n                    except ZeroDivisionError:\n                        sys.exit(1)\n\n        self.number_of_SAR_elements = uniq_values      \n        self.data_dict = data\n        self.get_latest_date_from_all_data()\n        self.get_earliest_gnss_date()\n        self.remove_bias_in_sar_data()\n        self.sar_data_validation()\n\n    def custom_sort(self, item):\n        parts = item.replace(\".txt\", '').split(\"_\")\n        prefix = parts[0][0]\n\n        suffix  = False\n        suffix2 = False\n        if len(parts)==1:\n            number=False\n        else:\n            number=parts[1]\n\n        if len(parts)&lt;=2:\n            suffix = False\n        else:\n            suffix = parts[2][0].isdigit()\n\n            if suffix is True:\n                suffix2 = parts[2][0]\n\n        return (prefix, number, suffix, suffix2)\n\n    def sar_data_validation(self):\n        \"\"\"\n        Validates SAR data and applies bias reduction if necessary.\n\n        This method iterates through SAR data obtained from different techniques (PSI and SBAS),\n        checks if the oldest date in the data is greater than the earliest GNSS date, and applies\n        bias reduction if the condition is met.\n\n        Returns:\n            None\n        \"\"\"\n        for data in self.get_data(\"SAR\", \"PSI\"):\n            if data.oldest_date &gt; self.earliest_date_gnss:\n                data.bias_reduction = True\n        for data in self.get_data(\"SAR\", \"SBAS\"):\n                 data.bias_reduction = True\n        for data in self.get_data(\"SAR\", \"DInSAR\"):\n            if data.oldest_date &lt; self.earliest_date_gnss:\n                data.data = data.data[data.data.index &gt;= self.earliest_date_gnss]\n\n    def get_data(self, technique, type):\n        \"\"\"\n        Retrieves and flattens data for a specified SAR technique and type.\n\n        Args:\n            technique (str): SAR technique (e.g., \"SAR\").\n            type (str): Data type (e.g., \"PSI\").\n\n        Returns:\n            List: Flattened list of data values for the specified technique and type.\n        \"\"\"\n        result_list = []\n        data = self.data_dict.get(technique, {}).get(type, {})\n        for key, values in data.items():\n            if isinstance(values, dict):\n                for subkey, subvalues in values.items():\n                    result_list.append((subvalues))\n            else:\n                result_list.append((values))\n        return result_list\n\n    def kalman_based_bias_removal(self, data_obj, date):\n        \"\"\"\n        Applies Kalman-based bias removal to SAR data.\n\n        This method takes a data object and a date, extracts relevant information, performs\n        Kalman-based bias removal, and updates the data object with the corrected values.\n\n        Args:\n            data_obj (DataObject): Object containing SAR data.\n            date (datetime): Date for bias removal.\n\n        Returns:\n            None\n        \"\"\"\n        start_col = 1 if data_obj.type == \"DInSAR\" else 0\n        rate = 1000 if data_obj.type == \"DInSAR\" else 1\n        head_ang_mean = data_obj.data['HEAD_ANG'].mean()\n        inc_ang_mean = data_obj.data['INC_ANG'].mean()\n        mat = np.array(\n            [np.sin(inc_ang_mean) * np.sin(head_ang_mean), - np.sin(inc_ang_mean) * np.cos(head_ang_mean),\n             np.cos(inc_ang_mean)]).reshape(1, -1)\n        df = self.forward[date - datetime.timedelta(days=1)]['xe'][start_col::2] * rate\n        LOS = np.dot(mat, df).T\n        data_obj.data['DSP'] = data_obj.data['DSP'] + LOS[0]\n        data_obj.bias_reduction = False\n\n    def get_latest_date_from_all_data(self):\n        \"\"\"\n        Updates the latest date among all data types.\n        \"\"\"\n        all_dates = []\n        if isinstance(self.data_dict, dict):\n            for value in self.data_dict.values():\n                if isinstance(value, dict):\n                    for value2 in value.values():\n                        if isinstance(value2, dict):\n                            for value3 in value2.values():\n                                if isinstance(value3, dict):\n                                    for value4 in value3.values():\n                                        all_dates.append(value4.latest_date)\n                                else:\n                                    all_dates.append(value3.latest_date)\n                else:\n                    all_dates.append(value.latest_date)\n        self.latest_date_all_data = max(all_dates, default=None)\n\n    def get_earliest_gnss_date(self):\n        \"\"\"\n        Updates the earliest date among GNSS data.\n        \"\"\"\n        gnss_dates = self.data_dict.get(\"GNSS\").oldest_date\n        self.earliest_date_gnss = gnss_dates\n\n    def _process_data_obj(self, data_obj, date, projection_matrix_list, error_matrix_list, obs_vector_list):\n        \"\"\"Process data from the given data object for a specific date.\n\n           Args:\n               data_obj: An object containing data to be processed.\n               date: The specific date for which data is to be processed.\n               projection_matrix_list: A list to store projection matrices generated during processing.\n               error_matrix_list: A list to store error matrices generated during processing.\n               obs_vector_list: A list to store observation vectors generated during processing.\n\n           Returns:\n               None: This function doesn't return anything; it appends processed data to the provided lists.\n        \"\"\"\n        row_data_by_date = data_obj.get_data_by_date(date)\n        if row_data_by_date is not None:\n            if isinstance(data_obj, SARData) and data_obj.bias_reduction:\n                self.kalman_based_bias_removal(data_obj, date)\n            projection_matrix, error_matrix = data_obj.create_projection_matrix_and_error(row_data_by_date)\n            values = data_obj.get_observation(row_data_by_date)\n\n            projection_matrix_list.append(projection_matrix)\n            error_matrix_list.append(error_matrix)\n            obs_vector_list.append(values)\n\n    def kalman_forward(self):\n        \"\"\"\n        Performs the forward pass of the Kalman filter.\n        \"\"\"\n        date_range = pd.date_range(start=self.earliest_date_gnss, end=self.latest_date_all_data, freq=self.TIME_INTERVAL)\n\n        xe = self.xe\n        Pe = self.system_noise_matrix\n        F = self.transition_matrix\n        Q = self.system_noise_matrix\n\n        for date in date_range:\n            obs_vector_list = []\n            projection_matrix_list = []\n            error_matrix_list = []\n            for data_type, data_dict in self.data_dict.items():\n                if not isinstance(data_dict, dict):\n                    self._process_data_obj(data_dict, date, projection_matrix_list, error_matrix_list, obs_vector_list)\n                else:\n                    for technique, data in data_dict.items():\n                        for orbit, orbit_data in data.items():\n                            if not isinstance(orbit_data, dict):\n                                self._process_data_obj(orbit_data, date, projection_matrix_list, error_matrix_list, obs_vector_list)\n                            else:\n                                for subkey, subdata in orbit_data.items():\n                                    self._process_data_obj(subdata, date, projection_matrix_list, error_matrix_list, obs_vector_list)\n            if len(projection_matrix_list) &gt; 0:\n                projection_matrix = np.vstack(projection_matrix_list)\n                error_matrix = block_diag(*error_matrix_list)\n                obs_vector = np.hstack(obs_vector_list).reshape(-1, 1)\n\n            # time update (predict)\n            xp, Pp = self.time_update(xe, Pe, F, Q)\n            self.predicted_state_and_variance[date] = {\"xp\": xp, \"Pp\": Pp}\n\n            # measurement update (estimate)\n            xe, Pe, v, Qv, K = self.measurement_update(xp, Pp, obs_vector, error_matrix, projection_matrix)\n            self.forward[date] = {\"xe\": xe, \"Pe\": Pe}\n        self.forward_df_xe = pd.DataFrame(\n            [(timestamp, *values[\"xe\"]) for timestamp, values in self.forward.items()],\n            columns=[\"timestamp\", \"N\", \"vN\", \"E\", \"vE\", \"U\", \"vU\"]).set_index(\"timestamp\").astype(float)\n\n    def kalman_forward_backward(self):\n        \"\"\"\n        Performs the forward-backward pass of the Kalman filter.\n        \"\"\"\n        if not self.forward or not self.predicted_state_and_variance:\n            self.kalman_forward()\n        date_range = pd.date_range(start=self.earliest_date_gnss, end=self.latest_date_all_data, freq=self.TIME_INTERVAL)\n\n        F = self.transition_matrix\n        xe_b = self.forward.get(date_range[-1])[\"xe\"]\n        Pe_b = self.forward.get(date_range[-1])[\"Pe\"]\n\n        for date in reversed(date_range[:-1]):\n            next_day = date + datetime.timedelta(days=self.TIME_INTERVAL_NUM)\n\n            L = np.dot(np.dot(self.forward.get(date)[\"Pe\"], np.transpose(F)), np.linalg.inv(self.predicted_state_and_variance.get(next_day)[\"Pp\"]))\n            xe_b = self.forward.get(date)[\"xe\"] + np.dot(L, (xe_b - self.predicted_state_and_variance.get(next_day)[\"xp\"]))\n            Pe_b = self.forward.get(date)[\"Pe\"] + np.dot(np.dot(L, (Pe_b - self.predicted_state_and_variance.get(next_day)[\"Pp\"])), np.transpose(L))\n            self.backward[date] = {\"xe_b\": xe_b, \"Pe_b\": Pe_b}\n        self.backward_df_xe = pd.DataFrame(\n            [(timestamp, *values[\"xe_b\"]) for timestamp, values in self.backward.items()],\n            columns=[\"timestamp\", \"N\", \"vN\", \"E\", \"vE\", \"U\", \"vU\"]).set_index(\"timestamp\").astype(float)\n\n    @staticmethod\n    def _process_technique_data(data, container_head, container_inc):\n        \"\"\"\n        Process HEAD_ANG and INC_ANG data for a given technique and populates the provided containers.\n\n        Args:\n        data (dict): Technique data containing orbit-wise information.\n        container_head (dict): Dictionary to store HEAD_ANG data for each orbit.\n        container_inc (dict): Dictionary to store INC_ANG data for each orbit.\n\n        Returns:\n        None\n        \"\"\"\n        for orbit, orbit_data in data.items():\n            container_head.setdefault(orbit, [])\n            container_inc.setdefault(orbit, [])\n            if isinstance(orbit_data, dict):\n                for subkey, subdata in orbit_data.items():\n                    container_head[orbit].append(subdata.data[\"HEAD_ANG\"])\n                    container_inc[orbit].append(subdata.data[\"INC_ANG\"])\n            else:\n                container_head[orbit].append(orbit_data.data[\"HEAD_ANG\"])\n                container_inc[orbit].append(orbit_data.data[\"INC_ANG\"])\n\n    def _process_for_orbit(self, head, inc, cols, name, rate):\n        \"\"\"\n        Processes HEAD_ANG and INC_ANG data for a specific orbit, computes the mean Line-of-Sight (LOS),\n        and updates the mean_data_dict with forward and backward mean values.\n\n        Args:\n        head (dict): Dictionary containing HEAD_ANG data for each orbit.\n        inc (dict): Dictionary containing INC_ANG data for each orbit.\n        cols (list): List of column names for data extraction.\n        name (str): Name to be assigned in the mean_data_dict.\n        rate (int): Rate factor to be applied during computation.\n\n        Returns:\n        None\n        \"\"\"\n        all_orbits = set(self.data_dict.get(\"SAR\", {}).get(\"PSI\", {}).keys()) | set(\n            self.data_dict.get(\"SAR\", {}).get(\"SBAS\", {}).keys()) | set(\n            self.data_dict.get(\"SAR\", {}).get(\"DInSAR\", {}).keys())\n\n        for orbit in all_orbits:\n            head_data = head.get(orbit, None)\n            inc_data = inc.get(orbit, None)\n            if head_data is not None and inc_data is not None:\n                head_ang_mean = pd.concat(head_data, ignore_index=True).mean()\n                inc_ang_mean = pd.concat(inc_data, ignore_index=True).mean()\n\n                mat = np.array(\n                    [np.sin(inc_ang_mean) * np.sin(head_ang_mean),\n                     - np.sin(inc_ang_mean) * np.cos(head_ang_mean),\n                     np.cos(inc_ang_mean)\n                     ]\n                ).reshape(1, -1)\n\n                df = self.forward_df_xe\n                df = df[cols] * rate\n                LOS = np.dot(mat, df.values.T).T\n                LOS = pd.DataFrame(LOS, index=self.forward_df_xe.index)\n                self.mean_data_dict.setdefault(name, {}).setdefault(orbit, {})[\"forward_mean\"] = LOS[0]\n\n                if self.backward_df_xe is not None:\n                    df = self.backward_df_xe\n                    df = df[cols] * rate\n                    LOS = np.dot(mat, df.values.T).T\n                    LOS = pd.DataFrame(LOS, index=self.backward_df_xe.index)\n                    self.mean_data_dict.setdefault(name, {}).setdefault(orbit, {})[\"backward_mean\"] = LOS[0]\n\n    def compute_mean_LOS_orbit(self):\n        \"\"\"\n        Computes the mean Line-of-Sight (LOS) orbit for SAR data, including PSI, SBAS, and DInSAR techniques.\n        Populates container_head_psi_sbas, container_inc_psi_sbas, container_head_DInSAR, and container_inc_DInSAR\n        with HEAD_ANG and INC_ANG data for each orbit.\n\n        Returns:\n        None\n        \"\"\"\n        container_head_psi_sbas, container_inc_psi_sbas = {}, {}\n        container_head_DInSAR, container_inc_DInSAR = {}, {}\n        if \"SAR\" in self.data_dict and self.data_dict.get(\"SAR\") is not None:\n            for technique, data in self.data_dict[\"SAR\"].items():\n                if technique == \"DInSAR\":\n                    self._process_technique_data(data, container_head_DInSAR, container_inc_DInSAR)\n                elif technique == \"PSI\" or technique == \"SBAS\":\n                    self._process_technique_data(data, container_head_psi_sbas, container_inc_psi_sbas)\n            self._process_for_orbit(container_head_psi_sbas, container_inc_psi_sbas, [\"N\", \"E\", \"U\"],\n                                    \"SAR_SBAS_PSI_MEAN\", 1)\n            self._process_for_orbit(container_head_DInSAR, container_inc_DInSAR, [\"vN\", \"vE\", \"vU\"], \"DInSAR\", 1000)\n</code></pre>"},{"location":"integration/#multidefusion.integration.DataIntegration.compute_mean_LOS_orbit","title":"<code>compute_mean_LOS_orbit()</code>","text":"<p>Computes the mean Line-of-Sight (LOS) orbit for SAR data, including PSI, SBAS, and DInSAR techniques. Populates container_head_psi_sbas, container_inc_psi_sbas, container_head_DInSAR, and container_inc_DInSAR with HEAD_ANG and INC_ANG data for each orbit.</p> <p>Returns: None</p> Source code in <code>multidefusion\\integration.py</code> <pre><code>def compute_mean_LOS_orbit(self):\n    \"\"\"\n    Computes the mean Line-of-Sight (LOS) orbit for SAR data, including PSI, SBAS, and DInSAR techniques.\n    Populates container_head_psi_sbas, container_inc_psi_sbas, container_head_DInSAR, and container_inc_DInSAR\n    with HEAD_ANG and INC_ANG data for each orbit.\n\n    Returns:\n    None\n    \"\"\"\n    container_head_psi_sbas, container_inc_psi_sbas = {}, {}\n    container_head_DInSAR, container_inc_DInSAR = {}, {}\n    if \"SAR\" in self.data_dict and self.data_dict.get(\"SAR\") is not None:\n        for technique, data in self.data_dict[\"SAR\"].items():\n            if technique == \"DInSAR\":\n                self._process_technique_data(data, container_head_DInSAR, container_inc_DInSAR)\n            elif technique == \"PSI\" or technique == \"SBAS\":\n                self._process_technique_data(data, container_head_psi_sbas, container_inc_psi_sbas)\n        self._process_for_orbit(container_head_psi_sbas, container_inc_psi_sbas, [\"N\", \"E\", \"U\"],\n                                \"SAR_SBAS_PSI_MEAN\", 1)\n        self._process_for_orbit(container_head_DInSAR, container_inc_DInSAR, [\"vN\", \"vE\", \"vU\"], \"DInSAR\", 1000)\n</code></pre>"},{"location":"integration/#multidefusion.integration.DataIntegration.connect_data","title":"<code>connect_data()</code>","text":"<p>Connects data objects based on data types and labels.</p> Source code in <code>multidefusion\\integration.py</code> <pre><code>def connect_data(self) -&gt; None:\n    \"\"\"\n    Connects data objects based on data types and labels.\n    \"\"\"\n\n    file_list = sorted(os.listdir(os.path.join(self.path, self.station)), key=self.custom_sort)\n    try:\n        if 'GNSS.txt' not in file_list:\n            raise ValueError(f\"The GNSS.txt file is missing in the {os.path.join(self.path, self.station)} folder.\")\n    except ZeroDivisionError:\n        sys.exit(1)\n\n    data = {}\n    orb_elements = {}\n    for file_name in file_list:\n        file_path = os.path.join(self.path, self.station, file_name)\n        data_type = self.extract_data_type(file_name)\n        keys = file_name.replace(\".txt\", '').split(\"_\")\n        try:\n            if len(keys) &gt; 3:\n                raise ValueError(f\"To many separators \\'_\\' in the {file_name} file name.\")\n            elif len(keys) == 3 and len(keys[2]) &gt; 10:\n                raise ValueError(f\"The sub-name {keys[2]} for is too long and because it exceeds 10 characters.\")\n        except ZeroDivisionError:\n            sys.exit(1)\n\n        if data_type is not None:\n            data_obj = self.create_data_object(data_type, file_path)\n            current_dict = data\n            if len(keys) &gt; 1:\n                orb_elements.setdefault(keys[0], {}).setdefault(keys[1], [])\n                if len(keys) &gt; 2:\n                    orb_elements[keys[0]][keys[1]].append(keys[2])\n                else:\n                    orb_elements[keys[0]][keys[1]].append('')\n\n            for key in keys[:-1]:\n                if key in [\"PSI\", \"SBAS\", \"DInSAR\"]:\n                    current_dict = current_dict.setdefault(\"SAR\", {}).setdefault(key, {})\n                else:\n                    current_dict = current_dict.setdefault(key, {})\n            current_dict[keys[-1]] = data_obj\n\n    uniq_values = {'GNSS': []}\n    if keys != ['GNSS']:\n        uniq_orbits = set()\n        for mainkey, subdict in orb_elements.items():\n            uniq_values[mainkey] = []\n            for subkey, elements in subdict.items():\n                uniq_values[mainkey].extend(elements)\n                uniq_values[mainkey] = list(set(uniq_values[mainkey]))\n\n                uniq_orbits.add(subkey)\n\n                try:\n                    if '' in elements:\n                        if len(elements) &gt; 1:\n                            raise ValueError(f\"One of the {mainkey} element for the {subkey} orbit does not have a sub-name determined.\")\n                        else:\n                            uniq_values[mainkey].remove('')\n                    elif len(elements) &gt; 9: \n                        raise ValueError(f\"The maximum allowed number of elements for the InSAR {subkey} orbit and {mainkey} calculation method is exceeded ({len(elements)} elements). The maximum acceptable number of elements is 9.\")\n\n                    if len(uniq_values[mainkey]) &gt; 10: \n                        raise ValueError(f\"The maximum allowed number of sub-names for the {mainkey} InSAR calculation method is exceeded (len(uniq_values[mainkey]) sub-names). The maximum acceptable number of sub-names is 10.\")\n                    elif len(uniq_orbits) &gt; 3:\n                         raise ValueError(f\"The maximum allowed number of the InSAR orbits is exceeded ({len(uniq_orbits)} orbits). The maximum acceptable number of orbits is 3.\")\n                except ZeroDivisionError:\n                    sys.exit(1)\n\n    self.number_of_SAR_elements = uniq_values      \n    self.data_dict = data\n    self.get_latest_date_from_all_data()\n    self.get_earliest_gnss_date()\n    self.remove_bias_in_sar_data()\n    self.sar_data_validation()\n</code></pre>"},{"location":"integration/#multidefusion.integration.DataIntegration.create_data_object","title":"<code>create_data_object(data_type, file_path)</code>  <code>staticmethod</code>","text":"<p>Creates a data object based on the data type.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>The type of data (\"GNSS\" or \"SAR\").</p> required <code>file_path</code> <code>str</code> <p>The path to the data file.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>An instance of the corresponding data class.</p> Source code in <code>multidefusion\\integration.py</code> <pre><code>@staticmethod\ndef create_data_object(data_type: str, file_path: str) -&gt; Any:\n    \"\"\"\n    Creates a data object based on the data type.\n\n    Args:\n        data_type (str): The type of data (\"GNSS\" or \"SAR\").\n        file_path (str): The path to the data file.\n\n    Returns:\n        Any: An instance of the corresponding data class.\n    \"\"\"\n    try:\n        if data_type == \"GNSS\":\n            return GNSSData(file_path)\n        elif data_type == \"SAR\":\n            return SARData(file_path)\n        # Add more conditions for other data types if needed\n        else:\n            raise ValueError(f\"Unsupported data type: {data_type}\")\n    except ZeroDivisionError:\n        sys.exit(1)\n</code></pre>"},{"location":"integration/#multidefusion.integration.DataIntegration.extract_data_type","title":"<code>extract_data_type(file_name)</code>  <code>staticmethod</code>","text":"<p>Extracts data type from the file name.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the data file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The extracted data type (\"GNSS\" or \"SAR\").</p> Source code in <code>multidefusion\\integration.py</code> <pre><code>@staticmethod\ndef extract_data_type(file_name: str) -&gt; str:\n    \"\"\"\n    Extracts data type from the file name.\n\n    Args:\n        file_name (str): The name of the data file.\n\n    Returns:\n        str: The extracted data type (\"GNSS\" or \"SAR\").\n    \"\"\"\n    if \"GNSS\" in file_name:\n        return \"GNSS\"\n    elif \"DInSAR\" in file_name or \"PSI\" in file_name or \"SBAS\" in file_name:\n        return \"SAR\"\n</code></pre>"},{"location":"integration/#multidefusion.integration.DataIntegration.get_data","title":"<code>get_data(technique, type)</code>","text":"<p>Retrieves and flattens data for a specified SAR technique and type.</p> <p>Parameters:</p> Name Type Description Default <code>technique</code> <code>str</code> <p>SAR technique (e.g., \"SAR\").</p> required <code>type</code> <code>str</code> <p>Data type (e.g., \"PSI\").</p> required <p>Returns:</p> Name Type Description <code>List</code> <p>Flattened list of data values for the specified technique and type.</p> Source code in <code>multidefusion\\integration.py</code> <pre><code>def get_data(self, technique, type):\n    \"\"\"\n    Retrieves and flattens data for a specified SAR technique and type.\n\n    Args:\n        technique (str): SAR technique (e.g., \"SAR\").\n        type (str): Data type (e.g., \"PSI\").\n\n    Returns:\n        List: Flattened list of data values for the specified technique and type.\n    \"\"\"\n    result_list = []\n    data = self.data_dict.get(technique, {}).get(type, {})\n    for key, values in data.items():\n        if isinstance(values, dict):\n            for subkey, subvalues in values.items():\n                result_list.append((subvalues))\n        else:\n            result_list.append((values))\n    return result_list\n</code></pre>"},{"location":"integration/#multidefusion.integration.DataIntegration.get_earliest_gnss_date","title":"<code>get_earliest_gnss_date()</code>","text":"<p>Updates the earliest date among GNSS data.</p> Source code in <code>multidefusion\\integration.py</code> <pre><code>def get_earliest_gnss_date(self):\n    \"\"\"\n    Updates the earliest date among GNSS data.\n    \"\"\"\n    gnss_dates = self.data_dict.get(\"GNSS\").oldest_date\n    self.earliest_date_gnss = gnss_dates\n</code></pre>"},{"location":"integration/#multidefusion.integration.DataIntegration.get_latest_date_from_all_data","title":"<code>get_latest_date_from_all_data()</code>","text":"<p>Updates the latest date among all data types.</p> Source code in <code>multidefusion\\integration.py</code> <pre><code>def get_latest_date_from_all_data(self):\n    \"\"\"\n    Updates the latest date among all data types.\n    \"\"\"\n    all_dates = []\n    if isinstance(self.data_dict, dict):\n        for value in self.data_dict.values():\n            if isinstance(value, dict):\n                for value2 in value.values():\n                    if isinstance(value2, dict):\n                        for value3 in value2.values():\n                            if isinstance(value3, dict):\n                                for value4 in value3.values():\n                                    all_dates.append(value4.latest_date)\n                            else:\n                                all_dates.append(value3.latest_date)\n            else:\n                all_dates.append(value.latest_date)\n    self.latest_date_all_data = max(all_dates, default=None)\n</code></pre>"},{"location":"integration/#multidefusion.integration.DataIntegration.kalman_based_bias_removal","title":"<code>kalman_based_bias_removal(data_obj, date)</code>","text":"<p>Applies Kalman-based bias removal to SAR data.</p> <p>This method takes a data object and a date, extracts relevant information, performs Kalman-based bias removal, and updates the data object with the corrected values.</p> <p>Parameters:</p> Name Type Description Default <code>data_obj</code> <code>DataObject</code> <p>Object containing SAR data.</p> required <code>date</code> <code>datetime</code> <p>Date for bias removal.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>multidefusion\\integration.py</code> <pre><code>def kalman_based_bias_removal(self, data_obj, date):\n    \"\"\"\n    Applies Kalman-based bias removal to SAR data.\n\n    This method takes a data object and a date, extracts relevant information, performs\n    Kalman-based bias removal, and updates the data object with the corrected values.\n\n    Args:\n        data_obj (DataObject): Object containing SAR data.\n        date (datetime): Date for bias removal.\n\n    Returns:\n        None\n    \"\"\"\n    start_col = 1 if data_obj.type == \"DInSAR\" else 0\n    rate = 1000 if data_obj.type == \"DInSAR\" else 1\n    head_ang_mean = data_obj.data['HEAD_ANG'].mean()\n    inc_ang_mean = data_obj.data['INC_ANG'].mean()\n    mat = np.array(\n        [np.sin(inc_ang_mean) * np.sin(head_ang_mean), - np.sin(inc_ang_mean) * np.cos(head_ang_mean),\n         np.cos(inc_ang_mean)]).reshape(1, -1)\n    df = self.forward[date - datetime.timedelta(days=1)]['xe'][start_col::2] * rate\n    LOS = np.dot(mat, df).T\n    data_obj.data['DSP'] = data_obj.data['DSP'] + LOS[0]\n    data_obj.bias_reduction = False\n</code></pre>"},{"location":"integration/#multidefusion.integration.DataIntegration.kalman_forward","title":"<code>kalman_forward()</code>","text":"<p>Performs the forward pass of the Kalman filter.</p> Source code in <code>multidefusion\\integration.py</code> <pre><code>def kalman_forward(self):\n    \"\"\"\n    Performs the forward pass of the Kalman filter.\n    \"\"\"\n    date_range = pd.date_range(start=self.earliest_date_gnss, end=self.latest_date_all_data, freq=self.TIME_INTERVAL)\n\n    xe = self.xe\n    Pe = self.system_noise_matrix\n    F = self.transition_matrix\n    Q = self.system_noise_matrix\n\n    for date in date_range:\n        obs_vector_list = []\n        projection_matrix_list = []\n        error_matrix_list = []\n        for data_type, data_dict in self.data_dict.items():\n            if not isinstance(data_dict, dict):\n                self._process_data_obj(data_dict, date, projection_matrix_list, error_matrix_list, obs_vector_list)\n            else:\n                for technique, data in data_dict.items():\n                    for orbit, orbit_data in data.items():\n                        if not isinstance(orbit_data, dict):\n                            self._process_data_obj(orbit_data, date, projection_matrix_list, error_matrix_list, obs_vector_list)\n                        else:\n                            for subkey, subdata in orbit_data.items():\n                                self._process_data_obj(subdata, date, projection_matrix_list, error_matrix_list, obs_vector_list)\n        if len(projection_matrix_list) &gt; 0:\n            projection_matrix = np.vstack(projection_matrix_list)\n            error_matrix = block_diag(*error_matrix_list)\n            obs_vector = np.hstack(obs_vector_list).reshape(-1, 1)\n\n        # time update (predict)\n        xp, Pp = self.time_update(xe, Pe, F, Q)\n        self.predicted_state_and_variance[date] = {\"xp\": xp, \"Pp\": Pp}\n\n        # measurement update (estimate)\n        xe, Pe, v, Qv, K = self.measurement_update(xp, Pp, obs_vector, error_matrix, projection_matrix)\n        self.forward[date] = {\"xe\": xe, \"Pe\": Pe}\n    self.forward_df_xe = pd.DataFrame(\n        [(timestamp, *values[\"xe\"]) for timestamp, values in self.forward.items()],\n        columns=[\"timestamp\", \"N\", \"vN\", \"E\", \"vE\", \"U\", \"vU\"]).set_index(\"timestamp\").astype(float)\n</code></pre>"},{"location":"integration/#multidefusion.integration.DataIntegration.kalman_forward_backward","title":"<code>kalman_forward_backward()</code>","text":"<p>Performs the forward-backward pass of the Kalman filter.</p> Source code in <code>multidefusion\\integration.py</code> <pre><code>def kalman_forward_backward(self):\n    \"\"\"\n    Performs the forward-backward pass of the Kalman filter.\n    \"\"\"\n    if not self.forward or not self.predicted_state_and_variance:\n        self.kalman_forward()\n    date_range = pd.date_range(start=self.earliest_date_gnss, end=self.latest_date_all_data, freq=self.TIME_INTERVAL)\n\n    F = self.transition_matrix\n    xe_b = self.forward.get(date_range[-1])[\"xe\"]\n    Pe_b = self.forward.get(date_range[-1])[\"Pe\"]\n\n    for date in reversed(date_range[:-1]):\n        next_day = date + datetime.timedelta(days=self.TIME_INTERVAL_NUM)\n\n        L = np.dot(np.dot(self.forward.get(date)[\"Pe\"], np.transpose(F)), np.linalg.inv(self.predicted_state_and_variance.get(next_day)[\"Pp\"]))\n        xe_b = self.forward.get(date)[\"xe\"] + np.dot(L, (xe_b - self.predicted_state_and_variance.get(next_day)[\"xp\"]))\n        Pe_b = self.forward.get(date)[\"Pe\"] + np.dot(np.dot(L, (Pe_b - self.predicted_state_and_variance.get(next_day)[\"Pp\"])), np.transpose(L))\n        self.backward[date] = {\"xe_b\": xe_b, \"Pe_b\": Pe_b}\n    self.backward_df_xe = pd.DataFrame(\n        [(timestamp, *values[\"xe_b\"]) for timestamp, values in self.backward.items()],\n        columns=[\"timestamp\", \"N\", \"vN\", \"E\", \"vE\", \"U\", \"vU\"]).set_index(\"timestamp\").astype(float)\n</code></pre>"},{"location":"integration/#multidefusion.integration.DataIntegration.measurement_update","title":"<code>measurement_update(xp, Pp, z, R, A)</code>  <code>staticmethod</code>","text":"<p>Measurement update step of the Kalman filter.</p> <p>Parameters:</p> Name Type Description Default <code>xp</code> <code>ndarray</code> <p>Predicted state.</p> required <code>Pp</code> <code>ndarray</code> <p>Variance matrix.</p> required <code>z</code> <code>ndarray</code> <p>Measurement vector.</p> required <code>R</code> <code>ndarray</code> <p>Measurement variance matrix.</p> required <code>A</code> <code>ndarray</code> <p>Design matrix.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:</p> <code>ndarray</code> <p>Updated estimated state, variance, measurement residual, measurement variance, and Kalman gain.</p> Source code in <code>multidefusion\\integration.py</code> <pre><code>@staticmethod\ndef measurement_update(xp: np.ndarray, Pp: np.ndarray, z: np.ndarray, R: np.ndarray, A: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Measurement update step of the Kalman filter.\n\n    Args:\n        xp (np.ndarray): Predicted state.\n        Pp (np.ndarray): Variance matrix.\n        z (np.ndarray): Measurement vector.\n        R (np.ndarray): Measurement variance matrix.\n        A (np.ndarray): Design matrix.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        Updated estimated state, variance, measurement residual, measurement variance, and Kalman gain.\n    \"\"\"\n    # number of states\n    n = xp.shape[0]\n    # number of measurements\n    m = z.shape[0]\n\n    # check sizes of input\n    assert xp.shape[1] == 1, \"predicted state must be a column vector\"\n    assert Pp.shape == (n, n), \"variance matrix must be n-by-n\"\n    assert z.shape[1] == 1, \"measurements must be a column vector\"\n    assert R.shape == (m, m), \"variance matrix must be m-by-m\"\n    assert A.shape == (m, n), \"design matrix must be m-by-n\"\n\n    # measurement update\n    v = z - np.dot(A, xp)\n    Qv = R + np.dot(np.dot(A, Pp), np.transpose(A))\n    K = np.dot(np.dot(Pp, np.transpose(A)), np.linalg.inv(Qv))\n    xe = xp + np.dot(K, v)\n    Pe = np.dot((np.eye(n) - np.dot(K, A)), Pp)\n\n    return xe, Pe, v, Qv, K\n</code></pre>"},{"location":"integration/#multidefusion.integration.DataIntegration.remove_bias_in_sar_data","title":"<code>remove_bias_in_sar_data()</code>","text":"<p>Removes bias in SAR data based on GNSS data.</p> <p>This method iterates through the SAR data stored in the 'data_dict' attribute and applies bias reduction to each dataset, excluding the 'DInSAR' technique. The bias reduction is performed using GNSS data up to the earliest date specified by 'earliest_date_gnss'.</p> <p>Note: - Bias reduction is applied to each dataset within the SAR data structure.</p> <p>Returns: None</p> Source code in <code>multidefusion\\integration.py</code> <pre><code>def remove_bias_in_sar_data(self) -&gt; None:\n    \"\"\"\n    Removes bias in SAR data based on GNSS data.\n\n    This method iterates through the SAR data stored in the 'data_dict' attribute and\n    applies bias reduction to each dataset, excluding the 'DInSAR' technique. The bias\n    reduction is performed using GNSS data up to the earliest date specified by\n    'earliest_date_gnss'.\n\n    Note:\n    - Bias reduction is applied to each dataset within the SAR data structure.\n\n    Returns:\n    None\n    \"\"\"\n    if self.data_dict.get(\"SAR\") is not None:\n        for technique, data in self.data_dict.get(\"SAR\").items():\n            if technique != \"DInSAR\":\n                if isinstance(data, dict):\n                    for orbit, orbit_data in data.items():\n                        if isinstance(orbit_data, dict):\n                            for _, (subkey, subdata) in enumerate(orbit_data.items()):\n                                subdata.reduce_bias_to_gnss(date=self.earliest_date_gnss)\n                        else:\n                            orbit_data.reduce_bias_to_gnss(date=self.earliest_date_gnss)\n</code></pre>"},{"location":"integration/#multidefusion.integration.DataIntegration.sar_data_validation","title":"<code>sar_data_validation()</code>","text":"<p>Validates SAR data and applies bias reduction if necessary.</p> <p>This method iterates through SAR data obtained from different techniques (PSI and SBAS), checks if the oldest date in the data is greater than the earliest GNSS date, and applies bias reduction if the condition is met.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>multidefusion\\integration.py</code> <pre><code>def sar_data_validation(self):\n    \"\"\"\n    Validates SAR data and applies bias reduction if necessary.\n\n    This method iterates through SAR data obtained from different techniques (PSI and SBAS),\n    checks if the oldest date in the data is greater than the earliest GNSS date, and applies\n    bias reduction if the condition is met.\n\n    Returns:\n        None\n    \"\"\"\n    for data in self.get_data(\"SAR\", \"PSI\"):\n        if data.oldest_date &gt; self.earliest_date_gnss:\n            data.bias_reduction = True\n    for data in self.get_data(\"SAR\", \"SBAS\"):\n             data.bias_reduction = True\n    for data in self.get_data(\"SAR\", \"DInSAR\"):\n        if data.oldest_date &lt; self.earliest_date_gnss:\n            data.data = data.data[data.data.index &gt;= self.earliest_date_gnss]\n</code></pre>"},{"location":"integration/#multidefusion.integration.DataIntegration.time_update","title":"<code>time_update(xe, Pe, Phi, S)</code>  <code>staticmethod</code>","text":"<p>Time update step of the Kalman filter.</p> <p>Parameters:</p> Name Type Description Default <code>xe</code> <code>ndarray</code> <p>Predicted state.</p> required <code>Pe</code> <code>ndarray</code> <p>Variance matrix.</p> required <code>Phi</code> <code>ndarray</code> <p>Transition matrix.</p> required <code>S</code> <code>ndarray</code> <p>Process noise matrix.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Updated predicted state and variance.</p> Source code in <code>multidefusion\\integration.py</code> <pre><code>@staticmethod\ndef time_update(xe: np.ndarray, Pe: np.ndarray, Phi: np.ndarray, S: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Time update step of the Kalman filter.\n\n    Args:\n        xe (np.ndarray): Predicted state.\n        Pe (np.ndarray): Variance matrix.\n        Phi (np.ndarray): Transition matrix.\n        S (np.ndarray): Process noise matrix.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Updated predicted state and variance.\n    \"\"\"\n    # number of states\n    n = xe.shape[0]\n\n    # check sizes of input\n    assert xe.shape[1] == 1, \"predicted state must be a column vector\"\n    assert Pe.shape == (n, n), \"variance matrix must be n-by-n\"\n    assert Phi.shape == (n, n), \"transition matrix must be n-by-n\"\n    assert S.shape == (n, n), \"process noise matrix must be n-by-n\"\n\n    # time update\n    xp = np.dot(Phi, xe)\n    Pp = np.dot(np.dot(Phi, Pe), np.transpose(Phi)) + S\n\n    return xp, Pp\n</code></pre>"},{"location":"output/","title":"Output","text":"<p>The MultiDEFusion final output can be divided into two parts: Graphic and Dataset</p>"},{"location":"output/#graphic-part","title":"Graphic part","text":"<p>The final plot is created involving the Plotly library and Dash framework. The graphic representation of the results is automatically launched on the localhost server on port 8050.</p> <p>The most important plot features:</p> <ol> <li>Visualisation: The default port for displaying results on localhost is 8050. For more than one station, the outputs will be displayed simultaneously on consecutive ports.</li> <li>Legend: Work for all of the traces and provide a general overview of the results by activating or deactivating groups of data using the legend entities.</li> <li>Hover: Working within a single trace and providing a detailed overview of the results using the cursor.</li> <li>Table: To provide the most relevant information about the integration procedure.</li> <li>Customisation: To provide individual ranges for dates or values:<ul> <li>Dates range: adjustable by providing initial and last date or by using the calendar.</li> <li>Horizontal range: adjustable by providing minimum and maximum values.</li> <li>Vertical &amp; LOS range: adjustable by providing minimum and maximum values.</li> <li>Rate range: adjustable by providing minimum and maximum values.</li> </ul> </li> <li>Buttons: To facilitate manipulation between the traces:<ul> <li>Restore default: Restore the original ranges of dates or values after user manipulations.</li> <li>Sync ranges: To synchronise the ranges to the same values (by default, the horizontal and vertical ranges of displacement are disjoint).</li> </ul> </li> <li>Mode bar: A default toolbar of the plotly library located in the top right corner:<ul> <li>Download plot: To save the plot in svg format after user's manipulations.</li> <li>Edit in Chart Studio: To provide more advanced modifications in the plotly Chart Studio.</li> <li>Zoom: Work for a selected part of the trace. Double click on the trace to zoom back out.</li> <li>Pan: Working within a single trace by moving the current view of the trace.</li> <li>Draw line: Working within a single trace.</li> <li>Draw circle: Working within a single trace.</li> <li>Draw rectangle: Working within a single trace.</li> <li>Erase the active shape: A single-click to activate the shape.</li> <li>Zoom in: Work simultaneously for all of the traces.</li> <li>Zoom out: Work simultaneously for all of the traces.</li> <li>Reset axes: Work simultaneously for all of the traces.</li> </ul> </li> </ol>"},{"location":"output/#dataset-part","title":"Dataset part","text":"<p>The MultDEFusion library returns a dataset of time series for a particular station.</p> <p>The collection of data is stored in the dictionary, where the key is the signature of station, and the value is the containing the dataset of time series.</p> <p>Using the example parameter signature given in the Trial section, to obtain the time series of Kalman filter results of PI03 station from forward approach, type in your IDE console: <code>integration[\"PI03\"].forward</code>. Furthermore, to get the data from Kalman backward approach, type: <code>integration[\"PI03\"].backward</code>. Warning: Some IDEs may require to stop the console beforehand.</p> <p>More information about attributes of Dataset output can be found in the API Reference integration module section.</p>"},{"location":"results/","title":"results module","text":""},{"location":"results/#multidefusion.results.Figures","title":"<code>Figures</code>","text":"<p>Class for creating displacement plots.</p> <p>The final plot is created involving the Plotly library and Dash framework. The graphic representation of the results is automatically launched on the localhost server on port 8050.</p> <p>The most important plot features:</p> <ol> <li>Visualisation: The default port for displaying results on localhost is 8050. For more than one station, the outputs will be displayed simultaneously on consecutive ports.</li> <li>Legend: Work for all of the traces and provide a general overview of the results by activating or deactivating groups of data using the legend entities.</li> <li>Hover: Working within a single trace and providing a detailed overview of the results using the cursor.</li> <li>Table: To provide the most relevant information about the integration procedure.</li> <li>Customisation: To provide individual ranges for dates or values:<ul> <li>Dates range: adjustable by providing initial and last date or by using the calendar.</li> <li>Horizontal range: adjustable by providing minimum and maximum values.</li> <li>Vertical &amp; LOS range: adjustable by providing minimum and maximum values.</li> <li>Rate range: adjustable by providing minimum and maximum values.</li> </ul> </li> <li>Buttons: To facilitate manipulation between the traces:<ul> <li>Restore default: Restore the original ranges of dates or values after user manipulations.</li> <li>Sync ranges: To synchronise the ranges to the same values (by default, the horizontal and vertical ranges of displacement are disjoint).</li> </ul> </li> <li>Mode bar: A default toolbar of the plotly library located in the top right corner:<ul> <li>Download plot: To save the plot in svg format after user's manipulations.</li> <li>Edit in Chart Studio: To provide more advanced modifications in the plotly Chart Studio.</li> <li>Zoom: Work for a selected part of the trace. Double click on the trace to zoom back out.</li> <li>Pan: Working within a single trace by moving the current view of the trace.</li> <li>Draw line: Working within a single trace.</li> <li>Draw circle: Working within a single trace.</li> <li>Draw rectangle: Working within a single trace.</li> <li>Erase the active shape: A single-click to activate the shape.</li> <li>Zoom in: Work simultaneously for all of the traces.</li> <li>Zoom out: Work simultaneously for all of the traces.</li> <li>Reset axes: Work simultaneously for all of the traces.</li> </ul> </li> </ol> Source code in <code>multidefusion\\results.py</code> <pre><code>class Figures:\n    \"\"\"Class for creating displacement plots.\n\n    The final plot is created involving the Plotly library and Dash framework. The graphic representation of the results is automatically launched on the localhost server on port 8050.\n\n    The most important plot features:\n\n    1. Visualisation: The default port for displaying results on localhost is 8050. For more than one station, the outputs will be displayed simultaneously on consecutive ports.\n    2. Legend: Work for all of the traces and provide a general overview of the results by activating or deactivating groups of data using the legend entities.\n    3. Hover: Working within a single trace and providing a detailed overview of the results using the cursor.\n    4. Table: To provide the most relevant information about the integration procedure.\n    5. Customisation: To provide individual ranges for dates or values:\n        - Dates range: adjustable by providing initial and last date or by using the calendar.\n        - Horizontal range: adjustable by providing minimum and maximum values.\n        - Vertical &amp; LOS range: adjustable by providing minimum and maximum values.\n        - Rate range: adjustable by providing minimum and maximum values.\n    6. Buttons: To facilitate manipulation between the traces:\n        - Restore default: Restore the original ranges of dates or values after user manipulations.\n        - Sync ranges: To synchronise the ranges to the same values (by default, the horizontal and vertical ranges of displacement are disjoint).\n    7. Mode bar: A default toolbar of the plotly library located in the top right corner:\n        - Download plot: To save the plot in svg format after user's manipulations.\n        - Edit in Chart Studio: To provide more advanced modifications in the plotly Chart Studio.\n        - Zoom: Work for a selected part of the trace. Double click on the trace to zoom back out.\n        - Pan: Working within a single trace by moving the current view of the trace.\n        - Draw line: Working within a single trace.\n        - Draw circle: Working within a single trace.\n        - Draw rectangle: Working within a single trace.\n        - Erase the active shape: A single-click to activate the shape.\n        - Zoom in: Work simultaneously for all of the traces.\n        - Zoom out: Work simultaneously for all of the traces.\n        - Reset axes: Work simultaneously for all of the traces.\n    \"\"\"\n    def __init__(self, data_integration):\n        \"\"\"\n        Initialize Figures object.\n\n        Args:\n            data_integration (object): Object containing integrated data.\n        \"\"\"\n        self.data_integration = data_integration\n        sar_data = self.data_integration.data_dict.get(\"SAR\")\n        if sar_data is not None:\n            list_of_orbits = list(set(orbit for technique_data in sar_data.values() for orbit in technique_data))\n            list_of_orbits = [int(x) for x in list_of_orbits]\n            list_of_orbits.sort()\n            list_of_orbits = [str(x) for x in list_of_orbits]\n            self.orbits = list_of_orbits\n            self.number_of_orbits = len(self.orbits)\n            self.sar_data_types = list(set([data for data in self.data_integration.data_dict.get(\"SAR\").keys()]))\n        else:\n            self.orbits = []\n            self.number_of_orbits = 0\n            self.sar_data_types = []\n\n    @staticmethod\n    def find_min_value(*lists):\n        \"\"\"\n        Find the minimum value from a set of lists.\n\n        Args:\n            *lists: Variable number of lists to find the minimum value from.\n\n        Returns:\n            Minimum value from the provided lists.\n        \"\"\"\n        if not lists:\n            return None\n        flattened_list = [item for sublist in lists for item in sublist]\n        return min(flattened_list)\n\n    @staticmethod\n    def find_max_value(*lists):\n        \"\"\"\n        Find the maximum value from a set of lists.\n\n        Args:\n            *lists: Variable number of lists to find the maximum value from.\n\n        Returns:\n            Maximum value from the provided lists.\n        \"\"\"\n        if not lists:\n            return None\n        flattened_list = [item for sublist in lists for item in sublist]\n        return max(flattened_list)\n\n    @staticmethod\n    def find_max_total(df):\n        \"\"\"\n        Find the maximum value (positive or negative) from each column from a set of dataframe.\n\n        Args:\n            df: dataframe\n\n        Returns:\n            Maximum value from from each column from a set of dataframe.\n        \"\"\"\n        return df.apply(lambda col: col.max() if col.max() &gt;= -col.min() else col.min(), axis=0)\n\n    @staticmethod\n    def add_vline(fig, timestamp_min_value, timestamp_max_value, row, col):    \n        \"\"\"\n        Add Thicker line for each subplot to indicate the beginning of a year\n\n        Args:\n            fig: Figure\n            timestamp_min_value: Earliest date\n            timestamp_max_value: Lastest date\n            row: figure's row\n            col: figure's column\n\n        Returns:\n            None\n        \"\"\"\n        [fig.add_shape(x0=date, x1=date, y0=-10, y1=10, type=\"line\", line=dict(color='lightgray', width=2), layer=\"below\", row=row, col=col) for date in\n        pd.date_range(start=timestamp_min_value - relativedelta(years=2),\n        end=timestamp_max_value + relativedelta(years=2), freq='YS')]\n\n    def create_displacement_plot(self):\n        \"\"\"\n        Create displacement plot using Plotly and Dash.\n        \"\"\"\n        # SETTINGS\n        print(\"Development of a figure...\")\n        additional_range = 0.05  # additional +/-5% of range for plots by x and y axis\n        golden_ratio = (1 + 5 ** 0.5) / 2  # for size on y axis\n        subplots_postition = {'3*': [[0.019, 0.335], [0.342, 0.658], [0.665, 0.981]],\n                              '3' : [[0.000, 0.316], [0.323, 0.639], [0.684, 1.000]],\n                              '2' : [[0.1805, 0.4965], [0.5035, 0.8195]],\n                              '1' : [[0.323, 0.639]]}\n\n        shift_rate_fig  = 0.007\n        shift_disp_fig  = 0.100\n        shift_title_fig = 0.003\n        shift_legend    = 0.030\n        rate_fig_size   = 7/25\n        top_limit_plot  = 0.970\n        vertical_postition = {}\n        vertical_postition['1']  = [top_limit_plot-subplots_postition['3'][0][1]/golden_ratio, top_limit_plot]\n        vertical_postition['2']  = [vertical_postition['1'][0]-subplots_postition['3'][0][1]/golden_ratio*rate_fig_size-shift_rate_fig , vertical_postition['1'][0]-shift_rate_fig]\n        vertical_postition['3*'] = [vertical_postition['2'][0]-subplots_postition['3'][0][1]/golden_ratio*rate_fig_size-shift_disp_fig , vertical_postition['2'][0]-shift_disp_fig]\n        vertical_postition['3']  = [vertical_postition['2'][0]-subplots_postition['3'][0][1]/golden_ratio              -shift_disp_fig , vertical_postition['2'][0]-shift_disp_fig]\n        vertical_postition['4']  = [vertical_postition['3'][0]-subplots_postition['3'][0][1]/golden_ratio*rate_fig_size-shift_rate_fig , vertical_postition['3'][0]-shift_rate_fig]\n\n        table_postition = {'GNSS'  : [vertical_postition['2'][0]-subplots_postition['3'][0][1]/golden_ratio-shift_disp_fig, vertical_postition['2'][0]-shift_disp_fig],\n                           'DInSAR': [vertical_postition['3*'][0]-subplots_postition['3'][0][1]/golden_ratio-shift_disp_fig, vertical_postition['3*'][0]-shift_disp_fig],\n                           'SBAS'  : [vertical_postition['3'][0]-subplots_postition['3'][0][1]/golden_ratio-shift_disp_fig, vertical_postition['3'][0]-shift_disp_fig],\n                           'MSAR'  : [vertical_postition['4'][0]-subplots_postition['3'][0][1]/golden_ratio-shift_disp_fig, vertical_postition['4'][0]-shift_disp_fig]}\n\n        shift_dash = 0\n        max_len_elements = max(len(lst) for lst in self.data_integration.number_of_SAR_elements.values())\n        if max_len_elements &gt; 2:\n            shift_dash = (max_len_elements-2)*7\n\n        custom_position = {'GNSS'  : 558,\n                           'DInSAR': 754+shift_dash,\n                           'SBAS'  : 929+shift_dash,\n                           'MSAR'  : 1009+shift_dash}\n\n        rows = 3\n        cols = 3\n        specs = [[{\"type\": \"xy\"}] * cols for _ in range(rows-1)]\n\n        # COLORS BY DATA TYPE\n        data_colors = {\"forward\": \"rgb(25, 255, 25)\",\n                       \"backward\": \"rgb(255, 0, 0)\",\n                       \"GNSS\": \"rgb(0, 190, 255)\",\n\n                       \"DInSAR\":[\"#0000ff\", '#1322FB', '#2541F8', '#375DF6', '#4976F3', '#5A8CF2', '#6AA0F0', '#7BB1EF', '#8AC0EF', '#9ACDEF'],\n                       \"PSI\":   [\"#771e87\", '#782390', '#792999', '#7A2FA2', '#7A36AB', '#7A3DB3', '#7A45BA', '#7B54BB', '#7E62BC', '#8370BD'],\n                       \"SBAS\":  [\"#7f822f\", '#8C8936', '#938B3E', '#9B8D46', '#A28E4E', '#A89057', '#AA9064', '#AC9372', '#AE967E', '#B19C8B'],\n                       }\n\n        disp_cols  = [\"N\", \"E\", \"U\"]\n        rate_cols  = [\"vN\", \"vE\", \"vU\"]\n        disp_units = {\"U\": \"ver\", \"N\": \"hor\", \"E\": \"hor\"}\n\n        # CREATE STORAGE FOR STORE MIN AND MAX FOR SPECIFIC DATA TYPE VALUES\n        max_disp = {\"ver\": [], \"hor\": []}\n        min_disp = {\"ver\": [], \"hor\": []}\n        max_rate = []\n        min_rate = []\n\n        name_forward = \"Forward\"\n        name_backward = \"Backward\"\n\n        subplot_titles = [\"&lt;b&gt;North&lt;/b&gt;\", \"&lt;b&gt;East&lt;/b&gt;\", \"&lt;b&gt;Up&lt;/b&gt;\"] + [''] * cols\n\n        orbit_titles = [\"&lt;b&gt;Orbit \" + orbit + \"&lt;/b&gt;\" for orbit in self.orbits]\n        empty_titles = [\"\"] * (3 - self.number_of_orbits)\n\n        # CREATE GRID FOR SUBPLOTS AND TITLES\n        if \"PSI\" in self.sar_data_types or \"SBAS\" in self.sar_data_types:\n            rows += 1\n            subplot_titles.extend(orbit_titles + empty_titles)\n\n            specs_temp = [[{\"type\": \"xy\"}] * self.number_of_orbits + [None] * (3 - self.number_of_orbits)]\n            specs.extend(specs_temp)\n\n        if \"DInSAR\" in self.sar_data_types:\n            rows += 1\n            specs_temp = [[{\"type\": \"xy\"}] * self.number_of_orbits + [None] * (3 - self.number_of_orbits)]\n            specs.extend(specs_temp)\n            if \"PSI\" not in self.sar_data_types and \"SBAS\" not in self.sar_data_types:\n                subplot_titles.extend(orbit_titles + empty_titles)\n            else:\n                subplot_titles.extend([\"\"] * self.number_of_orbits)\n\n        specs_temp = [ [{\"type\": \"table\"}] + [None]*2]\n        specs.extend(specs_temp)\n\n        # CREATE FIG OBJECT\n        fig = sp.make_subplots(rows=rows, cols=cols,\n                               specs=specs,\n                               subplot_titles=subplot_titles, shared_xaxes=True, )\n\n        # GET TIMESTAMPS FOR GNSS, FORWARD, BACKWARD IF EXIST\n        gnss_timestamp = self.data_integration.data_dict['GNSS'].data.index\n        forward_timestamp = self.data_integration.forward_df_xe.index\n        backward_timestamp = self.data_integration.backward_df_xe.index if self.data_integration.backward_df_xe is not None else None\n        timestamp_min_value = self.find_min_value(gnss_timestamp, forward_timestamp,\n                                                  backward_timestamp) if backward_timestamp is not None else self.find_min_value(\n            gnss_timestamp, forward_timestamp)\n        timestamp_max_value = self.find_max_value(gnss_timestamp, forward_timestamp,\n                                                  backward_timestamp) if backward_timestamp is not None else self.find_max_value(\n            gnss_timestamp, forward_timestamp)\n\n        # CREATE DATES RANGE INCLUDING ADDITIONAL RANGE\n        additional_range_abs = additional_range * abs(timestamp_max_value - timestamp_min_value)\n        dates_range = [timestamp_min_value - additional_range_abs, timestamp_max_value + additional_range_abs]\n\n        # ITERATION FOR GNSS DATA\n        for i, coord in enumerate(disp_cols):\n            showlegend = True if i == 0 else False\n            disp_gnss = self.data_integration.data_dict['GNSS'].data[coord]\n            disp_forward = self.data_integration.forward_df_xe[disp_cols[i]]\n            rate_forward = self.data_integration.forward_df_xe[rate_cols[i]] * 1000\n            disp_type = disp_units.get(coord, None)\n            if disp_type:\n                max_disp[disp_type].extend([max(disp_gnss), max(disp_forward)])\n                min_disp[disp_type].extend([min(disp_gnss), min(disp_forward)])\n            max_rate.extend([max(rate_forward)])\n            min_rate.extend([min(rate_forward)])\n\n            fig.add_trace(\n                go.Scatter(x=gnss_timestamp, y=disp_gnss, mode='markers', name='', showlegend=showlegend,\n                           legendgroup=\"GNSS\", legendgrouptitle_text=\"GNSS\", marker=dict(color=data_colors[\"GNSS\"], size = 8)), \n                row=1, col=i + 1)\n            fig.add_trace(\n                go.Scatter(x=forward_timestamp, y=disp_forward, mode='lines', name=name_forward, showlegend=showlegend,\n                           legendgroup=\"kalman\", legendgrouptitle_text=\"Kalman\",\n                           line=dict(width=5, color=data_colors[\"forward\"])), row=1, col=i + 1)\n            fig.add_trace(\n                go.Scatter(x=forward_timestamp, y=rate_forward, mode='lines', name=name_forward, showlegend=False,\n                           legendgroup=\"kalman\", legendgrouptitle_text=\"Kalman\",\n                           line=dict(width=5, color=data_colors[\"forward\"])), row=2, col=i + 1)\n\n            self.add_vline(fig, timestamp_min_value, timestamp_max_value, row=1, col=i + 1)\n            self.add_vline(fig, timestamp_min_value, timestamp_max_value, row=2, col=i + 1)\n\n            if self.data_integration.backward_df_xe is not None:\n                disp_backward = self.data_integration.backward_df_xe[disp_cols[i]]\n                rate_backward = self.data_integration.backward_df_xe[rate_cols[i]] * 1000\n\n                if disp_type:\n                    max_disp[disp_type].extend([max(disp_backward)])\n                    min_disp[disp_type].extend([min(disp_backward)])\n                max_rate.extend([max(rate_backward)])\n                min_rate.extend([min(rate_backward)])\n\n                fig.add_trace(go.Scatter(x=backward_timestamp, y=disp_backward, mode='lines', name=name_backward,\n                                         showlegend=showlegend, legendgroup=\"kalman\",\n                                         line=dict(width=4, color=data_colors[\"backward\"])), row=1, col=i + 1)\n                fig.add_trace(go.Scatter(x=backward_timestamp, y=rate_backward, mode='lines', name=name_backward,\n                                         showlegend=False, legendgroup=\"kalman\",\n                                         line=dict(width=4, color=data_colors[\"backward\"])), row=2, col=i + 1)\n\n        # ITERATION FOR SAR DATA - only DSP\n        if self.data_integration.data_dict.get('SAR') is not None:\n            for technique, data in self.data_integration.data_dict.get('SAR').items():\n                row = 3\n                if technique == \"DInSAR\" and (\"SBAS\" in self.sar_data_types or \"PSI\" in self.sar_data_types):\n                    row = 4\n                if technique == \"DInSAR\":\n                    for orbit in set(self.orbits).difference(set(data.keys())):\n                        col = self.orbits.index(orbit) + 1 if self.number_of_orbits &gt; 1 else 1\n                        fig.add_trace(\n                            go.Scatter(x=dates_range, y=pd.Series(dtype=object), mode='markers',\n                                       showlegend=False), row=row, col=col)\n                        self.add_vline(fig, timestamp_min_value, timestamp_max_value, row=row, col=col)\n                else:\n                    for orbit in set(self.orbits).difference(set(data.keys())):\n                        col = self.orbits.index(orbit) + 1 if self.number_of_orbits &gt; 1 else 1\n                        fig.add_trace(\n                            go.Scatter(x=dates_range, y=pd.Series(dtype=object), mode='markers',\n                                       showlegend=False), row=row, col=col)\n                        self.add_vline(fig, timestamp_min_value, timestamp_max_value, row=row, col=col)\n                if isinstance(data, dict):\n                    inner_keys = {}\n                    for orbit, orbit_data in data.items():\n                        col = self.orbits.index(orbit) + 1 if self.number_of_orbits &gt; 1 else 1\n                        if isinstance(orbit_data, dict):\n\n                            if len(self.data_integration.number_of_SAR_elements[technique]) == 1:\n                                color_index = [0]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 2:\n                                color_index = [0, 4]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 3:\n                                color_index = [0, 4, 8]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 4:\n                                color_index = [0, 2, 4, 8]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 5:\n                                color_index = [0, 2, 4, 6, 8]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 6:\n                                color_index = [0, 1, 2, 4, 6, 8]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 7:\n                                color_index = [0, 1, 2, 3, 4, 6, 8]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 8:\n                                color_index = [0, 1, 2, 3, 4, 5, 6, 8]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 9:\n                                color_index = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 10:\n                                color_index = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n                            for _, (subkey, subdata) in enumerate(orbit_data.items()):\n                                if subkey in inner_keys.keys():\n                                    showlegend = False\n                                else:\n                                    showlegend = True\n                                    inner_keys[subkey]=color_index[len(inner_keys)]\n\n                                color_for_data = data_colors[subdata.type][inner_keys[subkey]]\n\n                                rates = 1000 if subdata.type == \"DInSAR\" else 1\n                                if subdata.type == \"DInSAR\":\n                                    max_rate.extend([max(subdata.data[\"DSP\"] * rates)])\n                                    min_rate.extend([min(subdata.data[\"DSP\"] * rates)])\n                                else:\n                                    max_disp[\"ver\"].extend([max(subdata.data[\"DSP\"] * rates)])\n                                    min_disp[\"ver\"].extend([min(subdata.data[\"DSP\"] * rates)])\n\n                                fig.add_trace(\n                                    go.Scatter(x=subdata.data.index, y=subdata.data[\"DSP\"] * rates, mode='markers',\n                                               name=subdata.sublabel,\n                                               showlegend=showlegend, legendgroup=subdata.type,\n                                               legendgrouptitle_text=subdata.type,\n                                               marker=dict(color=color_for_data, size = 8)), row=row, col=col)\n                                self.add_vline(fig, timestamp_min_value, timestamp_max_value, row=row, col=col)\n                        else:\n                            showlegend = True if self.orbits.index(orbit) == 0 else False\n                            rates = 1000 if orbit_data.type == \"DInSAR\" else 1\n                            if orbit_data.type == \"DInSAR\":\n                                max_rate.extend([max(orbit_data.data[\"DSP\"] * rates)])\n                                min_rate.extend([min(orbit_data.data[\"DSP\"] * rates)])\n                            else:\n                                max_disp[\"ver\"].extend([max(orbit_data.data[\"DSP\"] * rates)])\n                                min_disp[\"ver\"].extend([min(orbit_data.data[\"DSP\"] * rates)])\n\n                            fig.add_trace(\n                                go.Scatter(x=orbit_data.data.index, y=orbit_data.data[\"DSP\"] * rates, mode='markers',\n                                           name='',\n                                           showlegend=showlegend, legendgroup=orbit_data.type,\n                                           legendgrouptitle_text=orbit_data.type,\n                                           marker=dict(color=data_colors[orbit_data.type][0], size = 8)), row=row, col=col)\n                            self.add_vline(fig, timestamp_min_value, timestamp_max_value, row=row, col=col)\n                    if technique == \"DInSAR\":\n                        for orbit in set(self.orbits).difference(set(data.keys())):\n                            col = self.orbits.index(orbit) + 1 if self.number_of_orbits &gt; 1 else 1\n                            fig.add_trace(\n                                go.Scatter(x=dates_range, y=pd.Series(dtype=object), mode='markers',\n                                           showlegend=False), row=row, col=col)\n                            self.add_vline(fig, timestamp_min_value, timestamp_max_value, row=row, col=col)\n        if self.data_integration.mean_data_dict.get(\"DInSAR\") is not None:\n            row = 4 if (\"SBAS\" in self.sar_data_types or \"PSI\" in self.sar_data_types) else 3\n            for orbit, orbit_data in self.data_integration.mean_data_dict.get(\"DInSAR\").items():\n                col = self.orbits.index(orbit) + 1 if self.number_of_orbits &gt; 1 else 1\n\n                max_rate.extend([max(orbit_data['forward_mean'])])\n                min_rate.extend([min(orbit_data['forward_mean'])])\n\n                fig.add_trace(go.Scatter(x=orbit_data['forward_mean'].index, y=orbit_data['forward_mean'], mode='lines',\n                                         name=name_forward, showlegend=False, legendgroup=\"kalman\",\n                                         line=dict(width=5, color=data_colors[\"forward\"])), row=row, col=col)\n                if orbit_data.get('backward_mean') is not None:\n                    max_rate.extend([max(orbit_data['backward_mean'])])\n                    min_rate.extend([min(orbit_data['backward_mean'])])\n\n                    fig.add_trace(\n                        go.Scatter(x=orbit_data['backward_mean'].index, y=orbit_data['backward_mean'], mode='lines',\n                                   name=name_backward, showlegend=False, legendgroup=\"kalman\",\n                                   line=dict(width=4, color=data_colors[\"backward\"])), row=row, col=col)\n\n        if self.data_integration.mean_data_dict.get(\"SAR_SBAS_PSI_MEAN\") is not None:\n            row = 3\n            for orbit, orbit_data in self.data_integration.mean_data_dict.get(\"SAR_SBAS_PSI_MEAN\").items():\n                col = self.orbits.index(orbit) + 1 if self.number_of_orbits &gt; 1 else 1\n\n                max_disp[\"ver\"].extend([max(orbit_data['forward_mean'].dropna())])\n                min_disp[\"ver\"].extend([min(orbit_data['forward_mean'].dropna())])\n\n                fig.add_trace(\n                    go.Scatter(x=orbit_data['forward_mean'].dropna().index, y=orbit_data['forward_mean'].dropna(),\n                               mode='lines', name=name_forward, showlegend=False, legendgroup=\"kalman\",\n                               line=dict(width=5, color=data_colors[\"forward\"])), row=row, col=col)\n\n                if orbit_data.get('backward_mean') is not None:\n                    max_disp[\"ver\"].extend([max(orbit_data['backward_mean'].dropna())])\n                    min_disp[\"ver\"].extend([min(orbit_data['backward_mean'].dropna())])\n\n                    fig.add_trace(\n                        go.Scatter(x=orbit_data['backward_mean'].dropna().index, y=orbit_data['backward_mean'].dropna(),\n                                   mode='lines', name=name_backward, showlegend=False, legendgroup=\"kalman\",\n                                   line=dict(width=4, color=data_colors[\"backward\"])), row=row, col=col)\n\n        # ORDER OF TRACES FOR LEGEND\n        new_data = [trace for trace in fig.data if trace.name != name_forward] + [trace for trace in fig.data if\n                                                                                  trace.name == name_forward]\n        new_data = [trace for trace in new_data if trace.name != name_backward] + [trace for trace in new_data if\n                                                                                   trace.name == name_backward]\n        fig.data = new_data\n\n        # CREATE Y RANGE\n        max_disp_ver = self.find_max_value(max_disp[\"ver\"])\n        max_disp_hor = self.find_max_value(max_disp[\"hor\"])\n        min_disp_ver = self.find_min_value(min_disp[\"ver\"])\n        min_disp_hor = self.find_min_value(min_disp[\"hor\"])\n\n        min_rate = self.find_min_value(min_rate)\n        max_rate = self.find_max_value(max_rate)\n\n        range_ver = [min_disp_ver - additional_range * abs(max_disp_ver - min_disp_ver),\n                     max_disp_ver + additional_range * abs(max_disp_ver - min_disp_ver)]\n        range_hor = [min_disp_hor - additional_range * abs(max_disp_hor - min_disp_hor),\n                     max_disp_hor + additional_range * abs(max_disp_hor - min_disp_hor)]\n        range_rate = [min_rate - additional_range * abs(max_rate - min_rate),\n                      max_rate + additional_range * abs(max_rate - min_rate)]\n\n        min_range_value = 0.05\n        if min(range_ver[0], range_hor[0]) &gt; -min_range_value and max(range_ver[1], range_hor[1]) &lt; min_range_value:\n            range_ver = [-min_range_value, min_range_value]\n            range_hor = [-min_range_value, min_range_value]\n\n        # UPDATE X AND Y RANGE FOR GNSS DISPLACEMENT    \n        fig.update_yaxes(range=range_hor,   row=1, col=1)\n        fig.update_yaxes(range=range_hor,   row=1, col=2)\n        fig.update_yaxes(range=range_ver,   row=1, col=3)\n        fig.update_xaxes(range=dates_range, row=1)\n\n        # UPDATE X AND Y RANGE FOR GNSS RATE\n        fig.update_xaxes(range=dates_range, row=2)\n        fig.update_yaxes(range=range_rate,  row=2)\n\n        # UPDATE X AND Y RANGE FOR DInSAR OR PSI&amp; SBAS\n        fig.update_xaxes(range=dates_range, row=3)\n        if rows == 4 and \"DInSAR\" in self.sar_data_types: \n            fig.update_yaxes(range=range_rate, row=3)\n        else:\n            fig.update_yaxes(range=range_ver,  row=3)\n\n        # UPDATE X AND Y RANGE FOR DInSAR WITH PSI&amp; SBAS\n        fig.update_xaxes(range=dates_range, row=4)\n        fig.update_yaxes(range=range_rate,  row=4)\n\n\n        # UPDATE EVERY ANNOTATIONS -&gt; FONT SIZE\n        fig.update_annotations(font_size=24, font_color = 'black')\n\n        layout_settings = {\n            'title_text': f\"MultiDEFusion: &lt;b&gt;{self.data_integration.station}&lt;/b&gt;\",\n            'font': dict(family='Helvetica', size=20, color = 'black'),\n            'height': 1400,\n            'width': 1600,\n            'showlegend': True,\n            'margin' : dict(r=10,b=10),\n            'legend': dict(orientation='h',\n                           # groupclick=\"toggleitem\",\n                           itemsizing='constant',\n                           bordercolor='black',\n                           borderwidth=1,\n                           yanchor=\"bottom\",\n                           y=vertical_postition['1'][1]+shift_legend,\n                           xanchor=\"right\",\n                           x=1),\n            'plot_bgcolor': 'white',\n        }\n\n        axis_settings = {\n            'gridcolor': 'lightgray',\n            'ticks': 'inside',\n            'linecolor': 'black',\n            'mirror': 'ticks',\n            'color': 'black',\n            'minor': dict(gridcolor='lightgray', gridwidth=0.1, ticks='inside', tickcolor='black'),\n            'automargin': 'height+width+left+right',    \n        }\n\n        # UPDATE POSITION AND SETTINGS FOR GNSS DISPLACEMENT\n        for i in range(1, 4):\n            layout_settings[f'xaxis{i}'] = dict(domain=subplots_postition[\"3\"][i - 1], showticklabels=False, hoverformat = '%d %b %Y',\n                                                **axis_settings)\n            layout_settings[f'yaxis{i}'] = dict(\n                domain=vertical_postition['1'],\n                title='&lt;b&gt;Displacement [m]&lt;/b&gt;' if i == 1 else '',\n                showticklabels=True if i == 1 or i == 3 else False,\n                tickformat='.2f',\n                zeroline=True,\n                zerolinewidth=2,\n                zerolinecolor='lightgray',\n                hoverformat = '.3f',\n                **axis_settings\n            )\n        # UPDATE POSITION AND SETTINGS FOR GNSS RATE\n        for i in range(1, 4):\n            layout_settings[f'xaxis{i + 3}'] = dict(domain=subplots_postition[\"3\"][i - 1],\n                                                    showticklabels=True, tickformat='%b&lt;br&gt;%Y', hoverformat = '%d %b %Y', **axis_settings)\n            layout_settings[f'yaxis{i + 3}'] = dict(\n                domain=vertical_postition['2'],\n                title='&lt;b&gt;Rate&lt;br&gt;[mm/day]&lt;/b&gt;' if i == 1 else '',\n                showticklabels=True if i == 1 or i == 3 else False,\n                tickformat='.2f',\n                zeroline=True,\n                zerolinewidth=2,\n                zerolinecolor='lightgray',\n                hoverformat = '.1f',\n                **axis_settings\n            )\n\n        if self.number_of_orbits == 3:\n            subplots_postition_orbits = str(self.number_of_orbits) + \"*\"\n        else:\n            subplots_postition_orbits = str(self.number_of_orbits)\n\n        table_vertical  = table_postition['GNSS']\n        table_title_pos = shift_title_fig*5\n        picker_position = custom_position['GNSS']\n\n        if rows == 4:\n            if \"DInSAR\" in self.sar_data_types and len(self.sar_data_types) == 1:\n                table_vertical  = table_postition['DInSAR']\n                table_title_pos = shift_title_fig*5\n                picker_position = custom_position['DInSAR']\n                for i in range(1, len(self.orbits) + 1):\n                    layout_settings[f'xaxis{i + 6}'] = dict(\n                        domain=subplots_postition[subplots_postition_orbits][i - 1],\n                        tickformat='%b&lt;br&gt;%Y', hoverformat = '%d %b %Y', **axis_settings)\n                    layout_settings[f'yaxis{i + 6}'] = dict(\n                        domain=vertical_postition['3*'],\n                        title='&lt;b&gt;Rate&lt;br&gt;[mm/day]&lt;/b&gt;' if i == 1 else '',\n                        showticklabels=True if i == 1 else False,\n                        tickformat='.2f',\n                        zeroline=True,\n                        zerolinewidth=2,\n                        zerolinecolor='lightgray',\n                        hoverformat = '.1f',\n                        **axis_settings\n                    )\n\n        if rows == 4:\n            if \"DInSAR\" not in self.sar_data_types and len(self.sar_data_types) &gt; 0:\n                table_vertical  = table_postition['SBAS']\n                table_title_pos = shift_title_fig\n                picker_position = custom_position['SBAS']\n                for i in range(1, len(self.orbits) + 1):\n                    layout_settings[f'xaxis{i + 6}'] = dict(\n                        domain=subplots_postition[subplots_postition_orbits][i - 1],\n                        tickformat='%b&lt;br&gt;%Y', hoverformat = '%d %b %Y', **axis_settings)\n                    layout_settings[f'yaxis{i + 6}'] = dict(\n                        domain=vertical_postition['3'],\n                        title='&lt;b&gt;Displacement [m]&lt;/b&gt;' if i == 1 else '',\n                        showticklabels=True if i == 1 else False,\n                        tickformat='.2f',\n                        zeroline=True,\n                        zerolinewidth=2,\n                        zerolinecolor='lightgray',\n                        hoverformat = '.3f',\n                        **axis_settings\n                    )\n\n        if rows == 5:\n            table_vertical  = table_postition['MSAR']\n            table_title_pos = shift_title_fig\n            picker_position = custom_position['MSAR']\n            for i in range(1, len(self.orbits) + 1):\n                layout_settings[f'xaxis{i + 6}'] = dict(domain=subplots_postition[subplots_postition_orbits][i - 1],\n                                                        showticklabels=False, hoverformat = '%d %b %Y', **axis_settings)\n                layout_settings[f'yaxis{i + 6}'] = dict(\n                    domain=vertical_postition['3'],\n                    title='&lt;b&gt;Displacement [m]&lt;/b&gt;' if i == 1 else '',\n                    showticklabels=True if i == 1 else False,\n                    tickformat='.2f',\n                    zeroline=True,\n                    zerolinewidth=2,            \n                    zerolinecolor='lightgray',\n                    hoverformat = '.3f',\n                    **axis_settings\n                )\n                layout_settings[f'xaxis{i + 6 + len(self.orbits)}'] = dict(\n                    domain=subplots_postition[subplots_postition_orbits][i - 1],\n                    tickformat='%b&lt;br&gt;%Y', hoverformat = '%d %b %Y', **axis_settings)\n                layout_settings[f'yaxis{i + 6 + len(self.orbits)}'] = dict(\n                    domain=vertical_postition['4'],\n                    title='&lt;b&gt;Rate&lt;br&gt;[mm/day]&lt;/b&gt;' if i == 1 else '',\n                    showticklabels=True if i == 1 else False,\n                    tickformat='.2f',\n                    zeroline=True,\n                    zerolinewidth=2,\n                    zerolinecolor='lightgray',\n                    hoverformat = '.1f',\n                    **axis_settings\n                )\n\n        fig.update_layout(**layout_settings)\n\n        fig.update_layout(xaxis=dict(matches='x'),\n                          xaxis4=dict(matches='x'),\n                          xaxis2=dict(matches='x2'),\n                          xaxis5=dict(matches='x2'),\n                          xaxis3=dict(matches='x3'),\n                          xaxis6=dict(matches='x3'),\n                          hovermode='x unified',\n                          )\n\n        fig.update_traces(hovertemplate='%{y} %{xother}')\n\n        if self.data_integration.backward_df_xe is not None:\n            max_total_values = self.find_max_total(self.data_integration.backward_df_xe)\n            mean_rate = self.data_integration.backward_df_xe.mean()\n            kalman_col = '  Kalman&lt;br&gt;Backward'\n        else:\n            kalman_col = ' Kalman&lt;br&gt;Forward'\n            max_total_values = self.find_max_total(self.data_integration.forward_df_xe)\n            mean_rate = self.data_integration.forward_df_xe.mean()\n\n\n        fig.add_trace(go.Table(\n            header=dict(values=[f\"{kalman_col}\", '     Max&lt;br&gt;  DEF [m]',  'Mean rate&lt;br&gt;  [m/year]'],\n                        align='center',\n                        fill_color = 'rgb(189, 215, 231)',\n                        line_color='black',\n                        font = dict(color = 'black', size = 24)),\n            cells =dict(values=[['North', 'East', 'Up'], max_total_values[disp_cols].apply(lambda x: f'{x:.3f}'), (mean_rate[rate_cols]*365.25).apply(lambda x: f'{x:.3f}')], \n                        align='center', \n                        height=41.5,\n                        fill_color = 'white',\n                        line_color='black',\n                        font = dict(color = 'black', size = 24))\n            ), row=rows, col=1)\n\n        fig.update_traces(\n            domain={'y': table_vertical, 'x': subplots_postition['3'][1]},\n            selector={'type': 'table'})\n\n        for annotation, domain in zip(fig['layout']['annotations'][:3], subplots_postition[\"3\"]):\n            annotation['x'] = (domain[0] + domain[1]) / 2\n            annotation['y'] = vertical_postition['1'][1]+shift_title_fig\n\n        if self.number_of_orbits == 3:\n            subplots_pos = subplots_postition[\"3*\"]\n        elif self.number_of_orbits in [1, 2]:\n            subplots_pos = subplots_postition[str(self.number_of_orbits)]\n\n        if rows &gt; 3:\n            for annotation, domain in zip(fig['layout']['annotations'][3:], subplots_pos):\n                annotation['x'] = (domain[0] + domain[1]) / 2\n                annotation['y'] = vertical_postition['3'][1]+shift_title_fig\n\n        fig.add_annotation(\n            x = mean(subplots_postition['3'][1]),\n            y = table_vertical[1]+table_title_pos,\n            xref = \"paper\",\n            yref = \"paper\",\n            showarrow=False,\n            font_size = 25,\n            font_color = 'black',\n            text = \"&lt;b&gt;Relevant values&lt;/b&gt;\")\n\n        fig.add_annotation(\n            x = mean(subplots_postition['3'][1]),\n            y = 0.005,\n            xref = \"paper\",\n            yref = \"paper\",\n            showarrow=False,\n            font_family = 'Helvetica',\n            font_size = 15,\n            font_color = 'black',\n            text = \"The Kalman filter displacements and rates shown for InSAR orbits in the Line of Sight (LOS) domain were determined based on the mean heading and incidence angle values.\")\n\n        #DASH PART\n        app = dash.Dash(__name__)\n        container1 = html.Div([\n            dcc.Graph(\n                id='subplot-graph',\n                figure = fig,\n                config = {\n                    'toImageButtonOptions': {'format': 'svg', 'filename': 'MultiDEFusion_'+self.data_integration.station},\n                    'displaylogo': False,\n                    'showEditInChartStudio': True,\n                    'plotlyServerURL': \"https://chart-studio.plotly.com\",\n                    'modeBarButtonsToRemove': ['select', 'lasso2d', 'autoScale'],\n                    'modeBarButtonsToAdd': ['drawline', 'drawcircle', 'drawrect', 'eraseshape', 'sendDataToCloud']\n                    }\n                ),\n            ])\n\n        container2 = html.Div([\n            html.H2('Customise dates range',\n                    style={'fontsize': '25px',\n                           'margin-bottom': '5px',\n                           'margin-left': '65px',}),\n\n\t\t    dcc.DatePickerRange(\n\t\t\t\tid='date_range',\n\t\t\t\tdisplay_format='DD/MM/YYYY',\n\t\t\t\tshow_outside_days = True,\n\t\t\t\tstart_date=dates_range[0],\n\t\t\t\tend_date=dates_range[1],\n                number_of_months_shown=2,\n                style={'border': '2px solid black',\n                       'margin-bottom': '10px',\n                       'vertical-align': 'middle',\n                       'display': 'inline-block'},\n\t\t\t),\n\n            html.Button('Restore default',\n                        id='reset_dates',\n                        n_clicks=0,\n                        style={'fontsize': '20px',\n                               'font-weight': 'bold',\n                               'height': '45px', \n                               'width': '100px', \n                               'margin-left': '10px',\n                               'margin-bottom': '10px',\n                               'vertical-align': 'middle',\n                               'display': 'inline-block'}),\n\n            html.Div(id='output_x_range',\n                     style={'fontsize': '15px',\n                            'color': 'red'}),\n\n            html.H2('Customise values range',\n                    style={'fontsize': '25px',\n                           'margin-bottom': '5px',\n                           'margin-left': '60px'}),\n\n            html.Div([\n            dcc.Dropdown(\n                id = 'show_or_hide',\n                options=[\n                    {'label': 'Horizontal range [m]', 'value': 'hor'},\n                    {'label': 'Vertical &amp; LOS range [m]', 'value': 'ver'},\n                    {'label': 'Rate range [mm/day]', 'value': 'rate'}],\n                value = 'hor',\n                clearable = False,\n                style={'width': '286px',\n                       'font-size': '20px', \n                       'border': '0.2px solid black',\n                       'border-radius': '0',\n                       'height': '40px', \n                       'vertical-align': 'middle',\n                       'display': 'inline-block'},\n            ),\n\n            html.Button('Sync ranges',\n                        id='fit_ranges',\n                        n_clicks=0,\n                        style={'fontsize': '20px',\n                               'font-weight': 'bold',\n                               'height': '38px', \n                               'width': '100px',\n                               'margin-left': '10px',\n                               'vertical-align': 'middle',\n                               'display': 'inline-block'}\n            ),\n            ]),\n\n            html.Div([\n                html.Div(id = 'hor_options', children=[\n                dcc.Input(\n                id = 'hor_min',\n                placeholder = 'Minimum value',\n                type='number',\n                step = 0.001,\n                value = float(\"{:.3f}\".format(range_hor[0])),\n                style = {'font-size': '17px',\n                         'border': '1px solid black',\n                         'height': '30px', \n                         'width': '138px'}\n                ),\n\n                dcc.Input(\n                id = 'hor_max',\n                placeholder = 'Maximum value',\n                type='number',\n                step = 0.001,\n                value = float(\"{:.3f}\".format(range_hor[1])),\n                style = {'font-size': '17px',\n                         'border': '1px solid black',\n                         'height': '30px', \n                         'width': '138px'}\n                )\n            ],\n            style={'display': 'block',\n                   'margin-top': '5px',\n                   'margin-bottom': '10px'},\n            ),\n\n            html.Div(id = 'ver_options', children=[\n                dcc.Input(\n                id = 'ver_min',\n                placeholder = 'Minimum value',\n                type='number',\n                step = 0.001,\n                value = float(\"{:.3f}\".format(range_ver[0])),\n                style = {'font-size': '17px',\n                         'border': '1px solid black',\n                         'height': '30px', \n                         'width': '138px'}\n                ),\n\n                dcc.Input(\n                id = 'ver_max',\n                placeholder = 'Maximum value',\n                type='number',\n                step = 0.001,\n                value = float(\"{:.3f}\".format(range_ver[1])),\n                style = {'font-size': '17px',\n                         'border': '1px solid black',\n                         'height': '30px', \n                         'width': '138px'}\n                )\n            ],\n            style={'display': 'block',\n                   'margin-bottom': '10px'},\n            ),\n\n            html.Div(id = 'rate_options', children=[\n                dcc.Input(\n                id = 'rate_min',\n                placeholder = 'Minimum value',\n                type='number',\n                step = 0.1,\n                value = float(\"{:.1f}\".format(range_rate[0])),\n                style = {'font-size': '17px',\n                         'border': '1px solid black',\n                         'height': '30px', \n                         'width': '138px'}\n                ),\n\n                dcc.Input(\n                id = 'rate_max',\n                placeholder = 'Maximum value',\n                type='number',\n                step = 0.1,\n                value = float(\"{:.1f}\".format(range_rate[1])),\n                style = {'font-size': '17px', \n                         'border': '1px solid black',\n                         'height': '30px', \n                         'width': '138px'}\n                ),\n\n            ],\n            style={'display': 'block',\n                   'margin-bottom': '10px'},\n            )\n            ], style={'display': 'inline-block',\n                      'vertical-align': 'middle'}),\n\n\n            html.Button('Restore default',\n                        id='reset_values',\n                        n_clicks=0,\n                        style={'fontsize': '20px',\n                               'font-weight': 'bold',\n                               'height': '38px', \n                               'width': '100px',\n                               'margin-left': '10px',\n                               'display': 'inline-block',\n                               'vertical-align': 'middle',}\n            ),\n\n\n            html.Div(id='output_y_range',\n                     style={'fontsize': '15px',\n                            'color': 'red'}\n                     ),\n        ], \n        style={'position': 'absolute', \n               'top': f'{picker_position}px', \n               'left': '145px', \n               'width': 'fit-content',\n               'fontFamily': 'Helvetica',\n               'color': 'black', \n               }\n        )\n\n        app.layout = html.Div([container1, container2])\n\n        @app.callback(\n            Output('hor_options', 'style'),\n            Output('ver_options', 'style'),\n            Output('rate_options', 'style'),\n            Input('show_or_hide', 'value'))\n\n        def show_hide_element(visibility_state):\n            if visibility_state == 'hor':\n                return {'display': 'block'}, {'display': 'none'}, {'display': 'none'}\n            elif visibility_state == 'ver':\n                return {'display': 'none'}, {'display': 'block'}, {'display': 'none'}\n            else:\n                return {'display': 'none'}, {'display': 'none'}, {'display': 'block'}\n\n        @app.callback(\n           Output('subplot-graph', 'figure'),\n           Output('output_x_range', 'children'),\n           Output('output_y_range', 'children'),\n           [Input('date_range', 'start_date'),\n            Input('date_range', 'end_date'),\n            Input('hor_min', 'value'), \n            Input('hor_max', 'value'),\n            Input('ver_min', 'value'), \n            Input('ver_max', 'value'),\n            Input('rate_min', 'value'), \n            Input('rate_max', 'value')]\n           )\n\n        def update_subplot(start_date, end_date, hor_min, hor_max, ver_min, ver_max, rate_min, rate_max):\n\n            warning_dates = None\n            if end_date &lt;= start_date and end_date is not None and start_date is not None:\n                warning_dates = 'The end date must be greater than the start date!'\n            if warning_dates == None:\n                fig.update_xaxes(range=(start_date, end_date))\n\n            warning_values = None\n            if hor_min is not None and hor_max is not None and hor_max &lt;= hor_min:\n                warning_values = 'Maximum value must be greater than minimum!'\n            if ver_min is not None and ver_max is not None and ver_max &lt;= ver_min:\n                warning_values = 'Maximum value must be greater than minimum!'\n            if rate_min is not None and rate_max is not None and rate_max &lt;= rate_min:\n                warning_values = 'Maximum value must be greater than minimum!'\n            if warning_values == None:                \n                # UPDATE Y RANGE FOR GNSS DISPLACEMENT    \n                fig.update_yaxes(range=[hor_min, hor_max], row=1, col=1)\n                fig.update_yaxes(range=[hor_min, hor_max], row=1, col=2)\n                fig.update_yaxes(range=[ver_min, ver_max], row=1, col=3)\n\n                # UPDATE X AND Y RANGE FOR GNSS RATE\n                fig.update_yaxes(range=[rate_min, rate_max], row=2)\n\n                # UPDATE X AND Y RANGE FOR DInSAR OR PSI &amp; SBAS\n                if rows == 4 and \"DInSAR\" in self.sar_data_types: \n                    fig.update_yaxes(range=[rate_min, rate_max], row=3)\n                else:\n                    fig.update_yaxes(range=[ver_min, ver_max], row=3)\n\n                # UPDATE X AND Y RANGE FOR DInSAR WITH PSI&amp; SBAS\n                fig.update_yaxes(range=[rate_min, rate_max], row=4)\n\n            return fig, warning_dates, warning_values\n\n\n        @app.callback(\n            Output('date_range', 'start_date'),\n            Output('date_range', 'end_date'),\n            Input('reset_dates', 'n_clicks')\n        )\n\n        def reset_date_range(n_clicks):\n            return dates_range[0], dates_range[1]\n\n        @app.callback(\n          [Output('hor_min', 'value'), \n            Output('hor_max', 'value'),\n            Output('ver_min', 'value'), \n            Output('ver_max', 'value'),\n            Output('rate_min', 'value'), \n            Output('rate_max', 'value')],\n            Input('reset_values', 'n_clicks'),\n            Input('fit_ranges', 'n_clicks'),\n            prevent_initial_call=True\n          )\n\n        def reset_fit_range(btn1, btn2):\n            min_range = float(\"{:.3f}\".format(min(range_ver[0], range_hor[0])))\n            max_range = float(\"{:.3f}\".format(max(range_ver[1], range_hor[1])))\n            if 'reset_values' == ctx.triggered_id:\n                return float(\"{:.3f}\".format(range_hor[0])), float(\"{:.3f}\".format(range_hor[1])), float(\"{:.3f}\".format(range_ver[0])), float(\"{:.3f}\".format(range_ver[1])),  float(\"{:.1f}\".format(range_rate[0])), float(\"{:.1f}\".format(range_rate[1]))\n            if 'fit_ranges' == ctx.triggered_id:\n                return min_range, max_range, min_range, max_range, float(\"{:.1f}\".format(range_rate[0])), float(\"{:.1f}\".format(range_rate[1]))\n\n\n        print(\"MultiDEFusion procedure accomplished.\\n\")\n        if __name__ == 'multidefusion.results':\n            app.run_server(debug=True, host=\"localhost\", port=self.data_integration.port, use_reloader=False);\n            webbrowser.open(f'http://localhost:{self.data_integration.port}')\n            print(\"___________________________________________________________\")\n</code></pre>"},{"location":"results/#multidefusion.results.Figures.__init__","title":"<code>__init__(data_integration)</code>","text":"<p>Initialize Figures object.</p> <p>Parameters:</p> Name Type Description Default <code>data_integration</code> <code>object</code> <p>Object containing integrated data.</p> required Source code in <code>multidefusion\\results.py</code> <pre><code>def __init__(self, data_integration):\n    \"\"\"\n    Initialize Figures object.\n\n    Args:\n        data_integration (object): Object containing integrated data.\n    \"\"\"\n    self.data_integration = data_integration\n    sar_data = self.data_integration.data_dict.get(\"SAR\")\n    if sar_data is not None:\n        list_of_orbits = list(set(orbit for technique_data in sar_data.values() for orbit in technique_data))\n        list_of_orbits = [int(x) for x in list_of_orbits]\n        list_of_orbits.sort()\n        list_of_orbits = [str(x) for x in list_of_orbits]\n        self.orbits = list_of_orbits\n        self.number_of_orbits = len(self.orbits)\n        self.sar_data_types = list(set([data for data in self.data_integration.data_dict.get(\"SAR\").keys()]))\n    else:\n        self.orbits = []\n        self.number_of_orbits = 0\n        self.sar_data_types = []\n</code></pre>"},{"location":"results/#multidefusion.results.Figures.add_vline","title":"<code>add_vline(fig, timestamp_min_value, timestamp_max_value, row, col)</code>  <code>staticmethod</code>","text":"<p>Add Thicker line for each subplot to indicate the beginning of a year</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <p>Figure</p> required <code>timestamp_min_value</code> <p>Earliest date</p> required <code>timestamp_max_value</code> <p>Lastest date</p> required <code>row</code> <p>figure's row</p> required <code>col</code> <p>figure's column</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>multidefusion\\results.py</code> <pre><code>@staticmethod\ndef add_vline(fig, timestamp_min_value, timestamp_max_value, row, col):    \n    \"\"\"\n    Add Thicker line for each subplot to indicate the beginning of a year\n\n    Args:\n        fig: Figure\n        timestamp_min_value: Earliest date\n        timestamp_max_value: Lastest date\n        row: figure's row\n        col: figure's column\n\n    Returns:\n        None\n    \"\"\"\n    [fig.add_shape(x0=date, x1=date, y0=-10, y1=10, type=\"line\", line=dict(color='lightgray', width=2), layer=\"below\", row=row, col=col) for date in\n    pd.date_range(start=timestamp_min_value - relativedelta(years=2),\n    end=timestamp_max_value + relativedelta(years=2), freq='YS')]\n</code></pre>"},{"location":"results/#multidefusion.results.Figures.create_displacement_plot","title":"<code>create_displacement_plot()</code>","text":"<p>Create displacement plot using Plotly and Dash.</p> Source code in <code>multidefusion\\results.py</code> <pre><code>    def create_displacement_plot(self):\n        \"\"\"\n        Create displacement plot using Plotly and Dash.\n        \"\"\"\n        # SETTINGS\n        print(\"Development of a figure...\")\n        additional_range = 0.05  # additional +/-5% of range for plots by x and y axis\n        golden_ratio = (1 + 5 ** 0.5) / 2  # for size on y axis\n        subplots_postition = {'3*': [[0.019, 0.335], [0.342, 0.658], [0.665, 0.981]],\n                              '3' : [[0.000, 0.316], [0.323, 0.639], [0.684, 1.000]],\n                              '2' : [[0.1805, 0.4965], [0.5035, 0.8195]],\n                              '1' : [[0.323, 0.639]]}\n\n        shift_rate_fig  = 0.007\n        shift_disp_fig  = 0.100\n        shift_title_fig = 0.003\n        shift_legend    = 0.030\n        rate_fig_size   = 7/25\n        top_limit_plot  = 0.970\n        vertical_postition = {}\n        vertical_postition['1']  = [top_limit_plot-subplots_postition['3'][0][1]/golden_ratio, top_limit_plot]\n        vertical_postition['2']  = [vertical_postition['1'][0]-subplots_postition['3'][0][1]/golden_ratio*rate_fig_size-shift_rate_fig , vertical_postition['1'][0]-shift_rate_fig]\n        vertical_postition['3*'] = [vertical_postition['2'][0]-subplots_postition['3'][0][1]/golden_ratio*rate_fig_size-shift_disp_fig , vertical_postition['2'][0]-shift_disp_fig]\n        vertical_postition['3']  = [vertical_postition['2'][0]-subplots_postition['3'][0][1]/golden_ratio              -shift_disp_fig , vertical_postition['2'][0]-shift_disp_fig]\n        vertical_postition['4']  = [vertical_postition['3'][0]-subplots_postition['3'][0][1]/golden_ratio*rate_fig_size-shift_rate_fig , vertical_postition['3'][0]-shift_rate_fig]\n\n        table_postition = {'GNSS'  : [vertical_postition['2'][0]-subplots_postition['3'][0][1]/golden_ratio-shift_disp_fig, vertical_postition['2'][0]-shift_disp_fig],\n                           'DInSAR': [vertical_postition['3*'][0]-subplots_postition['3'][0][1]/golden_ratio-shift_disp_fig, vertical_postition['3*'][0]-shift_disp_fig],\n                           'SBAS'  : [vertical_postition['3'][0]-subplots_postition['3'][0][1]/golden_ratio-shift_disp_fig, vertical_postition['3'][0]-shift_disp_fig],\n                           'MSAR'  : [vertical_postition['4'][0]-subplots_postition['3'][0][1]/golden_ratio-shift_disp_fig, vertical_postition['4'][0]-shift_disp_fig]}\n\n        shift_dash = 0\n        max_len_elements = max(len(lst) for lst in self.data_integration.number_of_SAR_elements.values())\n        if max_len_elements &gt; 2:\n            shift_dash = (max_len_elements-2)*7\n\n        custom_position = {'GNSS'  : 558,\n                           'DInSAR': 754+shift_dash,\n                           'SBAS'  : 929+shift_dash,\n                           'MSAR'  : 1009+shift_dash}\n\n        rows = 3\n        cols = 3\n        specs = [[{\"type\": \"xy\"}] * cols for _ in range(rows-1)]\n\n        # COLORS BY DATA TYPE\n        data_colors = {\"forward\": \"rgb(25, 255, 25)\",\n                       \"backward\": \"rgb(255, 0, 0)\",\n                       \"GNSS\": \"rgb(0, 190, 255)\",\n\n                       \"DInSAR\":[\"#0000ff\", '#1322FB', '#2541F8', '#375DF6', '#4976F3', '#5A8CF2', '#6AA0F0', '#7BB1EF', '#8AC0EF', '#9ACDEF'],\n                       \"PSI\":   [\"#771e87\", '#782390', '#792999', '#7A2FA2', '#7A36AB', '#7A3DB3', '#7A45BA', '#7B54BB', '#7E62BC', '#8370BD'],\n                       \"SBAS\":  [\"#7f822f\", '#8C8936', '#938B3E', '#9B8D46', '#A28E4E', '#A89057', '#AA9064', '#AC9372', '#AE967E', '#B19C8B'],\n                       }\n\n        disp_cols  = [\"N\", \"E\", \"U\"]\n        rate_cols  = [\"vN\", \"vE\", \"vU\"]\n        disp_units = {\"U\": \"ver\", \"N\": \"hor\", \"E\": \"hor\"}\n\n        # CREATE STORAGE FOR STORE MIN AND MAX FOR SPECIFIC DATA TYPE VALUES\n        max_disp = {\"ver\": [], \"hor\": []}\n        min_disp = {\"ver\": [], \"hor\": []}\n        max_rate = []\n        min_rate = []\n\n        name_forward = \"Forward\"\n        name_backward = \"Backward\"\n\n        subplot_titles = [\"&lt;b&gt;North&lt;/b&gt;\", \"&lt;b&gt;East&lt;/b&gt;\", \"&lt;b&gt;Up&lt;/b&gt;\"] + [''] * cols\n\n        orbit_titles = [\"&lt;b&gt;Orbit \" + orbit + \"&lt;/b&gt;\" for orbit in self.orbits]\n        empty_titles = [\"\"] * (3 - self.number_of_orbits)\n\n        # CREATE GRID FOR SUBPLOTS AND TITLES\n        if \"PSI\" in self.sar_data_types or \"SBAS\" in self.sar_data_types:\n            rows += 1\n            subplot_titles.extend(orbit_titles + empty_titles)\n\n            specs_temp = [[{\"type\": \"xy\"}] * self.number_of_orbits + [None] * (3 - self.number_of_orbits)]\n            specs.extend(specs_temp)\n\n        if \"DInSAR\" in self.sar_data_types:\n            rows += 1\n            specs_temp = [[{\"type\": \"xy\"}] * self.number_of_orbits + [None] * (3 - self.number_of_orbits)]\n            specs.extend(specs_temp)\n            if \"PSI\" not in self.sar_data_types and \"SBAS\" not in self.sar_data_types:\n                subplot_titles.extend(orbit_titles + empty_titles)\n            else:\n                subplot_titles.extend([\"\"] * self.number_of_orbits)\n\n        specs_temp = [ [{\"type\": \"table\"}] + [None]*2]\n        specs.extend(specs_temp)\n\n        # CREATE FIG OBJECT\n        fig = sp.make_subplots(rows=rows, cols=cols,\n                               specs=specs,\n                               subplot_titles=subplot_titles, shared_xaxes=True, )\n\n        # GET TIMESTAMPS FOR GNSS, FORWARD, BACKWARD IF EXIST\n        gnss_timestamp = self.data_integration.data_dict['GNSS'].data.index\n        forward_timestamp = self.data_integration.forward_df_xe.index\n        backward_timestamp = self.data_integration.backward_df_xe.index if self.data_integration.backward_df_xe is not None else None\n        timestamp_min_value = self.find_min_value(gnss_timestamp, forward_timestamp,\n                                                  backward_timestamp) if backward_timestamp is not None else self.find_min_value(\n            gnss_timestamp, forward_timestamp)\n        timestamp_max_value = self.find_max_value(gnss_timestamp, forward_timestamp,\n                                                  backward_timestamp) if backward_timestamp is not None else self.find_max_value(\n            gnss_timestamp, forward_timestamp)\n\n        # CREATE DATES RANGE INCLUDING ADDITIONAL RANGE\n        additional_range_abs = additional_range * abs(timestamp_max_value - timestamp_min_value)\n        dates_range = [timestamp_min_value - additional_range_abs, timestamp_max_value + additional_range_abs]\n\n        # ITERATION FOR GNSS DATA\n        for i, coord in enumerate(disp_cols):\n            showlegend = True if i == 0 else False\n            disp_gnss = self.data_integration.data_dict['GNSS'].data[coord]\n            disp_forward = self.data_integration.forward_df_xe[disp_cols[i]]\n            rate_forward = self.data_integration.forward_df_xe[rate_cols[i]] * 1000\n            disp_type = disp_units.get(coord, None)\n            if disp_type:\n                max_disp[disp_type].extend([max(disp_gnss), max(disp_forward)])\n                min_disp[disp_type].extend([min(disp_gnss), min(disp_forward)])\n            max_rate.extend([max(rate_forward)])\n            min_rate.extend([min(rate_forward)])\n\n            fig.add_trace(\n                go.Scatter(x=gnss_timestamp, y=disp_gnss, mode='markers', name='', showlegend=showlegend,\n                           legendgroup=\"GNSS\", legendgrouptitle_text=\"GNSS\", marker=dict(color=data_colors[\"GNSS\"], size = 8)), \n                row=1, col=i + 1)\n            fig.add_trace(\n                go.Scatter(x=forward_timestamp, y=disp_forward, mode='lines', name=name_forward, showlegend=showlegend,\n                           legendgroup=\"kalman\", legendgrouptitle_text=\"Kalman\",\n                           line=dict(width=5, color=data_colors[\"forward\"])), row=1, col=i + 1)\n            fig.add_trace(\n                go.Scatter(x=forward_timestamp, y=rate_forward, mode='lines', name=name_forward, showlegend=False,\n                           legendgroup=\"kalman\", legendgrouptitle_text=\"Kalman\",\n                           line=dict(width=5, color=data_colors[\"forward\"])), row=2, col=i + 1)\n\n            self.add_vline(fig, timestamp_min_value, timestamp_max_value, row=1, col=i + 1)\n            self.add_vline(fig, timestamp_min_value, timestamp_max_value, row=2, col=i + 1)\n\n            if self.data_integration.backward_df_xe is not None:\n                disp_backward = self.data_integration.backward_df_xe[disp_cols[i]]\n                rate_backward = self.data_integration.backward_df_xe[rate_cols[i]] * 1000\n\n                if disp_type:\n                    max_disp[disp_type].extend([max(disp_backward)])\n                    min_disp[disp_type].extend([min(disp_backward)])\n                max_rate.extend([max(rate_backward)])\n                min_rate.extend([min(rate_backward)])\n\n                fig.add_trace(go.Scatter(x=backward_timestamp, y=disp_backward, mode='lines', name=name_backward,\n                                         showlegend=showlegend, legendgroup=\"kalman\",\n                                         line=dict(width=4, color=data_colors[\"backward\"])), row=1, col=i + 1)\n                fig.add_trace(go.Scatter(x=backward_timestamp, y=rate_backward, mode='lines', name=name_backward,\n                                         showlegend=False, legendgroup=\"kalman\",\n                                         line=dict(width=4, color=data_colors[\"backward\"])), row=2, col=i + 1)\n\n        # ITERATION FOR SAR DATA - only DSP\n        if self.data_integration.data_dict.get('SAR') is not None:\n            for technique, data in self.data_integration.data_dict.get('SAR').items():\n                row = 3\n                if technique == \"DInSAR\" and (\"SBAS\" in self.sar_data_types or \"PSI\" in self.sar_data_types):\n                    row = 4\n                if technique == \"DInSAR\":\n                    for orbit in set(self.orbits).difference(set(data.keys())):\n                        col = self.orbits.index(orbit) + 1 if self.number_of_orbits &gt; 1 else 1\n                        fig.add_trace(\n                            go.Scatter(x=dates_range, y=pd.Series(dtype=object), mode='markers',\n                                       showlegend=False), row=row, col=col)\n                        self.add_vline(fig, timestamp_min_value, timestamp_max_value, row=row, col=col)\n                else:\n                    for orbit in set(self.orbits).difference(set(data.keys())):\n                        col = self.orbits.index(orbit) + 1 if self.number_of_orbits &gt; 1 else 1\n                        fig.add_trace(\n                            go.Scatter(x=dates_range, y=pd.Series(dtype=object), mode='markers',\n                                       showlegend=False), row=row, col=col)\n                        self.add_vline(fig, timestamp_min_value, timestamp_max_value, row=row, col=col)\n                if isinstance(data, dict):\n                    inner_keys = {}\n                    for orbit, orbit_data in data.items():\n                        col = self.orbits.index(orbit) + 1 if self.number_of_orbits &gt; 1 else 1\n                        if isinstance(orbit_data, dict):\n\n                            if len(self.data_integration.number_of_SAR_elements[technique]) == 1:\n                                color_index = [0]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 2:\n                                color_index = [0, 4]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 3:\n                                color_index = [0, 4, 8]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 4:\n                                color_index = [0, 2, 4, 8]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 5:\n                                color_index = [0, 2, 4, 6, 8]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 6:\n                                color_index = [0, 1, 2, 4, 6, 8]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 7:\n                                color_index = [0, 1, 2, 3, 4, 6, 8]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 8:\n                                color_index = [0, 1, 2, 3, 4, 5, 6, 8]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 9:\n                                color_index = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n                            elif len(self.data_integration.number_of_SAR_elements[technique]) == 10:\n                                color_index = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n                            for _, (subkey, subdata) in enumerate(orbit_data.items()):\n                                if subkey in inner_keys.keys():\n                                    showlegend = False\n                                else:\n                                    showlegend = True\n                                    inner_keys[subkey]=color_index[len(inner_keys)]\n\n                                color_for_data = data_colors[subdata.type][inner_keys[subkey]]\n\n                                rates = 1000 if subdata.type == \"DInSAR\" else 1\n                                if subdata.type == \"DInSAR\":\n                                    max_rate.extend([max(subdata.data[\"DSP\"] * rates)])\n                                    min_rate.extend([min(subdata.data[\"DSP\"] * rates)])\n                                else:\n                                    max_disp[\"ver\"].extend([max(subdata.data[\"DSP\"] * rates)])\n                                    min_disp[\"ver\"].extend([min(subdata.data[\"DSP\"] * rates)])\n\n                                fig.add_trace(\n                                    go.Scatter(x=subdata.data.index, y=subdata.data[\"DSP\"] * rates, mode='markers',\n                                               name=subdata.sublabel,\n                                               showlegend=showlegend, legendgroup=subdata.type,\n                                               legendgrouptitle_text=subdata.type,\n                                               marker=dict(color=color_for_data, size = 8)), row=row, col=col)\n                                self.add_vline(fig, timestamp_min_value, timestamp_max_value, row=row, col=col)\n                        else:\n                            showlegend = True if self.orbits.index(orbit) == 0 else False\n                            rates = 1000 if orbit_data.type == \"DInSAR\" else 1\n                            if orbit_data.type == \"DInSAR\":\n                                max_rate.extend([max(orbit_data.data[\"DSP\"] * rates)])\n                                min_rate.extend([min(orbit_data.data[\"DSP\"] * rates)])\n                            else:\n                                max_disp[\"ver\"].extend([max(orbit_data.data[\"DSP\"] * rates)])\n                                min_disp[\"ver\"].extend([min(orbit_data.data[\"DSP\"] * rates)])\n\n                            fig.add_trace(\n                                go.Scatter(x=orbit_data.data.index, y=orbit_data.data[\"DSP\"] * rates, mode='markers',\n                                           name='',\n                                           showlegend=showlegend, legendgroup=orbit_data.type,\n                                           legendgrouptitle_text=orbit_data.type,\n                                           marker=dict(color=data_colors[orbit_data.type][0], size = 8)), row=row, col=col)\n                            self.add_vline(fig, timestamp_min_value, timestamp_max_value, row=row, col=col)\n                    if technique == \"DInSAR\":\n                        for orbit in set(self.orbits).difference(set(data.keys())):\n                            col = self.orbits.index(orbit) + 1 if self.number_of_orbits &gt; 1 else 1\n                            fig.add_trace(\n                                go.Scatter(x=dates_range, y=pd.Series(dtype=object), mode='markers',\n                                           showlegend=False), row=row, col=col)\n                            self.add_vline(fig, timestamp_min_value, timestamp_max_value, row=row, col=col)\n        if self.data_integration.mean_data_dict.get(\"DInSAR\") is not None:\n            row = 4 if (\"SBAS\" in self.sar_data_types or \"PSI\" in self.sar_data_types) else 3\n            for orbit, orbit_data in self.data_integration.mean_data_dict.get(\"DInSAR\").items():\n                col = self.orbits.index(orbit) + 1 if self.number_of_orbits &gt; 1 else 1\n\n                max_rate.extend([max(orbit_data['forward_mean'])])\n                min_rate.extend([min(orbit_data['forward_mean'])])\n\n                fig.add_trace(go.Scatter(x=orbit_data['forward_mean'].index, y=orbit_data['forward_mean'], mode='lines',\n                                         name=name_forward, showlegend=False, legendgroup=\"kalman\",\n                                         line=dict(width=5, color=data_colors[\"forward\"])), row=row, col=col)\n                if orbit_data.get('backward_mean') is not None:\n                    max_rate.extend([max(orbit_data['backward_mean'])])\n                    min_rate.extend([min(orbit_data['backward_mean'])])\n\n                    fig.add_trace(\n                        go.Scatter(x=orbit_data['backward_mean'].index, y=orbit_data['backward_mean'], mode='lines',\n                                   name=name_backward, showlegend=False, legendgroup=\"kalman\",\n                                   line=dict(width=4, color=data_colors[\"backward\"])), row=row, col=col)\n\n        if self.data_integration.mean_data_dict.get(\"SAR_SBAS_PSI_MEAN\") is not None:\n            row = 3\n            for orbit, orbit_data in self.data_integration.mean_data_dict.get(\"SAR_SBAS_PSI_MEAN\").items():\n                col = self.orbits.index(orbit) + 1 if self.number_of_orbits &gt; 1 else 1\n\n                max_disp[\"ver\"].extend([max(orbit_data['forward_mean'].dropna())])\n                min_disp[\"ver\"].extend([min(orbit_data['forward_mean'].dropna())])\n\n                fig.add_trace(\n                    go.Scatter(x=orbit_data['forward_mean'].dropna().index, y=orbit_data['forward_mean'].dropna(),\n                               mode='lines', name=name_forward, showlegend=False, legendgroup=\"kalman\",\n                               line=dict(width=5, color=data_colors[\"forward\"])), row=row, col=col)\n\n                if orbit_data.get('backward_mean') is not None:\n                    max_disp[\"ver\"].extend([max(orbit_data['backward_mean'].dropna())])\n                    min_disp[\"ver\"].extend([min(orbit_data['backward_mean'].dropna())])\n\n                    fig.add_trace(\n                        go.Scatter(x=orbit_data['backward_mean'].dropna().index, y=orbit_data['backward_mean'].dropna(),\n                                   mode='lines', name=name_backward, showlegend=False, legendgroup=\"kalman\",\n                                   line=dict(width=4, color=data_colors[\"backward\"])), row=row, col=col)\n\n        # ORDER OF TRACES FOR LEGEND\n        new_data = [trace for trace in fig.data if trace.name != name_forward] + [trace for trace in fig.data if\n                                                                                  trace.name == name_forward]\n        new_data = [trace for trace in new_data if trace.name != name_backward] + [trace for trace in new_data if\n                                                                                   trace.name == name_backward]\n        fig.data = new_data\n\n        # CREATE Y RANGE\n        max_disp_ver = self.find_max_value(max_disp[\"ver\"])\n        max_disp_hor = self.find_max_value(max_disp[\"hor\"])\n        min_disp_ver = self.find_min_value(min_disp[\"ver\"])\n        min_disp_hor = self.find_min_value(min_disp[\"hor\"])\n\n        min_rate = self.find_min_value(min_rate)\n        max_rate = self.find_max_value(max_rate)\n\n        range_ver = [min_disp_ver - additional_range * abs(max_disp_ver - min_disp_ver),\n                     max_disp_ver + additional_range * abs(max_disp_ver - min_disp_ver)]\n        range_hor = [min_disp_hor - additional_range * abs(max_disp_hor - min_disp_hor),\n                     max_disp_hor + additional_range * abs(max_disp_hor - min_disp_hor)]\n        range_rate = [min_rate - additional_range * abs(max_rate - min_rate),\n                      max_rate + additional_range * abs(max_rate - min_rate)]\n\n        min_range_value = 0.05\n        if min(range_ver[0], range_hor[0]) &gt; -min_range_value and max(range_ver[1], range_hor[1]) &lt; min_range_value:\n            range_ver = [-min_range_value, min_range_value]\n            range_hor = [-min_range_value, min_range_value]\n\n        # UPDATE X AND Y RANGE FOR GNSS DISPLACEMENT    \n        fig.update_yaxes(range=range_hor,   row=1, col=1)\n        fig.update_yaxes(range=range_hor,   row=1, col=2)\n        fig.update_yaxes(range=range_ver,   row=1, col=3)\n        fig.update_xaxes(range=dates_range, row=1)\n\n        # UPDATE X AND Y RANGE FOR GNSS RATE\n        fig.update_xaxes(range=dates_range, row=2)\n        fig.update_yaxes(range=range_rate,  row=2)\n\n        # UPDATE X AND Y RANGE FOR DInSAR OR PSI&amp; SBAS\n        fig.update_xaxes(range=dates_range, row=3)\n        if rows == 4 and \"DInSAR\" in self.sar_data_types: \n            fig.update_yaxes(range=range_rate, row=3)\n        else:\n            fig.update_yaxes(range=range_ver,  row=3)\n\n        # UPDATE X AND Y RANGE FOR DInSAR WITH PSI&amp; SBAS\n        fig.update_xaxes(range=dates_range, row=4)\n        fig.update_yaxes(range=range_rate,  row=4)\n\n\n        # UPDATE EVERY ANNOTATIONS -&gt; FONT SIZE\n        fig.update_annotations(font_size=24, font_color = 'black')\n\n        layout_settings = {\n            'title_text': f\"MultiDEFusion: &lt;b&gt;{self.data_integration.station}&lt;/b&gt;\",\n            'font': dict(family='Helvetica', size=20, color = 'black'),\n            'height': 1400,\n            'width': 1600,\n            'showlegend': True,\n            'margin' : dict(r=10,b=10),\n            'legend': dict(orientation='h',\n                           # groupclick=\"toggleitem\",\n                           itemsizing='constant',\n                           bordercolor='black',\n                           borderwidth=1,\n                           yanchor=\"bottom\",\n                           y=vertical_postition['1'][1]+shift_legend,\n                           xanchor=\"right\",\n                           x=1),\n            'plot_bgcolor': 'white',\n        }\n\n        axis_settings = {\n            'gridcolor': 'lightgray',\n            'ticks': 'inside',\n            'linecolor': 'black',\n            'mirror': 'ticks',\n            'color': 'black',\n            'minor': dict(gridcolor='lightgray', gridwidth=0.1, ticks='inside', tickcolor='black'),\n            'automargin': 'height+width+left+right',    \n        }\n\n        # UPDATE POSITION AND SETTINGS FOR GNSS DISPLACEMENT\n        for i in range(1, 4):\n            layout_settings[f'xaxis{i}'] = dict(domain=subplots_postition[\"3\"][i - 1], showticklabels=False, hoverformat = '%d %b %Y',\n                                                **axis_settings)\n            layout_settings[f'yaxis{i}'] = dict(\n                domain=vertical_postition['1'],\n                title='&lt;b&gt;Displacement [m]&lt;/b&gt;' if i == 1 else '',\n                showticklabels=True if i == 1 or i == 3 else False,\n                tickformat='.2f',\n                zeroline=True,\n                zerolinewidth=2,\n                zerolinecolor='lightgray',\n                hoverformat = '.3f',\n                **axis_settings\n            )\n        # UPDATE POSITION AND SETTINGS FOR GNSS RATE\n        for i in range(1, 4):\n            layout_settings[f'xaxis{i + 3}'] = dict(domain=subplots_postition[\"3\"][i - 1],\n                                                    showticklabels=True, tickformat='%b&lt;br&gt;%Y', hoverformat = '%d %b %Y', **axis_settings)\n            layout_settings[f'yaxis{i + 3}'] = dict(\n                domain=vertical_postition['2'],\n                title='&lt;b&gt;Rate&lt;br&gt;[mm/day]&lt;/b&gt;' if i == 1 else '',\n                showticklabels=True if i == 1 or i == 3 else False,\n                tickformat='.2f',\n                zeroline=True,\n                zerolinewidth=2,\n                zerolinecolor='lightgray',\n                hoverformat = '.1f',\n                **axis_settings\n            )\n\n        if self.number_of_orbits == 3:\n            subplots_postition_orbits = str(self.number_of_orbits) + \"*\"\n        else:\n            subplots_postition_orbits = str(self.number_of_orbits)\n\n        table_vertical  = table_postition['GNSS']\n        table_title_pos = shift_title_fig*5\n        picker_position = custom_position['GNSS']\n\n        if rows == 4:\n            if \"DInSAR\" in self.sar_data_types and len(self.sar_data_types) == 1:\n                table_vertical  = table_postition['DInSAR']\n                table_title_pos = shift_title_fig*5\n                picker_position = custom_position['DInSAR']\n                for i in range(1, len(self.orbits) + 1):\n                    layout_settings[f'xaxis{i + 6}'] = dict(\n                        domain=subplots_postition[subplots_postition_orbits][i - 1],\n                        tickformat='%b&lt;br&gt;%Y', hoverformat = '%d %b %Y', **axis_settings)\n                    layout_settings[f'yaxis{i + 6}'] = dict(\n                        domain=vertical_postition['3*'],\n                        title='&lt;b&gt;Rate&lt;br&gt;[mm/day]&lt;/b&gt;' if i == 1 else '',\n                        showticklabels=True if i == 1 else False,\n                        tickformat='.2f',\n                        zeroline=True,\n                        zerolinewidth=2,\n                        zerolinecolor='lightgray',\n                        hoverformat = '.1f',\n                        **axis_settings\n                    )\n\n        if rows == 4:\n            if \"DInSAR\" not in self.sar_data_types and len(self.sar_data_types) &gt; 0:\n                table_vertical  = table_postition['SBAS']\n                table_title_pos = shift_title_fig\n                picker_position = custom_position['SBAS']\n                for i in range(1, len(self.orbits) + 1):\n                    layout_settings[f'xaxis{i + 6}'] = dict(\n                        domain=subplots_postition[subplots_postition_orbits][i - 1],\n                        tickformat='%b&lt;br&gt;%Y', hoverformat = '%d %b %Y', **axis_settings)\n                    layout_settings[f'yaxis{i + 6}'] = dict(\n                        domain=vertical_postition['3'],\n                        title='&lt;b&gt;Displacement [m]&lt;/b&gt;' if i == 1 else '',\n                        showticklabels=True if i == 1 else False,\n                        tickformat='.2f',\n                        zeroline=True,\n                        zerolinewidth=2,\n                        zerolinecolor='lightgray',\n                        hoverformat = '.3f',\n                        **axis_settings\n                    )\n\n        if rows == 5:\n            table_vertical  = table_postition['MSAR']\n            table_title_pos = shift_title_fig\n            picker_position = custom_position['MSAR']\n            for i in range(1, len(self.orbits) + 1):\n                layout_settings[f'xaxis{i + 6}'] = dict(domain=subplots_postition[subplots_postition_orbits][i - 1],\n                                                        showticklabels=False, hoverformat = '%d %b %Y', **axis_settings)\n                layout_settings[f'yaxis{i + 6}'] = dict(\n                    domain=vertical_postition['3'],\n                    title='&lt;b&gt;Displacement [m]&lt;/b&gt;' if i == 1 else '',\n                    showticklabels=True if i == 1 else False,\n                    tickformat='.2f',\n                    zeroline=True,\n                    zerolinewidth=2,            \n                    zerolinecolor='lightgray',\n                    hoverformat = '.3f',\n                    **axis_settings\n                )\n                layout_settings[f'xaxis{i + 6 + len(self.orbits)}'] = dict(\n                    domain=subplots_postition[subplots_postition_orbits][i - 1],\n                    tickformat='%b&lt;br&gt;%Y', hoverformat = '%d %b %Y', **axis_settings)\n                layout_settings[f'yaxis{i + 6 + len(self.orbits)}'] = dict(\n                    domain=vertical_postition['4'],\n                    title='&lt;b&gt;Rate&lt;br&gt;[mm/day]&lt;/b&gt;' if i == 1 else '',\n                    showticklabels=True if i == 1 else False,\n                    tickformat='.2f',\n                    zeroline=True,\n                    zerolinewidth=2,\n                    zerolinecolor='lightgray',\n                    hoverformat = '.1f',\n                    **axis_settings\n                )\n\n        fig.update_layout(**layout_settings)\n\n        fig.update_layout(xaxis=dict(matches='x'),\n                          xaxis4=dict(matches='x'),\n                          xaxis2=dict(matches='x2'),\n                          xaxis5=dict(matches='x2'),\n                          xaxis3=dict(matches='x3'),\n                          xaxis6=dict(matches='x3'),\n                          hovermode='x unified',\n                          )\n\n        fig.update_traces(hovertemplate='%{y} %{xother}')\n\n        if self.data_integration.backward_df_xe is not None:\n            max_total_values = self.find_max_total(self.data_integration.backward_df_xe)\n            mean_rate = self.data_integration.backward_df_xe.mean()\n            kalman_col = '  Kalman&lt;br&gt;Backward'\n        else:\n            kalman_col = ' Kalman&lt;br&gt;Forward'\n            max_total_values = self.find_max_total(self.data_integration.forward_df_xe)\n            mean_rate = self.data_integration.forward_df_xe.mean()\n\n\n        fig.add_trace(go.Table(\n            header=dict(values=[f\"{kalman_col}\", '     Max&lt;br&gt;  DEF [m]',  'Mean rate&lt;br&gt;  [m/year]'],\n                        align='center',\n                        fill_color = 'rgb(189, 215, 231)',\n                        line_color='black',\n                        font = dict(color = 'black', size = 24)),\n            cells =dict(values=[['North', 'East', 'Up'], max_total_values[disp_cols].apply(lambda x: f'{x:.3f}'), (mean_rate[rate_cols]*365.25).apply(lambda x: f'{x:.3f}')], \n                        align='center', \n                        height=41.5,\n                        fill_color = 'white',\n                        line_color='black',\n                        font = dict(color = 'black', size = 24))\n            ), row=rows, col=1)\n\n        fig.update_traces(\n            domain={'y': table_vertical, 'x': subplots_postition['3'][1]},\n            selector={'type': 'table'})\n\n        for annotation, domain in zip(fig['layout']['annotations'][:3], subplots_postition[\"3\"]):\n            annotation['x'] = (domain[0] + domain[1]) / 2\n            annotation['y'] = vertical_postition['1'][1]+shift_title_fig\n\n        if self.number_of_orbits == 3:\n            subplots_pos = subplots_postition[\"3*\"]\n        elif self.number_of_orbits in [1, 2]:\n            subplots_pos = subplots_postition[str(self.number_of_orbits)]\n\n        if rows &gt; 3:\n            for annotation, domain in zip(fig['layout']['annotations'][3:], subplots_pos):\n                annotation['x'] = (domain[0] + domain[1]) / 2\n                annotation['y'] = vertical_postition['3'][1]+shift_title_fig\n\n        fig.add_annotation(\n            x = mean(subplots_postition['3'][1]),\n            y = table_vertical[1]+table_title_pos,\n            xref = \"paper\",\n            yref = \"paper\",\n            showarrow=False,\n            font_size = 25,\n            font_color = 'black',\n            text = \"&lt;b&gt;Relevant values&lt;/b&gt;\")\n\n        fig.add_annotation(\n            x = mean(subplots_postition['3'][1]),\n            y = 0.005,\n            xref = \"paper\",\n            yref = \"paper\",\n            showarrow=False,\n            font_family = 'Helvetica',\n            font_size = 15,\n            font_color = 'black',\n            text = \"The Kalman filter displacements and rates shown for InSAR orbits in the Line of Sight (LOS) domain were determined based on the mean heading and incidence angle values.\")\n\n        #DASH PART\n        app = dash.Dash(__name__)\n        container1 = html.Div([\n            dcc.Graph(\n                id='subplot-graph',\n                figure = fig,\n                config = {\n                    'toImageButtonOptions': {'format': 'svg', 'filename': 'MultiDEFusion_'+self.data_integration.station},\n                    'displaylogo': False,\n                    'showEditInChartStudio': True,\n                    'plotlyServerURL': \"https://chart-studio.plotly.com\",\n                    'modeBarButtonsToRemove': ['select', 'lasso2d', 'autoScale'],\n                    'modeBarButtonsToAdd': ['drawline', 'drawcircle', 'drawrect', 'eraseshape', 'sendDataToCloud']\n                    }\n                ),\n            ])\n\n        container2 = html.Div([\n            html.H2('Customise dates range',\n                    style={'fontsize': '25px',\n                           'margin-bottom': '5px',\n                           'margin-left': '65px',}),\n\n\t\t    dcc.DatePickerRange(\n\t\t\t\tid='date_range',\n\t\t\t\tdisplay_format='DD/MM/YYYY',\n\t\t\t\tshow_outside_days = True,\n\t\t\t\tstart_date=dates_range[0],\n\t\t\t\tend_date=dates_range[1],\n                number_of_months_shown=2,\n                style={'border': '2px solid black',\n                       'margin-bottom': '10px',\n                       'vertical-align': 'middle',\n                       'display': 'inline-block'},\n\t\t\t),\n\n            html.Button('Restore default',\n                        id='reset_dates',\n                        n_clicks=0,\n                        style={'fontsize': '20px',\n                               'font-weight': 'bold',\n                               'height': '45px', \n                               'width': '100px', \n                               'margin-left': '10px',\n                               'margin-bottom': '10px',\n                               'vertical-align': 'middle',\n                               'display': 'inline-block'}),\n\n            html.Div(id='output_x_range',\n                     style={'fontsize': '15px',\n                            'color': 'red'}),\n\n            html.H2('Customise values range',\n                    style={'fontsize': '25px',\n                           'margin-bottom': '5px',\n                           'margin-left': '60px'}),\n\n            html.Div([\n            dcc.Dropdown(\n                id = 'show_or_hide',\n                options=[\n                    {'label': 'Horizontal range [m]', 'value': 'hor'},\n                    {'label': 'Vertical &amp; LOS range [m]', 'value': 'ver'},\n                    {'label': 'Rate range [mm/day]', 'value': 'rate'}],\n                value = 'hor',\n                clearable = False,\n                style={'width': '286px',\n                       'font-size': '20px', \n                       'border': '0.2px solid black',\n                       'border-radius': '0',\n                       'height': '40px', \n                       'vertical-align': 'middle',\n                       'display': 'inline-block'},\n            ),\n\n            html.Button('Sync ranges',\n                        id='fit_ranges',\n                        n_clicks=0,\n                        style={'fontsize': '20px',\n                               'font-weight': 'bold',\n                               'height': '38px', \n                               'width': '100px',\n                               'margin-left': '10px',\n                               'vertical-align': 'middle',\n                               'display': 'inline-block'}\n            ),\n            ]),\n\n            html.Div([\n                html.Div(id = 'hor_options', children=[\n                dcc.Input(\n                id = 'hor_min',\n                placeholder = 'Minimum value',\n                type='number',\n                step = 0.001,\n                value = float(\"{:.3f}\".format(range_hor[0])),\n                style = {'font-size': '17px',\n                         'border': '1px solid black',\n                         'height': '30px', \n                         'width': '138px'}\n                ),\n\n                dcc.Input(\n                id = 'hor_max',\n                placeholder = 'Maximum value',\n                type='number',\n                step = 0.001,\n                value = float(\"{:.3f}\".format(range_hor[1])),\n                style = {'font-size': '17px',\n                         'border': '1px solid black',\n                         'height': '30px', \n                         'width': '138px'}\n                )\n            ],\n            style={'display': 'block',\n                   'margin-top': '5px',\n                   'margin-bottom': '10px'},\n            ),\n\n            html.Div(id = 'ver_options', children=[\n                dcc.Input(\n                id = 'ver_min',\n                placeholder = 'Minimum value',\n                type='number',\n                step = 0.001,\n                value = float(\"{:.3f}\".format(range_ver[0])),\n                style = {'font-size': '17px',\n                         'border': '1px solid black',\n                         'height': '30px', \n                         'width': '138px'}\n                ),\n\n                dcc.Input(\n                id = 'ver_max',\n                placeholder = 'Maximum value',\n                type='number',\n                step = 0.001,\n                value = float(\"{:.3f}\".format(range_ver[1])),\n                style = {'font-size': '17px',\n                         'border': '1px solid black',\n                         'height': '30px', \n                         'width': '138px'}\n                )\n            ],\n            style={'display': 'block',\n                   'margin-bottom': '10px'},\n            ),\n\n            html.Div(id = 'rate_options', children=[\n                dcc.Input(\n                id = 'rate_min',\n                placeholder = 'Minimum value',\n                type='number',\n                step = 0.1,\n                value = float(\"{:.1f}\".format(range_rate[0])),\n                style = {'font-size': '17px',\n                         'border': '1px solid black',\n                         'height': '30px', \n                         'width': '138px'}\n                ),\n\n                dcc.Input(\n                id = 'rate_max',\n                placeholder = 'Maximum value',\n                type='number',\n                step = 0.1,\n                value = float(\"{:.1f}\".format(range_rate[1])),\n                style = {'font-size': '17px', \n                         'border': '1px solid black',\n                         'height': '30px', \n                         'width': '138px'}\n                ),\n\n            ],\n            style={'display': 'block',\n                   'margin-bottom': '10px'},\n            )\n            ], style={'display': 'inline-block',\n                      'vertical-align': 'middle'}),\n\n\n            html.Button('Restore default',\n                        id='reset_values',\n                        n_clicks=0,\n                        style={'fontsize': '20px',\n                               'font-weight': 'bold',\n                               'height': '38px', \n                               'width': '100px',\n                               'margin-left': '10px',\n                               'display': 'inline-block',\n                               'vertical-align': 'middle',}\n            ),\n\n\n            html.Div(id='output_y_range',\n                     style={'fontsize': '15px',\n                            'color': 'red'}\n                     ),\n        ], \n        style={'position': 'absolute', \n               'top': f'{picker_position}px', \n               'left': '145px', \n               'width': 'fit-content',\n               'fontFamily': 'Helvetica',\n               'color': 'black', \n               }\n        )\n\n        app.layout = html.Div([container1, container2])\n\n        @app.callback(\n            Output('hor_options', 'style'),\n            Output('ver_options', 'style'),\n            Output('rate_options', 'style'),\n            Input('show_or_hide', 'value'))\n\n        def show_hide_element(visibility_state):\n            if visibility_state == 'hor':\n                return {'display': 'block'}, {'display': 'none'}, {'display': 'none'}\n            elif visibility_state == 'ver':\n                return {'display': 'none'}, {'display': 'block'}, {'display': 'none'}\n            else:\n                return {'display': 'none'}, {'display': 'none'}, {'display': 'block'}\n\n        @app.callback(\n           Output('subplot-graph', 'figure'),\n           Output('output_x_range', 'children'),\n           Output('output_y_range', 'children'),\n           [Input('date_range', 'start_date'),\n            Input('date_range', 'end_date'),\n            Input('hor_min', 'value'), \n            Input('hor_max', 'value'),\n            Input('ver_min', 'value'), \n            Input('ver_max', 'value'),\n            Input('rate_min', 'value'), \n            Input('rate_max', 'value')]\n           )\n\n        def update_subplot(start_date, end_date, hor_min, hor_max, ver_min, ver_max, rate_min, rate_max):\n\n            warning_dates = None\n            if end_date &lt;= start_date and end_date is not None and start_date is not None:\n                warning_dates = 'The end date must be greater than the start date!'\n            if warning_dates == None:\n                fig.update_xaxes(range=(start_date, end_date))\n\n            warning_values = None\n            if hor_min is not None and hor_max is not None and hor_max &lt;= hor_min:\n                warning_values = 'Maximum value must be greater than minimum!'\n            if ver_min is not None and ver_max is not None and ver_max &lt;= ver_min:\n                warning_values = 'Maximum value must be greater than minimum!'\n            if rate_min is not None and rate_max is not None and rate_max &lt;= rate_min:\n                warning_values = 'Maximum value must be greater than minimum!'\n            if warning_values == None:                \n                # UPDATE Y RANGE FOR GNSS DISPLACEMENT    \n                fig.update_yaxes(range=[hor_min, hor_max], row=1, col=1)\n                fig.update_yaxes(range=[hor_min, hor_max], row=1, col=2)\n                fig.update_yaxes(range=[ver_min, ver_max], row=1, col=3)\n\n                # UPDATE X AND Y RANGE FOR GNSS RATE\n                fig.update_yaxes(range=[rate_min, rate_max], row=2)\n\n                # UPDATE X AND Y RANGE FOR DInSAR OR PSI &amp; SBAS\n                if rows == 4 and \"DInSAR\" in self.sar_data_types: \n                    fig.update_yaxes(range=[rate_min, rate_max], row=3)\n                else:\n                    fig.update_yaxes(range=[ver_min, ver_max], row=3)\n\n                # UPDATE X AND Y RANGE FOR DInSAR WITH PSI&amp; SBAS\n                fig.update_yaxes(range=[rate_min, rate_max], row=4)\n\n            return fig, warning_dates, warning_values\n\n\n        @app.callback(\n            Output('date_range', 'start_date'),\n            Output('date_range', 'end_date'),\n            Input('reset_dates', 'n_clicks')\n        )\n\n        def reset_date_range(n_clicks):\n            return dates_range[0], dates_range[1]\n\n        @app.callback(\n          [Output('hor_min', 'value'), \n            Output('hor_max', 'value'),\n            Output('ver_min', 'value'), \n            Output('ver_max', 'value'),\n            Output('rate_min', 'value'), \n            Output('rate_max', 'value')],\n            Input('reset_values', 'n_clicks'),\n            Input('fit_ranges', 'n_clicks'),\n            prevent_initial_call=True\n          )\n\n        def reset_fit_range(btn1, btn2):\n            min_range = float(\"{:.3f}\".format(min(range_ver[0], range_hor[0])))\n            max_range = float(\"{:.3f}\".format(max(range_ver[1], range_hor[1])))\n            if 'reset_values' == ctx.triggered_id:\n                return float(\"{:.3f}\".format(range_hor[0])), float(\"{:.3f}\".format(range_hor[1])), float(\"{:.3f}\".format(range_ver[0])), float(\"{:.3f}\".format(range_ver[1])),  float(\"{:.1f}\".format(range_rate[0])), float(\"{:.1f}\".format(range_rate[1]))\n            if 'fit_ranges' == ctx.triggered_id:\n                return min_range, max_range, min_range, max_range, float(\"{:.1f}\".format(range_rate[0])), float(\"{:.1f}\".format(range_rate[1]))\n\n\n        print(\"MultiDEFusion procedure accomplished.\\n\")\n        if __name__ == 'multidefusion.results':\n            app.run_server(debug=True, host=\"localhost\", port=self.data_integration.port, use_reloader=False);\n            webbrowser.open(f'http://localhost:{self.data_integration.port}')\n            print(\"___________________________________________________________\")\n</code></pre>"},{"location":"results/#multidefusion.results.Figures.find_max_total","title":"<code>find_max_total(df)</code>  <code>staticmethod</code>","text":"<p>Find the maximum value (positive or negative) from each column from a set of dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>dataframe</p> required <p>Returns:</p> Type Description <p>Maximum value from from each column from a set of dataframe.</p> Source code in <code>multidefusion\\results.py</code> <pre><code>@staticmethod\ndef find_max_total(df):\n    \"\"\"\n    Find the maximum value (positive or negative) from each column from a set of dataframe.\n\n    Args:\n        df: dataframe\n\n    Returns:\n        Maximum value from from each column from a set of dataframe.\n    \"\"\"\n    return df.apply(lambda col: col.max() if col.max() &gt;= -col.min() else col.min(), axis=0)\n</code></pre>"},{"location":"results/#multidefusion.results.Figures.find_max_value","title":"<code>find_max_value(*lists)</code>  <code>staticmethod</code>","text":"<p>Find the maximum value from a set of lists.</p> <p>Parameters:</p> Name Type Description Default <code>*lists</code> <p>Variable number of lists to find the maximum value from.</p> <code>()</code> <p>Returns:</p> Type Description <p>Maximum value from the provided lists.</p> Source code in <code>multidefusion\\results.py</code> <pre><code>@staticmethod\ndef find_max_value(*lists):\n    \"\"\"\n    Find the maximum value from a set of lists.\n\n    Args:\n        *lists: Variable number of lists to find the maximum value from.\n\n    Returns:\n        Maximum value from the provided lists.\n    \"\"\"\n    if not lists:\n        return None\n    flattened_list = [item for sublist in lists for item in sublist]\n    return max(flattened_list)\n</code></pre>"},{"location":"results/#multidefusion.results.Figures.find_min_value","title":"<code>find_min_value(*lists)</code>  <code>staticmethod</code>","text":"<p>Find the minimum value from a set of lists.</p> <p>Parameters:</p> Name Type Description Default <code>*lists</code> <p>Variable number of lists to find the minimum value from.</p> <code>()</code> <p>Returns:</p> Type Description <p>Minimum value from the provided lists.</p> Source code in <code>multidefusion\\results.py</code> <pre><code>@staticmethod\ndef find_min_value(*lists):\n    \"\"\"\n    Find the minimum value from a set of lists.\n\n    Args:\n        *lists: Variable number of lists to find the minimum value from.\n\n    Returns:\n        Minimum value from the provided lists.\n    \"\"\"\n    if not lists:\n        return None\n    flattened_list = [item for sublist in lists for item in sublist]\n    return min(flattened_list)\n</code></pre>"},{"location":"trial/","title":"Instructions for downloading a trial repository for the MultiDEFusion library","text":"<p>The following repository has been created as a trial dataset for the MultiDEFusion library. The dataset can be downloaded from GitHub or Zenodo platform.</p>"},{"location":"trial/#cloning-the-repository-from-github","title":"Cloning the repository from GitHub","text":"<ol> <li>Open a terminal or command prompt.</li> <li> <p>Use the git clone command to clone the repository to your device: <code>git clone https://github.com/damiantondas/multidefusion_trial.git</code></p> </li> <li> <p>The repository will be downloaded to the current directory. You can now navigate to the repository directory using the <code>cd</code> command: <code>cd multidefusion_trial</code></p> </li> </ol>"},{"location":"trial/#cloning-the-repository-from-zenodo","title":"Cloning the repository from Zenodo","text":"<ol> <li> <p>Go to the Zenodo repository: </p> </li> <li> <p>Download the multidefusion_trial.zip folder.</p> </li> <li> <p>Unzip the folder.</p> </li> </ol>"},{"location":"trial/#running-the-integration-procedure","title":"Running the integration procedure","text":"<ol> <li>Before starting the fusion process, ensure that your IDE is configured correctly (see Installation).</li> <li> <p>To run the integration procedure in the Python environment, the initial parameters are required to be defined by the user. In the following, you can find an example script to run fusion for <code>ALL</code> stations in <code>multidefusion_trial</code> folder using <code>forward-backward</code> method with <code>0.03</code> mm/day<sup>2</sup> noise level: </p> <pre><code>from multidefusion.fusion import run_fusion\n\nintegration = run_fusion(stations=\"ALL\", path=\"/path/to/multidefusion_trial/\", method=\"forward-backward\", noise=0.03)\n</code></pre> </li> <li> <p>More examples can be found in the Usage section.</p> </li> </ol>"},{"location":"usage/","title":"Usage and examples","text":"<p>The software provides integration of permanent GNSS data and radar InSAR observations, considering a particular computational method such as DInSAR, SBAS and PSI.</p>"},{"location":"usage/#import-the-library","title":"Import the library","text":"<ol> <li>MultiDEFusion software works in the Python environment. Before starting the fusion process, ensure that your IDE is configured correctly (see Installation).</li> <li>To run the integration procedure at the beginning of the code, import run_fusion procedure:</li> </ol> <p><code>from multidefusion.fusion import run_fusion</code></p>"},{"location":"usage/#description-of-initial-parameters","title":"Description of initial parameters","text":"<p>In the following, to <code>run_fusion</code>, the initial arguments are required to be defined by the user.</p> <p><code>integration = run_fusion(stations, path, method, noise)</code></p> <p>The description of initial parameters:</p> Argument Type Description stations <code>list</code> or <code>str</code> List of a particular station folders or \"ALL\" to process all folders found in the specified path path <code>str</code> Path to the directory containing station data method <code>str</code> Fusion method. Options are \"forward\" or \"forward-backward\" noise <code>float</code> Noise level of the integration system [mm/day<sup>2</sup>]"},{"location":"usage/#important-remarks","title":"Important remarks","text":"<ol> <li> <p>The integration procedure can include a single station folder (e.g., <code>stations = [\"ID01\"]</code>) stored in the <code>path</code>, a list of stations (e.g., <code>stations = [\"ID01\", \"ID02\", \"POINT_5\"]</code>) or ALL of them (<code>stations = \"ALL\"</code>).</p> </li> <li> <p>For each particular station's folder, it is necessary to store the geodetic data in the ASCII files (see Input).</p> </li> <li> <p>Every ASCII file stored in the station's folder will be included in the integration procedure with respect to the chosen method (<code>\"forward\"</code> or <code>\"forward-backward\"</code>).</p> </li> <li> <p>The <code>noise</code> level expressed as acceleration in mm/day<sup>2</sup> should by assigned by user in the empirical way.</p> </li> <li> <p>In the library, the zero-mean acceleration model is introduced as the system noise matrix (Teunissen, 2009).</p> </li> </ol>"},{"location":"usage/#examples","title":"Examples","text":"<ol> <li> <p>An example script to run fusion for <code>ALL</code> stations in <code>/your/path/to/multidefusion_trial/</code> folder using <code>forward-backward</code> method with <code>0.03</code> mm/day<sup>2</sup> noise level. To get a trial MultiDEFusion repository, see Trial section:  </p> <pre><code>from multidefusion.fusion import run_fusion\n\nintegration = run_fusion(stations=\"ALL\", path=\"/your/path/to/multidefusion_trial/\", method=\"forward-backward\", noise=0.03)\n</code></pre> </li> <li> <p>An example script to run fusion for <code>ID01</code> station in <code>C:\\path\\to\\folder\\</code> folder using <code>forward</code> method with <code>0.05</code> mm/day<sup>2</sup> noise level.</p> <pre><code>from multidefusion.fusion import run_fusion\n\nintegration = run_fusion(stations=[\"ID01\"], path=\"C:\\\\path\\\\to\\\\folder\\\\\", method=\"forward\", noise=0.05)\n</code></pre> </li> <li> <p>An example script to run fusion for <code>ID01</code> and <code>POINT_5</code> stations in <code>C:\\path\\to\\folder\\</code> folder using <code>forward-backward</code> method with <code>0.045</code> mm/day<sup>2</sup> noise level.</p> <pre><code>from multidefusion.fusion import run_fusion\n\nintegration = run_fusion(stations=[\"ID01\", \"POINT_5\"], path=\"C:\\\\path\\\\to\\\\folder\\\\\", method=\"forward-backward\", noise=0.045)\n</code></pre> </li> </ol> <p>Teunissen, P. (2009). Dynamic data processing: Recursive least-squares.</p>"}]}